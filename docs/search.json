[
  {
    "objectID": "solution-day4-visualizing.html",
    "href": "solution-day4-visualizing.html",
    "title": "Solutions Day 4",
    "section": "",
    "text": "welcome to class!"
  },
  {
    "objectID": "solution-day4-visualizing.html#data-introduction",
    "href": "solution-day4-visualizing.html#data-introduction",
    "title": "Solutions Day 4",
    "section": "Data Introduction",
    "text": "Data Introduction\nBefore we begin, let’s bring in the data we’ve been working with.\n\ndf &lt;- read_csv(\"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries.csv\")\n\nRows: 64925 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): femaDeclarationString, state, declarationType, incidentType, decl...\ndbl   (9): disasterNumber, fyDeclared, ihProgramDeclared, iaProgramDeclared,...\ndttm  (6): declarationDate, incidentBeginDate, incidentEndDate, disasterClos...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_new &lt;- df |&gt; \n  mutate(GEOID=str_c(fipsStateCode, fipsCountyCode))\n\ncounty_pop &lt;- read_csv(\"data-raw/county_population.csv\")\n\nRows: 3221 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): GEOID, NAME, variable\ndbl (2): estimate, moe\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\njoined_new &lt;- left_join(df_new, county_pop, by=\"GEOID\") |&gt; \n  mutate(year=year(incidentBeginDate))\n\nLet’s take a look at what were working with. Check out joined_new with the usual function we use:\n\nglimpse(joined_new)\n\nRows: 64,925\nColumns: 31\n$ femaDeclarationString    &lt;chr&gt; \"FM-5389-AZ\", \"FM-5389-AZ\", \"FM-5464-RI\", \"FM…\n$ disasterNumber           &lt;dbl&gt; 5389, 5389, 5464, 5463, 5462, 4731, 5460, 545…\n$ state                    &lt;chr&gt; \"AZ\", \"AZ\", \"RI\", \"KS\", \"NE\", \"CO\", \"OK\", \"OK…\n$ declarationType          &lt;chr&gt; \"FM\", \"FM\", \"FM\", \"FM\", \"FM\", \"DR\", \"FM\", \"FM…\n$ declarationDate          &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ fyDeclared               &lt;dbl&gt; 2021, 2021, 2023, 2023, 2023, 2023, 2023, 202…\n$ incidentType             &lt;chr&gt; \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Floo…\n$ declarationTitle         &lt;chr&gt; \"TELEGRAPH FIRE\", \"TELEGRAPH FIRE\", \"QUEENS R…\n$ ihProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ iaProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ paProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hmProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ incidentBeginDate        &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ incidentEndDate          &lt;dttm&gt; NA, NA, 2023-04-16, 2023-04-16, NA, 2023-06-…\n$ disasterCloseoutDate     &lt;dttm&gt; 2023-09-29, 2023-09-29, NA, NA, NA, NA, NA, …\n$ tribalRequest            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fipsStateCode            &lt;chr&gt; \"04\", \"04\", \"44\", \"20\", \"31\", \"08\", \"40\", \"40…\n$ fipsCountyCode           &lt;chr&gt; \"007\", \"021\", \"009\", \"201\", \"025\", \"009\", \"14…\n$ placeCode                &lt;dbl&gt; 99007, 99021, 99009, 99201, 99025, 99009, 991…\n$ designatedArea           &lt;chr&gt; \"Gila (County)\", \"Pinal (County)\", \"Washingto…\n$ declarationRequestNumber &lt;dbl&gt; 21041, 21041, 23042, 23038, 23036, 23081, 230…\n$ lastIAFilingDate         &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ lastRefresh              &lt;dttm&gt; 2023-09-29 21:02:14, 2023-09-29 21:02:14, 20…\n$ hash                     &lt;chr&gt; \"14b4a2c314124cae33e8ec790782a12c2af10a4c\", \"…\n$ id                       &lt;chr&gt; \"226d44a3-418a-4102-9a55-385ea6599b2a\", \"7304…\n$ GEOID                    &lt;chr&gt; \"04007\", \"04021\", \"44009\", \"20201\", \"31025\", …\n$ NAME                     &lt;chr&gt; \"Gila County, Arizona\", \"Pinal County, Arizon…\n$ variable                 &lt;chr&gt; \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B0…\n$ estimate                 &lt;dbl&gt; 53846, 447559, 126139, 5474, 26022, 3570, 519…\n$ moe                      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, 164, NA, …\n$ year                     &lt;dbl&gt; 2021, 2021, 2023, 2023, 2023, 2023, 2023, 202…\n\n\nOkay, let’s transform the data like we did before.\nCan you count up how many disasters there have been per year? Not total disasters. Individual disasters.\nCall the new column “total”.\n\nannual_disasters &lt;- joined_new |&gt; \n  count(incidentType, year, name=\"total\") \n  \n    \nannual_disasters\n\n\n\n  \n\n\n\n\nYou only need to add one new line. \nThe function starts with a *c* and don't forget to name the column you're creating.\nyear should be the second argument in the function.\n\nAlright, we’ve got a lot of data going back decades.\nThe benefit of working quickly with data in R is how you can quickly visualize it to spot any trends.\nLet’s do that.\nBut before we do, let’s create another data frame specifically for fires.\nFilter incidentType for “Fire”, please.\n\nannual_fires &lt;- annual_disasters  |&gt; \n  filter(incidentType==\"Fire\")\n\nannual_fires\n\n\n\n  \n\n\n\n\nThe function starts with a *f* and don't forget =="
  },
  {
    "objectID": "solution-day4-visualizing.html#grammar-of-graphics",
    "href": "solution-day4-visualizing.html#grammar-of-graphics",
    "title": "Solutions Day 4",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nThe grammar of graphics lets you approach visualizations structurally, letting you combine and swap out graphical elements into figures that display data meaningfully.\nIt takes two lines of code.\nThis is what the code and chart looks like.\nRun the code below.\n\nggplot(data=annual_fires) +\n  geom_col(aes(x=year, y=total)) \n\n\n\n\nBasically, every of a chart can created using these aesthetic components and mapping them:\n\nMapping data to aesthetics\nAesthetic\n\nThe visual property of a graph\nPosition, shape, color, etc.\n\nData\n\nA column in a data set\n\n\nHere’s are the core components of the chart above:\n\n\n\n\nData\n\n\nAesthetic\n\n\nGraphic/Geometry\n\n\n\n\nYear\n\n\nPosition (x-axis) \n\n\nColumn\n\n\n\n\nTotal disasters\n\n\nPosition (y-axis)\n\n\nPoint\n\n\n\n\n\nHere’s how the data was mapped in ggplot2 code from the annual_fires data frame:\n\n\n\n\nData\n\n\naes()\n\n\ngeom\n\n\n\n\nyear\n\n\nx\n\n\ngeom_col()\n\n\n\n\ntotal\n\n\ny\n\n\ngeom_col()\n\n\n\n\n\n\nggplot() template\nHere’s the dataframe called annual_fires as a reminder:\n\nannual_fires |&gt; slice(1:5)\n\n\n\n  \n\n\n\nOkay, now that you see where all the pieces come from, here’s how ggplot() works.\nAt its core you need to tell it what data you’re using, what type of visual geometry you want to use, and what variables you want represented from the data.\nImportant: We have to use + signs between each line, not |&gt;. This is because ggplot() was created before the tidyverse piping method was established.\n\ninclude_graphics(\"images/flair.png\")\n\n\n\n\n\n\n\nGrammatical layers\nWhen constructing charts, so far we know about data, aesthetics, and geometries.\nThink of these components as layers.\n\n\n\n\n\n\n\n\n\nAdd them to foundational ggplot() with +\nThese are all the arguments we can enhance the data viz with.\nChange the colors of the viz based on a column. Or the size of the shape.\nOr the opacity or the gradient.\nPossible aesthetics\n\n\n\n\n\nWe can also swap out the different geometry types.\nIf you don’t want a bar, you can use a line. Or a point.\nYou can even use shapefiles.\nPossible geoms\n\n\n\n\n\n\n\n\n\nTHERE ARE SO MANY GEOMS for different visualizations. Here are the official ones.\nTry the code from above again but this time use geom_point() and then try it with geom_line()\n\nggplot(data=annual_fires) +\n  geom_point(aes(x=year, y=total)) \n\n\n\n\nYou can really start to see the power of cycling quickly through different chart styles to see which one is most effective at telling the story you want to tell.\nSo after you have the very basic elements needed to create a chart, you can build and style it with more layers.\nBecause the defaults are rarely what you want and effective dataviz comes from small decisions you make along the way.\n\n\nAdditional layers\nThere are many of other grammatical layers we can use to describe graphs.\nWe sequentially add layers onto the foundational ggplot() plot to create complex figures.\n\n\n\n\n\n\n\n\n\nScales change the properties of the variable mapping.\nHere are a few examples:\n\n\n\n\nExample layer\n\n\nWhat it does\n\n\n\n\nscale_x_continuous()\n\n\nMake the x-axis continuous\n\n\n\n\nscale_x_continuous(breaks = 1:5) \n\n\nManually specify axis ticks\n\n\n\n\nscale_x_date()\n\n\nConsiders x-axis dates\n\n\n\n\nscale_color_gradient()\n\n\nUse a gradient\n\n\n\n\nscale_fill_viridis_d()\n\n\nFill with discrete viridis colors\n\n\n\n\n\nCheck out the x-axis.\n\n\nExercise 2\nNow add scale_x_continuous(limits=c(2010, 2022), breaks=2010:2022) to the bottom of the code.\n\nggplot(data=annual_fires) +\n  geom_col(aes(x=year, y=total)) +\n  scale_x_continuous(limits=c(2010, 2022), breaks=2010:2022)\n\n\n\n\nDo you see the difference at the bottom of the chart compared to the one above it?\nIt limited the scope of the x-axis so it didn’t go back to the ’50s anymore.\nAnd it specifically labeled the years 2010 through 2022."
  },
  {
    "objectID": "solution-day4-visualizing.html#facets",
    "href": "solution-day4-visualizing.html#facets",
    "title": "Solutions Day 4",
    "section": "Facets",
    "text": "Facets\nThe next possible layer allows for small multiples. It’s really neat.\nFacets show subplots for different subsets of data.\n\n\n\n\nExample layer\n\n\nWhat it does\n\n\n\n\nfacet_wrap(vars(incidentType))\n\n\nPlot for each disaster type\n\n\n\n\nfacet_wrap(vars(incidentType, year)) \n\n\nPlot for each disaster type/year\n\n\n\n\nfacet_wrap(…, ncol = 1)\n\n\nPut all facets in one column\n\n\n\n\nfacet_wrap(…, nrow = 1)\n\n\nPut all facets in one row\n\n\n\n\nThe table above shows all the different ways you can use facets– you can break it out by one extra variable or even two.\nWe’ll use the annual disasters this time so we have more than just the fires.\nBut we’ll filter it to hurricanes and fires and floods.\nAnd we can combine it with pipes before we use ggplot() it.\nAdd the facet_wrap() line on the variable incidentType (like the first example in the table above).\n\nannual_disasters |&gt; \n  filter(incidentType %in% c(\"Hurricane\", \"Fire\", \"Flood\")) |&gt; \nggplot() +\n  geom_col(mapping=aes(x= year, y= total)) +\n  scale_x_continuous(limits=c(2010, 2022), breaks=2010:2022) +\n  facet_wrap(vars(incidentType))\n\n\n\n\nAlright, looks like the x-axis labels are getting a little crowded.\nWe can’t even read it!\n\nTry again!\nNow, try it with ncol=1 as an additional argument in facet_wrap()\n\nannual_disasters |&gt; \n  filter(incidentType %in% c(\"Hurricane\", \"Fire\", \"Flood\")) |&gt; \nggplot() +\n  geom_col(mapping=aes(x= year, y= total)) +\n  scale_x_continuous(limits=c(2010, 2022), breaks=2010:2022) +\n  facet_wrap(vars(incidentType), ncol=1)"
  },
  {
    "objectID": "solution-day4-visualizing.html#labels",
    "href": "solution-day4-visualizing.html#labels",
    "title": "Solutions Day 4",
    "section": "Labels",
    "text": "Labels\nNow we can add more customization to the chart.\nTo make it really shine!\n\n\n\n\nExample layer\n\n\nWhat it does\n\n\n\n\nlabs(title = “Neat title”)\n\n\nTitle\n\n\n\n\nlabs(caption = “Something”)\n\n\nCaption\n\n\n\n\nlabs(y = “Something”)\n\n\ny-axis\n\n\n\n\nlabs(color = “Type”)\n\n\nTitle of size legend\n\n\n\n\n\nTitle should be “Disaster declarations since 2010”\nLabel for the x-axis should be blank (aka ““) because the years are obvious\nLabel for the y-axis should be “Total”\nCaption should be “Data: FEMA”\n\nAdd those labels below:\n\nannual_disasters |&gt; \n  filter(incidentType %in% c(\"Hurricane\", \"Fire\", \"Flood\")) |&gt; \nggplot() +\n  geom_col(mapping=aes(x= year, y= total)) +\n  scale_x_continuous(limits=c(2010, 2022), breaks=2010:2022) +\n  facet_wrap(vars(incidentType), ncol=1) +\n  labs(\n    title = \"Disaster declarations since 2010\",\n    x = \"\",\n    y = \"Total\",\n    caption= \"Data: FEMA\"\n  )\n\n\n\n\n\nYou only need to call labs() once.\nWithin parentheses, just separate the arguments with commas. You don't use the plus signs."
  },
  {
    "objectID": "solution-day4-visualizing.html#themes",
    "href": "solution-day4-visualizing.html#themes",
    "title": "Solutions Day 4",
    "section": "Themes",
    "text": "Themes\nChange the appearance of anything in the plot.\nWhile you can customize every font, color, gradient, etc, you can set these styles up ahead of time or use the ones others have created.\nThere are many built-in themes.\n\n\n\n\nExample layer\n\n\nWhat it does\n\n\n\n\ntheme_grey()\n\n\nDefault grey background\n\n\n\n\ntheme_bw()\n\n\nBlack and white\n\n\n\n\ntheme_dark()\n\n\nDark\n\n\n\n\ntheme_minimal()\n\n\nMinimal\n\n\n\n\nTry out the different themes listed above in the code below.\n\nannual_disasters |&gt; \n  filter(incidentType %in% c(\"Hurricane\", \"Fire\", \"Flood\")) |&gt; \nggplot() +\n  geom_col(mapping=aes(x= year, y= total)) +\n  scale_x_continuous(limits=c(2010, 2022), breaks=2010:2022) +\n  facet_wrap(vars(incidentType), ncol=1) +\n  labs(\n    title = \"Disaster declarations since 2010\",\n    x = \"\",\n    y = \"Total\",\n    caption= \"Data: FEMA\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nMore themes\nThere are a collections of pre-built themes online, like the ggthemes package.\nOrganizations often make their own custom themes, like the BBC.\n\n\n\n\n\n\n\nTheme adjustments\nMake theme adjustments with theme()\nThere are a billion options here!\nAdd this chunk of code in the exercise below it:\n\ntheme_bw() + \ntheme(plot.title = element_text(face = \"bold\"),\n      panel.grid = element_blank(),\n      axis.title.y = element_text(face = \"italic\"))\n\n\n\nExercise 5\n\nannual_disasters |&gt; \n  filter(incidentType %in% c(\"Hurricane\", \"Fire\", \"Flood\")) |&gt; \nggplot() +\n  geom_col(mapping=aes(x= year, y= total)) +\n  scale_x_continuous(limits=c(2010, 2022), breaks=2010:2022) +\n  facet_wrap(vars(incidentType), ncol=1) +\n  labs(\n    title = \"Disaster declarations since 2010\",\n    x = \"\",\n    y = \"Total\",\n    caption= \"Data: FEMA\"\n  ) +\ntheme_bw() + \ntheme(plot.title = element_text(face = \"bold\"),\n      panel.grid = element_blank(),\n      axis.title.y = element_text(face = \"italic\"))\n\nWarning: Removed 144 rows containing missing values (`position_stack()`).\n\n\nWarning: Removed 6 rows containing missing values (`geom_col()`).\n\n\n\n\n\nThese were just a few examples of layers.\n\n\n\n\n\n\n\n\n\nSee the ggplot2 documentation for complete examples of everything you can do\n\n\nDone!\nCongrats on completing the walkthroughs for Class 4!\n\n\nBut there’s more"
  },
  {
    "objectID": "solution-day4-visualizing.html#putting-it-all-together",
    "href": "solution-day4-visualizing.html#putting-it-all-together",
    "title": "Solutions Day 4",
    "section": "Putting it all together",
    "text": "Putting it all together\nUsually we’d next go over all the different geom_ visualizations you can create using a single data set.\n\nA ggplot2 tutorial for beautiful plotting in R\n\nBut we’re going to use more real-life data that I think will be relevant to your journalism.\nThe data set is raw deaths data from San Diego. It’s a combination of 1997-2019 data from San Diego’s data portal and 2020 data from a public information request on MuckRock.\nDownloads this data san_diego.csv and place it in your project folder.\nMake sure you’ve got the proper libraries loaded.\n\nReady for the code?\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\n\nsd &lt;- read_csv(\"data-raw/san_diego.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 66218 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (21): Death Date, Security Status, Gender, Race, Ethnic Group (Standardi...\ndbl  (5): Year, Age in Years, Event Zip, Death Zip, Res Zip\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsd &lt;- clean_names(sd)\n\nsd_adjusted &lt;- sd |&gt; \n  mutate(death_date=mdy(death_date)) |&gt; \n  mutate(month=month(death_date, label=TRUE, abbr=TRUE)) \n\n\nsd_adjusted\n\n\n\n  \n\n\n\nAlright, I’ve cleaned it up for you.\nThere’s some really great data here. It’s got gender, race, and several levels of manner of death stretching back to 1997 and through possibly October of 2020.\nLet’s start summarizing the data so we can start looking for trends.\n\n\nExercise 7\nCan you count up the number of deaths by manner_of_death by month and year, please?\n\nsd_month &lt;- sd_adjusted |&gt; \n  count(year, month, manner_of_death, name=\"deaths\") |&gt; \n  mutate(date=mdy(paste0(month, \" 1, \", year)))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date = mdy(paste0(month, \" 1, \", year))`.\nCaused by warning:\n!  35 failed to parse.\n\nsd_month\n\n\n\n  \n\n\n\n\nHint: count / deaths\n\n\n\nExercise 8\nNow, fill in the blank below to create a line chart for each type of death\n\nsd_month |&gt; \n  ggplot(aes(x=date, y=deaths, color=manner_of_death, group=manner_of_death)) +\n  geom_line() +\n  labs(title=\"Monthly deaths in San Diego\",\n       caption=\"Source; San Diego Medical Examiner\")\n\nWarning: Removed 35 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAlright, a little messy. We can see some overall growing trend in one category, but that’s it.\nPerhaps looking at the data by month is too granular. Let’s step back and aggregate by year.\n\n\nBy year\nI went ahead and created a new sd_year dataframe counting up the deaths by year (while excluding October, November, and December) so we can compare prior years to this year.\n\nsd_year &lt;- sd_adjusted |&gt; \n  # if we're going to compare this year to previous years, we need to exclude data we don't have yet\n  filter(!month %in% c(\"Oct\", \"Nov\", \"Dec\")) |&gt; \n  count(year, manner_of_death, name=\"deaths\") \n\ndatatable(sd_year)\n\n\n\n\n\n\n\n\nExercise 9\nOkay, your turn to make a chart.\nMake me a faceted chart that breaks out all the individual manner_of_death into its own chart, small-multiple style.\n\nsd_year |&gt; ggplot(aes(x=year, y=deaths)) +\n  geom_col() +\n  facet_wrap(vars(manner_of_death), ncol=4)\n\n\n\n\nAlright, now we’re getting somewhere.\nLooks like accidents have been trending up year over year.\nIf we focus on 2020, it looks like Natural causes have increased. But it also increased in 2019.\nSuicides actually look down this year compared to prior years.\nHm…\nWhat else can we do?\nWe can try to measure Excess Deaths\nAverage each month by every year prior to 2020 and compare it to 2020’s trend line.\nI’ll give you the code again.\nWe’re going to use a function called case_when to create a new column called year_type. If the year is 2020, then it will be “2020” otherwise it will be “1997-2020”. And then we find the average number of deaths for each month for those two groups.\n\nsd_group &lt;- sd_adjusted |&gt; \n  filter(!month %in% c(\"Oct\", \"Nov\", \"Dec\")) |&gt; \n  count(year, month, manner_of_death, name=\"deaths\") |&gt; \n  mutate(year_type=case_when(\n    year==2020 ~ \"2020\",\n    TRUE ~ \"1997-2019\"\n  )) |&gt; \n  group_by(month, manner_of_death, year_type) |&gt; \n  summarize(avg_deaths=mean(deaths, na.rm=T)) |&gt; \n  filter(!is.na(month))\n\ndatatable(sd_group)\n\n\n\n\n\n\nLooking very smooth.\nLet’s chart it.\n\n\nExercise 10\nCan you please create a faceted line chart of the data above? But with year_type as two different lines?\nFill in the three blanks to generate the chart.\n\nggplot(sd_group, aes(x=month, y=avg_deaths, color=year_type, group=year_type)) +\n  geom_line() +\n  facet_wrap(vars(manner_of_death), scales=\"free_y\", ncol=2)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\nGreat.\nNow there’s some interesting trend in July and August, right?\nAnd it shows that maybe the last month of data is so low. It’s possible that the data for the month is incomplete and should be excluded from analysis.\nHow about we come up with a number for the nutgraf of a story?\nHow many more accidental cause deaths are there in 2020 compared to the historical average?\nExclude September since it seems so off\nHere’s a look at the dataframe you can estimate this with.\n\ndatatable(sd_group)\n\n\n\n\n\n\nWhat code do you put below to prove this sentence:\n“There were X% more accidental deaths in in Jan 2020 than the historical average of January in San Diego”\nhint: you’ll need filter(), pivot_wider(), group_by(), summarize(), mutate(), and math\n\nsd_group  |&gt; \n  filter(month==\"Jan\" & manner_of_death==\"Accident\") |&gt;\n  pivot_wider(names_from=\"year_type\", values_from=\"avg_deaths\") %&gt;% \n  mutate(change=(`2020`-`1997-2019`)/`1997-2019`*100)\n\n\n\n  \n\n\n\nGood job!\n\n\nFurther exploratory visual analysis\nAlright, comparing 2020 to the average from previous years seems to be a good decision.\nSome interesting trends that we could actually write about are surfacing.\nLet’s compare that overall instead of by month.\nHere’s the code. We’re also excluding September based on what we discovered in the chart above.\n\nsd_group_compare &lt;- sd_adjusted |&gt; \n  filter(!month %in% c(\"Sep\", \"Oct\", \"Nov\", \"Dec\")) |&gt; \n  count(year, manner_of_death, name=\"deaths\") |&gt; \n  mutate(year_type=case_when(\n    year==2020 ~ \"2020\",\n    TRUE ~ \"1997-2019\"\n  )) |&gt; \n  group_by(manner_of_death, year_type) |&gt; \n  summarize(avg_deaths=round(mean(deaths, na.rm=T)))\n\n`summarise()` has grouped output by 'manner_of_death'. You can override using\nthe `.groups` argument.\n\ndatatable(sd_group_compare)\n\n\n\n\n\n\n\n\nExercise 11\nRun the code below with manner_of_death as x and avg_deaths as y.\nThen swap them.\nWhich do you prefer and why?\n\nggplot(sd_group_compare, aes(x=manner_of_death, y=avg_deaths, fill=year_type)) +\n  geom_bar(position=\"dodge\", stat=\"identity\") \n\n\n\n\nAlright, before we go, I want to clean things up some.\nI want to get rid of the manners of death that have barely any and I want to reorder the labels so that it’s in alphabetical order.\n\n\nExercise 12\nTake a look at the code below. Absorb it.\nThen generate the code and see what pops up.\n\nsd_group_compare |&gt; \n  filter(!manner_of_death %in% c(\"Other\", \"Family Paid Autopsy\")) |&gt; \n  filter(!is.na(manner_of_death)) |&gt; \n  ggplot(aes(x=avg_deaths, y=forcats::fct_rev(manner_of_death),  fill=year_type)) +\n  geom_bar(position=\"dodge\", stat=\"identity\") +\n  labs(title=\"Manner of death in San Diego\",\n       subtitle=\"January and August deaths in 2020 compared to average deaths between 1997 and 2019\",\n       caption=\"Source: San Diego Medical Examiner\",\n       y=\"Manner of death\",\n       x=\"Average deaths\",\n       fill=\"Year\") +\n  theme_minimal()\n\n\n\n\n\n\nStory\nSo, what do you think the story is?\nIn San Diego, accidents are way up, suicides are slightly up, and meanwhile homicides are down.\nWhat can we do next?\nWell, dig into the accidents, perhaps, and see if there’s any explanation for the huge increase.\nAlright, congratulations on going on this exploratory data visualization journey.\nSome of the answers won’t appear right away unless you poke around and look at the data in as many ways as possible."
  },
  {
    "objectID": "solution-day2-analysis.html",
    "href": "solution-day2-analysis.html",
    "title": "Solutions Day 2",
    "section": "",
    "text": "These is a completed notebook from the second day of the Center for Health Journalism Data Fellowship."
  },
  {
    "objectID": "solution-day2-analysis.html#goals",
    "href": "solution-day2-analysis.html#goals",
    "title": "Solutions Day 2",
    "section": "Goals",
    "text": "Goals\nTo learn about: arrange, filter, slice, group_by, summarize\nTo find several values from our data:\n\nThe coldest and warmest days\nThe rainiest and snowiest days\nYears with most snow days\nYears with most 100+ days\nYears with most rain\nEarliest day to reach 100+ each year\n\nWith this lesson we’ll just use Texas data. (You theoretically could use a different state, but would need to adjust your code to import the right data, use valid cities, etc.)"
  },
  {
    "objectID": "solution-day2-analysis.html#setup",
    "href": "solution-day2-analysis.html#setup",
    "title": "Solutions Day 2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "solution-day2-analysis.html#import",
    "href": "solution-day2-analysis.html#import",
    "title": "Solutions Day 2",
    "section": "Import",
    "text": "Import\nImport your cleaned data using read_rds() and save it into an object:\n\ntx_clean &lt;- read_rds(\"data-processed/tx_clean.rds\")"
  },
  {
    "objectID": "solution-day2-analysis.html#arrange",
    "href": "solution-day2-analysis.html#arrange",
    "title": "Solutions Day 2",
    "section": "Arrange",
    "text": "Arrange\nFind the coldest day, warmest day, most snow, most rain.\n\nColdest day\n\ntx_clean |&gt; \n  arrange(tmin) |&gt; \n  select(city, date, tmin)\n\n\n\n  \n\n\n\n\n\nHotest day\n\ntx_clean |&gt; \n  arrange(desc(tmax)) |&gt; \n  select(city, date, tmax)\n\n\n\n  \n\n\n\n\n\nOYO: Most rain\nFind the days with the most rain.\n\ntx_clean |&gt; \n  arrange(desc(rain)) |&gt; \n  select(city, date, rain)\n\n\n\n  \n\n\n\n\n\nOYO: Most snow\nFind the days with the most snow.\n\ntx_clean |&gt; \n  arrange(desc(snow)) |&gt; \n  select(city, date, snow)"
  },
  {
    "objectID": "solution-day2-analysis.html#filter",
    "href": "solution-day2-analysis.html#filter",
    "title": "Solutions Day 2",
    "section": "Filter",
    "text": "Filter\nFind days that are 100+.\n\ntx_clean |&gt; \n  filter(tmax &gt;= 100) |&gt; \n  select(city, date, tmax)\n\n\n\n  \n\n\n\nFilter for days in Dallas that are 100+\n\ntx_clean |&gt; \n  filter(tmax &gt;= 100, city == \"Dallas\") |&gt; \n  select(city, date, tmax)\n\n\n\n  \n\n\n\nFind days where it snowed, or there is snow still on the ground.\n\ntx_clean |&gt; \n  filter(snow &gt; 0 | snwd &gt; 0) |&gt; \n  select(city, date, snow, snwd)\n\n\n\n  \n\n\n\n\nOYO: Snow days in Dallas\nFind days where it snowed or there is snow on the ground, but only in Dallas.\n\ntx_clean |&gt; \n  filter(snow &gt; 0 | snwd &gt; 0, city == \"Dallas\") |&gt; \n  select(city, date, snow, snwd)"
  },
  {
    "objectID": "solution-day2-analysis.html#slice",
    "href": "solution-day2-analysis.html#slice",
    "title": "Solutions Day 2",
    "section": "Slice",
    "text": "Slice\nUse slice_min to find the coldest day in our data.\n\ntx_clean |&gt; \n  slice_min(tmin) |&gt; \n  select(city, date, tmin)"
  },
  {
    "objectID": "solution-day2-analysis.html#group-and-slice",
    "href": "solution-day2-analysis.html#group-and-slice",
    "title": "Solutions Day 2",
    "section": "Group and slice",
    "text": "Group and slice\nAdd group_by to find the coldest day in each city.\n\ntx_clean |&gt; \n  group_by(city) |&gt; \n  slice_min(tmin) |&gt; \n  select(city, date, tmin)\n\n\n\n  \n\n\n\n\nOYO: Hottest day in each city\nUse group_by and slice_max to find the hottest days in each city. Note there might be some ties.\n\ntx_clean |&gt; \n  group_by(city) |&gt; \n  slice_max(tmax) |&gt; \n  select(city, date, tmax)\n\n\n\n  \n\n\n\n\n\nMultiple groups\nHottest day each year in each city\n\ntx_clean |&gt; \n  group_by(yr, city) |&gt; \n  slice_max(tmax) |&gt; \n  select(city, tmax) |&gt; \n  distinct()\n\nAdding missing grouping variables: `yr`"
  },
  {
    "objectID": "solution-day2-analysis.html#summarize",
    "href": "solution-day2-analysis.html#summarize",
    "title": "Solutions Day 2",
    "section": "Summarize",
    "text": "Summarize\nSummarize to find our first date, last date and number of rows.\n\ntx_clean |&gt; \n  summarize(\n    e_date = min(date),\n    l_date = max(date),\n    cnt = n()\n  )"
  },
  {
    "objectID": "solution-day2-analysis.html#group-and-summarize",
    "href": "solution-day2-analysis.html#group-and-summarize",
    "title": "Solutions Day 2",
    "section": "Group and summarize",
    "text": "Group and summarize\nGroup the data by city and find the first date, last date and number of rows.\n\ntx_clean |&gt; \n  group_by(city) |&gt; \n  summarise(\n    e_date = min(date),\n    l_date = max(date),\n    cnt = n()\n  )\n\n\n\n  \n\n\n\nAdd city and yr as a group:\n\ntx_clean |&gt; \n  group_by(city, yr) |&gt; \n  summarise(\n    e_date = min(date),\n    l_date = max(date),\n    cnt = n()\n  )\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "solution-day2-analysis.html#group-and-summarize-count",
    "href": "solution-day2-analysis.html#group-and-summarize-count",
    "title": "Solutions Day 2",
    "section": "Group and summarize: Count",
    "text": "Group and summarize: Count\nFind the number of days in Austin that were 100+. Start first with prepping the data to get the date ranges we need:\n\n# get Austin data\natx &lt;- tx_clean |&gt; filter(city == \"Austin\", yr &gt; 1938)\n\n# check dates\natx$date |&gt; summary()\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"1939-01-01\" \"1960-03-09\" \"1981-05-16\" \"1981-05-16\" \"2002-07-23\" \"2023-09-30\" \n\n# peek\natx |&gt; glimpse()\n\nRows: 30,954\nColumns: 10\n$ city &lt;chr&gt; \"Austin\", \"Austin\", \"Austin\", \"Austin\", \"Austin\", \"Austin\", \"Aust…\n$ date &lt;date&gt; 1939-01-01, 1939-01-02, 1939-01-03, 1939-01-04, 1939-01-05, 1939…\n$ rain &lt;dbl&gt; 0.00, 0.00, 0.01, 0.14, 0.00, 0.00, 0.00, 0.12, 0.41, 0.00, 0.34,…\n$ snow &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ snwd &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ tmax &lt;dbl&gt; 69, 73, 74, 73, 70, 72, 72, 69, 73, 66, 58, 49, 54, 58, 53, 55, 6…\n$ tmin &lt;dbl&gt; 32, 37, 56, 49, 41, 39, 52, 54, 53, 48, 46, 43, 35, 42, 34, 29, 4…\n$ yr   &lt;dbl&gt; 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939,…\n$ mn   &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, …\n$ yd   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n\n\nThen do the calculation to get hot days:\n\natx |&gt; \n  filter(tmax &gt;= 100) |&gt; \n  group_by(yr) |&gt; \n  summarize(hot_days = n()) |&gt; \n  arrange(desc(hot_days)) |&gt; \n  filter(hot_days &gt;= 50)\n\n\n\n  \n\n\n\nFind the years with the most 100+ degree days in each city.\n\ntx_clean |&gt; \n  filter(tmax &gt;= 100) |&gt; \n  group_by(yr, city) |&gt; \n  summarize(hot_days = n()) |&gt; \n  arrange(desc(hot_days)) |&gt; \n  filter(hot_days &gt;= 30)\n\n`summarise()` has grouped output by 'yr'. You can override using the `.groups`\nargument.\n\n\n\n\n  \n\n\n\n\nOYO: Most snow days by city each year\nCount only the days that where it snowed.\n\ntx_clean |&gt; \n  filter(snow &gt; 0) |&gt; \n  group_by(city, yr) |&gt; \n  summarise(snow_days = n()) |&gt; \n  arrange(desc(snow_days))\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "solution-day2-analysis.html#group-and-summarize-math",
    "href": "solution-day2-analysis.html#group-and-summarize-math",
    "title": "Solutions Day 2",
    "section": "Group and Summarize: Math",
    "text": "Group and Summarize: Math\nYears with most rain in each city.\n\ntx_yr_rain &lt;- tx_clean |&gt; \n  filter(yr &gt; 1939, yr &lt; 2023) |&gt;\n  group_by(city, yr) |&gt; \n  summarise(tot_rain = sum(rain, na.rm = TRUE)) |&gt; \n  arrange(city, desc(tot_rain))\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\ntx_yr_rain\n\n\n\n  \n\n\n\nThe most rain in each city, sliced:\n\ntx_yr_rain |&gt; \n  group_by(city) |&gt; \n  slice_max(tot_rain, n = 3)\n\n\n\n  \n\n\n\nThe least rain in each city, sliced:\n\ntx_yr_rain |&gt; \n  group_by(city) |&gt; \n  slice_min(tot_rain, n = 3)\n\n\n\n  \n\n\n\n\nOYO: Years with most snow\nFind the years with the most total snow in each city\n\ntx_yr_snow &lt;- tx_clean |&gt; \n  group_by(city, yr) |&gt; \n  summarize(tot_snow = sum(snow)) |&gt; \n  arrange(city, desc(tot_snow))\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\ntx_yr_snow\n\n\n\n  \n\n\n\nMost snow, sliced:\n\ntx_yr_snow |&gt; \n  group_by(city) |&gt; \n  slice_max(tot_snow, n = 3)"
  },
  {
    "objectID": "solution-day2-analysis.html#working-through-logic",
    "href": "solution-day2-analysis.html#working-through-logic",
    "title": "Solutions Day 2",
    "section": "Working through logic",
    "text": "Working through logic\nGetting average monthly rain for each city.\nFirst get the total rain for each month/year:\n\ntx_mn_yr_rain &lt;- tx_clean |&gt; \n  filter(yr &gt;= 1940, yr &lt;= 2022) |&gt;\n  group_by(city, mn, yr) |&gt;\n  summarize(mn_yr_rain = sum(rain, na.rm = TRUE))\n\ntx_mn_yr_rain  \n\n\n\n  \n\n\n\nThen calculate the average for the months in each city:\n\ncity_avg_rain &lt;- tx_mn_yr_rain |&gt; \n  group_by(city, mn) |&gt;\n  summarise(avg_mn_rain = mean(mn_yr_rain))\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\ncity_avg_rain\n\n\n\n  \n\n\n\nAnd as a tease, we plot it:\n\ncity_avg_rain |&gt; \n  ggplot(aes(x = mn, y = avg_mn_rain, group = city)) +\n  geom_line(aes(color = city)) +\n  ylim(0,6) +\n  labs(\n    title = \"Average monthly rainfall, 1940-2022\",\n    x = \"\", y = \"Average monthly rain\",\n    color = \"City\"\n  )"
  },
  {
    "objectID": "solution-day2-analysis.html#challenge-earliest-100-day-each-city",
    "href": "solution-day2-analysis.html#challenge-earliest-100-day-each-city",
    "title": "Solutions Day 2",
    "section": "Challenge: Earliest 100+ day each city",
    "text": "Challenge: Earliest 100+ day each city\nFor each city, find the earliest day of a year in which it reached 100 degrees.\n\ntx_clean |&gt; \n  filter(tmax &gt;= 100) |&gt; \n  group_by(city) |&gt; \n  slice_min(yd) |&gt; \n  select(city, date, tmax)"
  },
  {
    "objectID": "slides/sd1-01-intro.html#install-r-rstudio",
    "href": "slides/sd1-01-intro.html#install-r-rstudio",
    "title": "Quarto, R & RStudio",
    "section": "Install R & RStudio",
    "text": "Install R & RStudio\nYou’ll need to go to https://posit.co/downloads/ and click on the big Download RStudio button there. There are two parts:\n\nInstall the R language\nInstall the RStudio Desktop program\n\nMake sure you run the installer or copy into your Applications folder as noted.\nThe R language gets installed on your machine, but you never launch it like a regular program."
  },
  {
    "objectID": "slides/sd1-01-intro.html#install-quarto",
    "href": "slides/sd1-01-intro.html#install-quarto",
    "title": "Quarto, R & RStudio",
    "section": "Install Quarto",
    "text": "Install Quarto\n\nGo to quarto.org/ and then click the Getting Started link.\nDownload and install the software for your machine."
  },
  {
    "objectID": "slides/sd1-01-intro.html#launch-rstudio",
    "href": "slides/sd1-01-intro.html#launch-rstudio",
    "title": "Quarto, R & RStudio",
    "section": "Launch RStudio",
    "text": "Launch RStudio\n\nFrom your Applications folder (or through your Start menu) launch the program called RStudio\n\n\nlauncher"
  },
  {
    "objectID": "slides/sd1-01-intro.html#about-rstudio-desktop",
    "href": "slides/sd1-01-intro.html#about-rstudio-desktop",
    "title": "Quarto, R & RStudio",
    "section": "About RStudio Desktop",
    "text": "About RStudio Desktop\nWe’ll orient a bit in class.\n\nRStudio Desktop"
  },
  {
    "objectID": "slides/sd1-01-intro.html#look-for-the-console",
    "href": "slides/sd1-01-intro.html#look-for-the-console",
    "title": "Quarto, R & RStudio",
    "section": "Look for the console",
    "text": "Look for the console\nWe need you to run some commands in the RStudio Console. We’ll provide code to copy/paste.\nInstall packages\nThis will take some time to run, depending on the internet.\ninstall.packages(c(\"usethis\", \"tidyverse\", \"quarto\", \"rmarkdown\", \"janitor\"))\nDownload, open the material\nAs you are asked questions, choose the default answers or ones that make the most sense.\nusethis::use_course(\"https://github.com/r-journalism/chjr/archive/master.zip\")"
  },
  {
    "objectID": "slides/sd1-01-intro.html#using-projects",
    "href": "slides/sd1-01-intro.html#using-projects",
    "title": "Quarto, R & RStudio",
    "section": "Using projects",
    "text": "Using projects\n\n\nR needs to know where things live on your computer so it can find them.\nWe use projects to organize our work. One folder for each project, with everything you need."
  },
  {
    "objectID": "slides/sd1-01-intro.html#open-our-first-lesson",
    "href": "slides/sd1-01-intro.html#open-our-first-lesson",
    "title": "Quarto, R & RStudio",
    "section": "Open our first lesson",
    "text": "Open our first lesson\n\nIn the Files pane, find our first file in lessons/d1-01-intro.qmd and open it.\nThis should open in your pane to the left, with the Console below it.\n\nWe’ll work from there now."
  },
  {
    "objectID": "slides/sd1-01-intro.html#tour-of-our-project",
    "href": "slides/sd1-01-intro.html#tour-of-our-project",
    "title": "Quarto, R & RStudio",
    "section": "Tour of our project",
    "text": "Tour of our project"
  },
  {
    "objectID": "resources/wx-testing.html",
    "href": "resources/wx-testing.html",
    "title": "Weather testing",
    "section": "",
    "text": "Here we look at all the weather data gathered here, checking the dates and such.\nThe data are Global Historical Climate Network Daily Summaries."
  },
  {
    "objectID": "resources/wx-testing.html#about-this-notebook",
    "href": "resources/wx-testing.html#about-this-notebook",
    "title": "Weather testing",
    "section": "",
    "text": "Here we look at all the weather data gathered here, checking the dates and such.\nThe data are Global Historical Climate Network Daily Summaries."
  },
  {
    "objectID": "resources/wx-testing.html#stations",
    "href": "resources/wx-testing.html#stations",
    "title": "Weather testing",
    "section": "Stations",
    "text": "Stations\nThese are the station identifiers:\n\nAR: USW00013963, USC00033466, USW00013964\nCA: USW00093134, USW00023271, USW00023272\nNC: USW00013881, USW00013748, USW00013722\nTX: USW00013960, USW00012918, USW00013958\nNY: USW00094728, USW00014735, USW00014733\n\nI really only need TX, CA & NC, I think.\n\nExtra column notes\n\nDAPR = Number of days included in the multiday precipitation total (MDPR)\nMDPR = Multiday precipitation total (mm or inches as per user preference; use with DAPR and DWPR, if available)\nDASF = Number of days included in the multiday snowfall total (MDSF)\nMDSF = Multiday snowfall total (mm or inches as per user preference)\nTOBS = Temperature at the time of observation (Fahrenheit or Celsius as per user preference)\nTAVG = Average of hourly values\n\nTesting the weather data\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "resources/wx-testing.html#import",
    "href": "resources/wx-testing.html#import",
    "title": "Weather testing",
    "section": "Import",
    "text": "Import\n\nca_raw &lt;- read_csv(\"../data-raw/ca.csv\") |&gt; clean_names()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 131099 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\nlgl  (2): DAPR, MDPR\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntx_raw &lt;- read_csv(\"../data-raw/tx.csv\") |&gt; clean_names()\n\nRows: 94503 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnc_raw &lt;- read_csv(\"../data-raw/nc.csv\") |&gt; clean_names()\n\nRows: 87427 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (6): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nny_raw &lt;- read_csv(\"../data-raw/ny.csv\") |&gt; clean_names()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 118885 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (6): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN\nlgl  (3): DASF, MDSF, TOBS\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nar_raw &lt;- read_csv(\"../data-raw/ar.csv\") |&gt; clean_names()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 106432 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\nlgl  (2): DAPR, MDPR\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "resources/wx-testing.html#bind-data",
    "href": "resources/wx-testing.html#bind-data",
    "title": "Weather testing",
    "section": "Bind data",
    "text": "Bind data\n\nwx_raw &lt;- ar_raw |&gt; \n  bind_rows(ca_raw, nc_raw, ny_raw, tx_raw)\n\nwx_raw |&gt; glimpse()\n\nRows: 538,346\nColumns: 14\n$ station &lt;chr&gt; \"USW00013964\", \"USW00013964\", \"USW00013964\", \"USW00013964\", \"U…\n$ name    &lt;chr&gt; \"FORT SMITH REGIONAL AIRPORT, AR US\", \"FORT SMITH REGIONAL AIR…\n$ date    &lt;date&gt; 1945-09-27, 1945-09-28, 1945-09-29, 1945-09-30, 1945-10-01, 1…\n$ dapr    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ mdpr    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ prcp    &lt;dbl&gt; 1.22, 0.73, 0.04, 0.02, 0.32, 0.00, 0.02, 0.29, 0.00, 0.00, 0.…\n$ snow    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ snwd    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 81, 75, 64, 69, 69, 77, 65, 60, 65, 77, 77, 70, 56, 67, 75, 81…\n$ tmin    &lt;dbl&gt; 65, 63, 57, 59, 61, 54, 53, 56, 57, 61, 58, 51, 46, 42, 44, 46…\n$ tobs    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ dasf    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ mdsf    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "resources/wx-testing.html#add-state",
    "href": "resources/wx-testing.html#add-state",
    "title": "Weather testing",
    "section": "Add state",
    "text": "Add state\nGet the state name from the station name.\n\nwx_states &lt;- wx_raw |&gt; \n  mutate(\n    state = str_sub(name, -5, -3)\n  )\n\nwx_states |&gt; glimpse()\n\nRows: 538,346\nColumns: 15\n$ station &lt;chr&gt; \"USW00013964\", \"USW00013964\", \"USW00013964\", \"USW00013964\", \"U…\n$ name    &lt;chr&gt; \"FORT SMITH REGIONAL AIRPORT, AR US\", \"FORT SMITH REGIONAL AIR…\n$ date    &lt;date&gt; 1945-09-27, 1945-09-28, 1945-09-29, 1945-09-30, 1945-10-01, 1…\n$ dapr    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ mdpr    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ prcp    &lt;dbl&gt; 1.22, 0.73, 0.04, 0.02, 0.32, 0.00, 0.02, 0.29, 0.00, 0.00, 0.…\n$ snow    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ snwd    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 81, 75, 64, 69, 69, 77, 65, 60, 65, 77, 77, 70, 56, 67, 75, 81…\n$ tmin    &lt;dbl&gt; 65, 63, 57, 59, 61, 54, 53, 56, 57, 61, 58, 51, 46, 42, 44, 46…\n$ tobs    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ dasf    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ mdsf    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ state   &lt;chr&gt; \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \",…"
  },
  {
    "objectID": "resources/wx-testing.html#recode-for-city",
    "href": "resources/wx-testing.html#recode-for-city",
    "title": "Weather testing",
    "section": "Recode for city",
    "text": "Recode for city\nGet the names:\n\nwx_states |&gt; \n  distinct(name)\n\n\n\n  \n\n\n\nRecode for city names\n\nwx_cities &lt;- wx_states |&gt; \n  mutate(city = recode(\n    name,\n    \"FORT SMITH REGIONAL AIRPORT, AR US\" = \"Fort Smith\",\n    \"LITTLE ROCK AIRPORT ADAMS FIELD, AR US\" = \"Little Rock\",\n    \"HOT SPRINGS 1 NNE, AR US\" = \"Hot Springs\",\n    \"SAN FRANCISCO DOWNTOWN, CA US\" = \"San Francisco\",\n    \"LOS ANGELES DOWNTOWN USC, CA US\" = \"Los Angeles\",\n    \"SACRAMENTO 5 ESE, CA US\" = \"Sacramento\",\n    \"CHARLOTTE DOUGLAS AIRPORT, NC US\" = \"Charlotte\",\n    \"WILMINGTON INTERNATIONAL AIRPORT, NC US\" = \"Wilmington\",\n    \"RALEIGH DURHAM INTERNATIONAL AIRPORT, NC US\" = \"Raleigh\",\n    \"NY CITY CENTRAL PARK, NY US\" = \"New York City\",\n    \"BUFFALO NIAGARA INTERNATIONAL, NY US\" = \"Buffalo\",\n    \"ALBANY INTERNATIONAL AIRPORT, NY US\" = \"Albany\",\n    \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\" = \"Houston\",\n    \"AUSTIN CAMP MABRY, TX US\" = \"Austin\",\n    \"DALLAS FAA AIRPORT, TX US\" = \"Dallas\",\n  ))\n\nwx_cities |&gt; distinct(name, city)"
  },
  {
    "objectID": "resources/wx-testing.html#select-cols",
    "href": "resources/wx-testing.html#select-cols",
    "title": "Weather testing",
    "section": "Select cols",
    "text": "Select cols\n\nwx_tight &lt;- wx_cities |&gt; \n  select(\n    name,\n    city,\n    state,\n    date,\n    prcp,\n    snow,\n    snwd,\n    tmax,\n    tmin\n  )\n\nwx_tight |&gt; glimpse()\n\nRows: 538,346\nColumns: 9\n$ name  &lt;chr&gt; \"FORT SMITH REGIONAL AIRPORT, AR US\", \"FORT SMITH REGIONAL AIRPO…\n$ city  &lt;chr&gt; \"Fort Smith\", \"Fort Smith\", \"Fort Smith\", \"Fort Smith\", \"Fort Sm…\n$ state &lt;chr&gt; \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"AR \", \"…\n$ date  &lt;date&gt; 1945-09-27, 1945-09-28, 1945-09-29, 1945-09-30, 1945-10-01, 194…\n$ prcp  &lt;dbl&gt; 1.22, 0.73, 0.04, 0.02, 0.32, 0.00, 0.02, 0.29, 0.00, 0.00, 0.00…\n$ snow  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ snwd  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ tmax  &lt;dbl&gt; 81, 75, 64, 69, 69, 77, 65, 60, 65, 77, 77, 70, 56, 67, 75, 81, …\n$ tmin  &lt;dbl&gt; 65, 63, 57, 59, 61, 54, 53, 56, 57, 61, 58, 51, 46, 42, 44, 46, …"
  },
  {
    "objectID": "resources/wx-testing.html#check-the-dates",
    "href": "resources/wx-testing.html#check-the-dates",
    "title": "Weather testing",
    "section": "Check the dates",
    "text": "Check the dates\n\nwx_tight |&gt; \n  group_by(city, state) |&gt; \n  summarise(\n    f_date = min(date),\n    l_date = max(date),\n    cnt = n()\n  ) |&gt; \n  arrange(state, f_date)\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "resources/leftovers.html",
    "href": "resources/leftovers.html",
    "title": "Leftovers",
    "section": "",
    "text": "Some thing we probably don’t have time for but might be useful."
  },
  {
    "objectID": "resources/leftovers.html#about-r-chunks",
    "href": "resources/leftovers.html#about-r-chunks",
    "title": "Leftovers",
    "section": "About R chunks",
    "text": "About R chunks\nWhile you used the R console above to interact with R (yes, that is what you were doing), we will mostly add code to our notebooks using code chunks.\n\nIn this RStudio notebook in the code chunk below, click on the green play button to run the code in the chunk.\nYou should get a response something like [1] 2 which is a row number [1] and the answer to the match equation 2.\n\n\n1 + 1\n\n[1] 2\n\n\nThere is a keyboard command to run a line of code, and it is among five key commands we really suggest you use. Presented for Mac:\n\nCmd-return runs a single line of code that your cursor is on.\nCmd-shift-return runs all the lines in the code chunk where your cursor is.\n\nAll of these options are also in the Run menu at the top of the document.\n\nOYO: Insert a code chunk\nThe best way to insert a new code chunk is to use the keyboard command Cmd-option-i. That said, there is also a green +c button above the document near that Run menu.\n\nIn the space below this direction but before the next headline, add a new code chunk using the keyboard command Cmd-option-i.\n\nNotice it adds all the characters needed and then puts your cursor inside so you are ready to type.\n\nInside that code chunk, add 2 + 2 and then run the chunk to see the answer.\n\n\n\nData frames and plots\nThere is a sample data set in R called cars that has the stopping distance of cars as measured some time in history. We’ll use it to show a couple more things about code blocks.\nThe code chunk below has two lines of code:\n\nThe first just says cars, which prints out the data.\nThe second line is a function called plot() with cars inside of it. That function acts on the cars data to create a scatter plot.\n\n\nRun the code below.\n\n\ncars\n\n\n\n  \n\n\nplot(cars)\n\n\n\n\nSince we have two lines with two different outputs, the Quarto notebook shows them as panes you can swap back and forth. The rendered page shows each line of code, then its output.\n\nThe data.frame part is printing out data. There is a lot of information available in that printout, including the number of rows (we call them observations) and columns (we call them variables), the names of those columns/variables, the data types of each variable, and then their values. We call data like this a “data frame” or “tibble”. It’s like a highly structured spreadsheet.\nThe second pane is a chart (or plot as we call them). The function plot() took the only two variables in the data and, well, plotted them.\n\nYes, it can be that easy to visualize data in R. But not usually.\n\n\nRender your first document\n\nWith this document open in R Studio, look at the toolbar directly above the writing and look for the Render button with the blue arrow next to it. Click on that word.\nThis will create (or render) a “publishable” version of this document in HTML and present it in the Viewer page (or perhaps in a new window, which we can fix).\n\nYou can read this lesson text there, but realize that rendered page doesn’t update with edits in the Quarto document unless you tell it to.\nKeep this in mind:\n\nThere are keyboard commands to render a document: Cmd-shift-k.\nWe can also easily publish the same work in other formats, like PDF, Word or even a slide show.\nWe can also choose NOT to “publish” our work. We don’t have to share our work on the internet, we are just ready if we want to.\n\n\n\nOYO: Re-render this page one last time\nNow that you’ve been introduced to how a Quarto document is written, go ahead and re-render this page by using the ."
  },
  {
    "objectID": "resources/homework.html",
    "href": "resources/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Go through the Managing a project page and video\nCreate a new quarto website\n\nYou can just use the index.qmd file for this exercise\n\nImport our Skittles data (link TK) using read_csv()\nFind the answers to the following questions:\n\nWho got the most candy? Who the least?\nWhat is the average number of candies per bag?\nOn average, which color candy is most prevalent?"
  },
  {
    "objectID": "lesson-day4-01-visualizing.html",
    "href": "lesson-day4-01-visualizing.html",
    "title": "Visualizing data",
    "section": "",
    "text": "Render, dammit!\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n\n\n\n\n\n\nggplot2 is based on the grammar of graphics, the idea that you can build every graph from the same components: a data set, a coordinate system, and geoms—visual marks that represent data points\n\n\n\n\n\n\nTo display values, map variables in the data to visual properties of the geom (aesthetics) like size, color, and x and y locations.\n\n\n\n\n\n\nYou only need two lines of code, really.\nThe rest is just extra customization.\n\n\n\n\n\n\n\nburgers &lt;- data.frame(employee=c(\"Bob\", \"Gene\",\n                              \"Linda\", \"Louise\",\n                              \"Tina\"),\n                   time=c(25, 30, 60, 20, 50),\n                   age=c(42, 11, 39, 9, 13),\n                   interest=c(\"cooking\", \"music\", \"wine\", \"chaos\", \"horses\"))\n\nburgers\n\n\n\n  \n\n\nburgers &lt;- burgers |&gt; \n  mutate(where=\"front\")\n\nburgers\n\n\n\n  \n\n\n\n\n\nburgers\n\n\n\n  \n\n\nggplot(burgers, aes(x=where, y=time, fill=employee)) + \n  geom_col(position=\"stack\") \n\n\n\n\n\n\nburgers\n\n\n\n  \n\n\nggplot(burgers, aes(x=employee, y=time, fill=employee)) + \n  geom_col(position=\"stack\")\n\n\n\n\n\n\nLayers\n\n\n\n\n\n\n\nAesthetics\n\n\n\n\n\n\n  ggplot(burgers) + \n    geom_point(aes(x=employee, y=time))\n\n\n\n\n\n  ggplot(burgers) + \n    geom_point(aes(x=employee, y=time, fill=employee))\n\n\n\n\n\n  ggplot(burgers) + \n    geom_point(aes(x=employee, y=time, fill=employee, color=employee)) \n\n\n\n\n\n  ggplot(burgers) + \n    geom_point(aes(x=employee, y=time, fill=employee, color=employee, size=age))  \n\n\n\n\n\n\ngeoms\nMany more geom types for different visualizations.\n\n\n\n\n\n\n\nYour turn\npractice-day4-visualizing.qmd\n\n  ggplot(burgers) + \n    geom_col(aes(x=employee, y=time), stat=\"identity\") \n\n\n\n\n\n  ggplot(burgers) + \n    geom_col(aes(x=time, y=employee), stat=\"identity\") \n\n\n\n\n\ndisney &lt;- read_csv(\"data-raw/disney_movies_total_gross.csv\")\n\nRows: 579 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): movie_title, release_date, genre, MPAA_rating, total_gross, inflati...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(disney)\n\nRows: 579\nColumns: 6\n$ movie_title              &lt;chr&gt; \"Snow White and the Seven Dwarfs\", \"Pinocchio…\n$ release_date             &lt;chr&gt; \"Dec 21, 1937\", \"Feb 9, 1940\", \"Nov 13, 1940\"…\n$ genre                    &lt;chr&gt; \"Musical\", \"Adventure\", \"Musical\", \"Adventure…\n$ MPAA_rating              &lt;chr&gt; \"G\", \"G\", \"G\", \"G\", \"G\", NA, \"G\", NA, \"G\", NA…\n$ total_gross              &lt;chr&gt; \"$184,925,485\", \"$84,300,000\", \"$83,320,000\",…\n$ inflation_adjusted_gross &lt;chr&gt; \"$5,228,953,251\", \"$2,188,229,052\", \"$2,187,0…\n\n\n\ndisney &lt;- disney |&gt; \n  mutate(release_date=mdy(release_date),\n         total_gross=parse_number(total_gross),\n         inflation_adjusted_gross=parse_number(inflation_adjusted_gross)) \n\nglimpse(disney)\n\nRows: 579\nColumns: 6\n$ movie_title              &lt;chr&gt; \"Snow White and the Seven Dwarfs\", \"Pinocchio…\n$ release_date             &lt;date&gt; 1937-12-21, 1940-02-09, 1940-11-13, 1946-11-…\n$ genre                    &lt;chr&gt; \"Musical\", \"Adventure\", \"Musical\", \"Adventure…\n$ MPAA_rating              &lt;chr&gt; \"G\", \"G\", \"G\", \"G\", \"G\", NA, \"G\", NA, \"G\", NA…\n$ total_gross              &lt;dbl&gt; 184925485, 84300000, 83320000, 65000000, 8500…\n$ inflation_adjusted_gross &lt;dbl&gt; 5228953251, 2188229052, 2187090808, 107851057…\n\n\n\ndisney |&gt; \n  ggplot(aes(x=release_date, \n             y=inflation_adjusted_gross,\n             color=genre)) +\n  geom_point() \n\n\n\n\n\n\nlayers\n\n\n\n\n\n\n\ndisney |&gt; \n  ggplot(aes(x=release_date, \n             y=inflation_adjusted_gross, \n             color=genre)) +\n  geom_point() +\n  scale_x_date(limits=c(ymd(\"1980-01-01\"), ymd(\"2020-01-01\")))\n\n\n\n\n\n\ndisney |&gt; \n  ggplot(aes(x=release_date, \n             y=inflation_adjusted_gross, \n             color=genre)) +\n  geom_point() +\n  scale_x_date(limits=c(ymd(\"1980-01-01\"), \n                              ymd(\"2020-01-01\"))) +\n  scale_y_continuous(limits=c(0, 1000000000)) \n\n\n\n\n\n\ndisney |&gt; \n  ggplot(aes(x=release_date, \n             y=inflation_adjusted_gross, \n             color=genre)) +\n  geom_point() +\n  scale_x_date(limits=c(ymd(\"1980-01-01\"), \n                              ymd(\"2020-01-01\")),\n                     labels=scales::date_format(\"%Y\")) +\n  scale_y_continuous(limits=c(0, 1000000000),\n                     labels=scales::dollar_format()) \n\n\n\n\n\n\n\nYour turn\npractice-day4-visualizing.qmd\nGo through Exercise 2!\n\n\n\nfacets\n(also known as small multiples)\n\n\ndisney |&gt; \n  ggplot(aes(x=release_date, \n             y=inflation_adjusted_gross)) +\n  geom_point() +\n  scale_x_date(limits=c(ymd(\"1980-01-01\"), \n                              ymd(\"2020-01-01\")),\n                     labels=scales::date_format(\"%Y\")) +\n  scale_y_continuous(limits=c(0, 1000000000),\n                     labels=scales::dollar_format()) +\n  facet_wrap(~genre)\n\n\n\n\n\n\n\nYour turn\n4_a_viz\npractice-day4-visualizing.qmd\nGo through the two exercises in the Facets section.\n\n\nlabels\n\ndisney |&gt; \n  ggplot(aes(x=release_date, \n             y=inflation_adjusted_gross)) +\n  geom_point() +\n  scale_x_date(limits=c(ymd(\"1980-01-01\"), \n                              ymd(\"2020-01-01\")),\n                     labels=scales::date_format(\"%Y\")) +\n  scale_y_continuous(limits=c(0, 1000000000),\n                     labels=scales::dollar_format()) +\n  facet_wrap(~genre) +\n  labs(title=\"Disney animated movie gross profit\") +\n  labs(subtitle=\"Adjusted for inflation\") +\n  labs(y=\"\", x=\"\") +\n  labs(caption=\"Data: Source Goes Here\")\n\n\n\n\n\n\nYour turn\npractice-day4-visualizing.qmd\nGo through the Labels section.\n\n\nthemes\n\ndisney |&gt; \n  ggplot(aes(x=release_date, \n             y=inflation_adjusted_gross)) +\n  geom_point() +\n  scale_x_date(limits=c(ymd(\"1980-01-01\"), \n                              ymd(\"2020-01-01\")),\n                     labels=scales::date_format(\"%Y\")) +\n  scale_y_continuous(limits=c(0, 1000000000),\n                     labels=scales::dollar_format()) +\n  facet_wrap(~genre) +\n  labs(title=\"Disney animated movie gross profit\") +\n  labs(subtitle=\"Adjusted for inflation\") +\n  labs(y=\"\", x=\"\") +\n  labs(caption=\"Data: Source Goes Here\") +\n  theme(strip.background = element_rect(colour = \"black\", fill = \"white\")) +\n  theme(legend.key = element_rect(fill = \"white\", colour = \"black\")) +\n  theme_minimal()\n\n\n\n\n\n\nYour turn\npractice-day4-visualizing.qmd\nGo through the Themes section."
  },
  {
    "objectID": "lesson-day2-01-analysis.html",
    "href": "lesson-day2-01-analysis.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "For this lesson we’ll learn about data wrangling functions, many of them from dplyr. These are the functions that are much like you do in spreadsheets, like filtering, pivoting and the like. Most of these skills are necessary to prepare data before you make charts.\nFunctions we will learn about include: arrange(), filter(), slice(), group_by() and summarize().\nWe’ll use these to find several findings from our data, including:\n\nThe coldest and warmest days\nThe rainiest and snowiest days\nYears with most snow days\nYears with most 100+ days\nYears with most rain\nThe earliest day to reach 100+ each year\n\nTo perhaps avoid some confusion we’ll use just the Texas data for this lesson. I’ll still be giving you your own mini-quests.\n(You theoretically could use a your state, but you would need to adjust your code to import that data, use valid cities, etc.)"
  },
  {
    "objectID": "lesson-day2-01-analysis.html#goals-of-this-lesson",
    "href": "lesson-day2-01-analysis.html#goals-of-this-lesson",
    "title": "Data Wrangling",
    "section": "",
    "text": "For this lesson we’ll learn about data wrangling functions, many of them from dplyr. These are the functions that are much like you do in spreadsheets, like filtering, pivoting and the like. Most of these skills are necessary to prepare data before you make charts.\nFunctions we will learn about include: arrange(), filter(), slice(), group_by() and summarize().\nWe’ll use these to find several findings from our data, including:\n\nThe coldest and warmest days\nThe rainiest and snowiest days\nYears with most snow days\nYears with most 100+ days\nYears with most rain\nThe earliest day to reach 100+ each year\n\nTo perhaps avoid some confusion we’ll use just the Texas data for this lesson. I’ll still be giving you your own mini-quests.\n(You theoretically could use a your state, but you would need to adjust your code to import that data, use valid cities, etc.)"
  },
  {
    "objectID": "lesson-day2-01-analysis.html#clean-up-our-workspace",
    "href": "lesson-day2-01-analysis.html#clean-up-our-workspace",
    "title": "Data Wrangling",
    "section": "Clean up our workspace",
    "text": "Clean up our workspace\nAgain I have a practice notebook ready for you to fill in to keep you on track. Let’s open the file and clean up our environment before we get going.\n\nOpen your chjr23 project if it isn’t already.\nOpen the file practice-day1-analysis.qmd.\nUnder the Run menu at the top of the page, choose “Restart R and Clear Output”.\nCheck your Environment tab. If there is anything listed there, click on the broom icon to clear it out.\n\nWe do this so we don’t have any leftovers from our previous notebook. Each notebook should run independently. That’s also necessary to Render pages within a project."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#add-your-setup-chunk",
    "href": "lesson-day2-01-analysis.html#add-your-setup-chunk",
    "title": "Data Wrangling",
    "section": "Add your setup chunk",
    "text": "Add your setup chunk\nI’m going to include the whole setup chunk code here again so you get the execution options.\n\n```{r}\n#| label: setup\n#| message: false\n\nlibrary(tidyverse)\n```\n\nWe only need the tidyverse library for this notebook."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#import-our-cleaned-data",
    "href": "lesson-day2-01-analysis.html#import-our-cleaned-data",
    "title": "Data Wrangling",
    "section": "Import our cleaned data",
    "text": "Import our cleaned data\nNow to reap the benefit our the hard work from the last lesson, let’s import our cleaned data.\n\nIn the import section of the notebook, add a code chunk.\nAdd the read_rds() function below and fill out the path to your cleaned data, as indicated and run it to make sure it works.\nSave that data into a new object called tx_clean and run it again. Make sure it shows up in your Environment tab.\n\n\ntx_clean &lt;- read_rds(\"data-processed/tx_clean.rds\")"
  },
  {
    "objectID": "lesson-day2-01-analysis.html#arrange",
    "href": "lesson-day2-01-analysis.html#arrange",
    "title": "Data Wrangling",
    "section": "Arrange",
    "text": "Arrange\nThe arrange() function is what we use to sort our data. We can use this function to find a couple of answers were are looking for, like what are the hottest and coldest days in our data.\nThe function is pretty simple, just feed it the data (usually from a pipe) and then add the column you want to sort on. By default it is in “ascending” order: low to high or alphabetically. If you want the opposite (and journalists usually do) you have to wrap the column in another function, desc().\n\nIn the arrange section, add a code chunk for the “Coldest day” section.\nStart with your data, the pipe into arrange() and add the tmin() column.\nRun the chunk so you can see the result. Note the data is ordered by the tmin column, but it’s hard to see. Let’s add a select() function to focus on what we care about.\nPipe into select() adding the city, date and tmin columns.\n\n\ntx_clean |&gt; \n  arrange(tmin) |&gt; \n  select(city, date, tmin)\n\n\n\n  \n\n\n\nGlad I was not in Austin in 1949. Now to find the hottest days.\n\nAfter the “Hottest day” header add a new code chunk.\nUse arrange to find the highest tmax value. Run the chunk to make sure it worked.\nUse select to clean up the fields.\n\n\ntx_clean |&gt;\n  arrange(desc(tmax)) |&gt; \n  select(city, date, tmax)\n\n\n\n  \n\n\n\nUgh, I was in Austin on both of those days in 2000 and 2011.\n\nOYO: Most rain\nUsing the same tools, find:\n\nThe days with the most rain\nThe days with the most snow"
  },
  {
    "objectID": "lesson-day2-01-analysis.html#filter",
    "href": "lesson-day2-01-analysis.html#filter",
    "title": "Data Wrangling",
    "section": "Filter",
    "text": "Filter\nWe use the filter() function when we want to specify rows based on a condition. This is the equivalent of clicking on the Filter tool in Excel and choosing values to keep or exclude, but we do it with code that can be fixed and repeated.\nWe’ll use filter as one function of many for some of our answers, like which years had the most 100+ degree days. We need to work on the filtering skill here first, as there are nuances.\nThe function works like this:\n# this is psuedo code. don't add it\ndata |&gt; \n  filter(variable comparison value)\n\n# example\ntx_clean |&gt; \n  filter(city == \"Austin\")\nThe filter() function typically works in this order:\n\nWhat is the variable (or column) you are searching in.\nWhat is the comparison you want to do. Equal to? Greater than?\nWhat is the observation (or value in the data) you are looking for?\n\nNote the two equals signs == in our Austin example above. It is important to use two of them when you are asking if a value is “true” or “equal to”, as a single = typically means you are assigning a value to something.\n\nComparisons: Logical tests\nThere are a number of these logical tests for the comparison:\n\n\n\nOperator\nDefinition\n\n\n\n\nx &lt; y\nLess than\n\n\nx &gt; y\nGreater than\n\n\nx == y\nEqual to\n\n\nx &lt;= y\nLess than or equal to\n\n\nx &gt;= y\nGreater than or equal to\n\n\nx != y\nNot equal to\n\n\nx %in% c(y,z)\nIn a group\n\n\nis.na(x)\nIs NA (missing values)\n\n\n!is.na(x)\nIs not NA\n\n\n\nWhere you apply a filter matters. If we want to consider only certain data before other operations, then we need to do the filter first. In other cases we may filter after all our calculations to show just rows of interest.\n\n\nSingle condition\nLet’s find days that are 100+.\n\nIn the ## Filter section in the part about 100+ days, add a code chunk.\nStart with the data, then pipe into filter().\nFor the condition, look in the tmax column using &gt;= to find values “greater or equal to” 100.\nRun the code to make sure it works.\nUse select() to focuse on the variables of interest.\n\n\ntx_clean |&gt; \n  filter(tmax &gt;= 100) |&gt; \n  select(city, date, tmax)\n\n\n\n  \n\n\n\n\n\nMultiple “and” conditions\nOK, that’s fine, but our list is not long enough to see the days in Dallas. Let’s do this again, but add a second condition to test. When you use a comma , or ampersand & between conditions, both conditions must be true to keep the rows.\n\nIn the Dallas 100+ section, start a new code chunk.\nDo the same code as above and run it to make sure it still works.\nUse a comma after the first condition to add a second one: city == \"Dallas\".\n\n\ntx_clean |&gt; \n  filter(tmax &gt;= 100, city == \"Dallas\") |&gt; \n  select(city, date, tmax)\n\n\n\n  \n\n\n\n\n\nMultiple “or” conditions\nBut what if we have an “either or” case where one of two conditions could be true. This would be the case if we wanted to find days where it either a) snowed that day, or b) there was snow left on the ground from a previous day. This is a true snow day, right?\nYou can use the | operator for an “or” condition. That character is the shift of the \\ key just above your return/enter key.\n\nIn the section about snow days, add a chunk.\nAdd the code, but run after adding the first condition, before you add the second condition, so you can compare them when you are done.\nAdd the second condition and run it again.\nAdd the select to focus on our columns of interest.\n\n\ntx_clean |&gt; \n  filter(snow &gt; 0 | snwd &gt; 0) |&gt; \n  select(city, date, snow, snwd)\n\n\n\n  \n\n\n\n\n\nBut I need “and” and “or”\nYou can mix “and” and “or” conditions, but note the order of them might matter depending on what you are doing.\n\n\nOYO: Real snow days in Dallas\nIn your notebook, start with the snow days we had above, but add a new condition to the filter that filters city to just “Dallas”.\n\n\nFiltering text\nWe don’t really have a need for this in our data, but we can use filter to find parts of words as well. There are many ways, but the one I use the most is str_detect().\n\ntx_clean |&gt; \n1  filter(str_detect(city, \"st\")) |&gt;\n2  distinct(city)\n\n\n1\n\nInside the filter, we start with str_detect(). The first argument it needs is which column in our data to look at, so we fed it city. The second argument (after a comma) is the string of text we are looking for in the column, which is st in our case.\n\n2\n\nI used distinct(city) here so we could more easily see our results.\n\n\n\n\n\n\n  \n\n\n\nThe code above found both “Houston” and “Austin” because they have “st” in them. It didn’t capture “Dallas”."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#slice",
    "href": "lesson-day2-01-analysis.html#slice",
    "title": "Data Wrangling",
    "section": "Slice",
    "text": "Slice\nAnother way to pick out specific rows of data based on a condition is to use slice variables like slice_max(), slice_min().\nLet’s use slice_min() to find the coldest day in our data.\n\nIn the slice section of the notebook about coldest day, add the following:\n\n\ntx_clean |&gt; \n  slice_min(tmin) |&gt; \n  select(city, date, tmin)\n\n\n\n  \n\n\n\nWe get one result, the coldest day in the data. But what if we want the coldest day for each city? We will introduce group_by() to solve that."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#group-by",
    "href": "lesson-day2-01-analysis.html#group-by",
    "title": "Data Wrangling",
    "section": "Group by",
    "text": "Group by\nThe group_by() function goes behind the scenes to organize your data into groups, and any function that follows it gets executed within those groups.\nThe columns you feed into group_by() determine the groups. If we do group_by(city) then all the “Austin” rows are grouped together, then all the “Dallas” rows, then all the “Houston” rows.\nI sometimes think of these groups as piles of data, separate from each other. We would have three piles of data, one for each city. Functions that follow happen independently on each pile."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#group-and-slice",
    "href": "lesson-day2-01-analysis.html#group-and-slice",
    "title": "Data Wrangling",
    "section": "Group and slice",
    "text": "Group and slice\nIf we add our group_by(city) before slice, then it works within each group. Like this:\n\ntx_clean |&gt; \n  group_by(city) |&gt; \n  slice_min(tmin) |&gt; \n  select(city, date, tmin)\n\n\n\n  \n\n\n\nLook at the difference in this result. Now we get a result for each city, because the rows we “grouped” the data before performing the slice. Since there are three cities, we get three results."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#multiple-groupings",
    "href": "lesson-day2-01-analysis.html#multiple-groupings",
    "title": "Data Wrangling",
    "section": "Multiple groupings",
    "text": "Multiple groupings\nWe can also group by multiple columns. What that does is create a group (or pile!) of data for each matching combination of values.\nSo, if we group_by(city, yr) then we will get a pile for each year of Austin (85 piles because there are 85 years of data for Austin), then a pile for each year in Houston, etc.\nIf we were to find the hottest day in each of those piles, it would look like this:\n\nCreate a new chunk and add this to your notebook using copy-to-clipboard.\nTry it with and without the distinct() at the end and think about why you got those results.\n\n\ntx_clean |&gt; \n  group_by(yr, city) |&gt; \n  slice_max(tmax) |&gt; \n  select(city, tmax) |&gt; \n  distinct()\n\nAdding missing grouping variables: `yr`\n\n\n\n\n  \n\n\n\nI added distinct the distinct() there to remove some ties where there were multiple days in a year with that same high temperature."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#summarize",
    "href": "lesson-day2-01-analysis.html#summarize",
    "title": "Data Wrangling",
    "section": "Summarize",
    "text": "Summarize\nWhile slice is nice, we really went through this exercise to understand group_by() so we can use it with summarize(), which allows us to summarize data much like a pivot table in Excel.\n\n\n\n\n\n\nTip\n\n\n\nsummarise() and summarize() are the same function. The creator of tidyverse is from New Zealand so he has both spellings. I tend to use them both by whim, though the “s” version comes first in type-assist.\n\n\nIf there are no group_by variables, the output of summarize() will be a single row summarizing all observations in the input. If we have groups, it will contain one column for each grouping variable and one column for each summary statistic we specify. Much easier to understand if we do it.\nLet’s first do one without groups.\n\nIn the Summarize section, add the following chunk and code.\n\n\ntx_clean |&gt; \n  summarize(\n    e_date = min(date),\n    l_date = max(date),\n    cnt = n()\n  )\n\n\n\n  \n\n\n\nWe have no groups here, so we just get the stats we specified … the earliest date in our data, the latest date in our data and the number of rows.\nIt is important to name our summarized columns like the e_date = part above. If we don’t, the function becomes a new column name it is hard to work with later. We can name the summarized column anything we want, but choose something that describes the column.\n\nAdd city as a group\n\nJust use the copy-to-clipboard tool to add this to your notebook and run it to see it.\n\n\ntx_clean |&gt; \n1  group_by(city) |&gt;\n  summarise(\n    e_date = min(date),\n    l_date = max(date),\n    cnt = n()\n  )\n\n\n1\n\nThis is where we add the group.\n\n\n\n\n\n\n  \n\n\n\nNow that we’ve added a group, we get a result for each unique value within that column. We have three cities, so we get three results.\n\n\nAdd both city and yr as a group\nIf we group by multiple things, then again we’ll get results for each unique combination between the two variables.\n\nJust use the copy-to-clipboard tool to add this to your notebook and run it to see it.\n\n\ntx_clean |&gt; \n1  group_by(city, yr) |&gt;\n  summarise(\n    e_date = min(date),\n    l_date = max(date),\n    cnt = n()\n  )\n\n\n1\n\nThis is where we add the second group to get both city and yr.\n\n\n\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nNow we get a result for each year within each city.\nYou’ll also see this message about “summarise() has grouped output by ‘city’.” That is not an error, it’s just telling you that your data will remain grouped by city if you add more functions. It’s a rabbit hole we won’t jump down today, but know it is not a problem.\n\n\nCommon summarize stats\nThere are a number of statistics we can find about our data within summarize.\n\nn() counts the number of rows\nn_distinct() counts the number of distinct values in the column\nmin() gives the smallest value\nmax() gives the largest value\n\nSome math operators might need the argument na.rm = TRUE to ignore NA (empty) values.\n\nsum() adds values together.\nmean() gives the mean (or average) value.\nmedian() gives the median or middle value\n\nThere are other useful ones in the summarise documentation."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#group-and-summarize-count",
    "href": "lesson-day2-01-analysis.html#group-and-summarize-count",
    "title": "Data Wrangling",
    "section": "Group and summarize: Count",
    "text": "Group and summarize: Count\nA very typical workflow to answer a data-driven question is to count records. Often the logic is to:\n\nDo we need to consider all the data or just some of it?\nGroup the records by variables of interest, like categories or dates.\nCount the number of records in each group.\n\nWe’ll use these general steps to answer this question: What years have had the most 100+ degree days. We’ll start with Austin with careful consideration, then generally show how do it for all the cities.\nHere are the steps I used to do this, along with my thought process. In some cases I’m adding code to check something and then deleting it before moving in, but the code below just shows the end result.\n\nI started with a new code chunk with the tx_clean data.\nI then filtered it to Austin and ran it to check it. I saved that result into a new object, atx.\nI added a line that used summary() to check the dates of the atx data. It looks like the data starts in June 1938 and there could have been days before that that were 100, so I amended my filter to cut out 1938. The latest date is Sept. 30. 2023. Since this is our current year and there are typically few 100+ days in October, I’ll keep this year, but I’ll note it if I use this in a chart.\nI glimpse it again just for convenience to see the column names.\n\nHere is the code:\n\n# get Austin data\natx &lt;- tx_clean |&gt; filter(city == \"Austin\", yr &gt; 1938)\n\n# check dates\natx$date |&gt; summary()\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"1939-01-01\" \"1960-03-09\" \"1981-05-16\" \"1981-05-16\" \"2002-07-23\" \"2023-09-30\" \n\n# peek\natx |&gt; glimpse()\n\nRows: 30,954\nColumns: 10\n$ city &lt;chr&gt; \"Austin\", \"Austin\", \"Austin\", \"Austin\", \"Austin\", \"Austin\", \"Aust…\n$ date &lt;date&gt; 1939-01-01, 1939-01-02, 1939-01-03, 1939-01-04, 1939-01-05, 1939…\n$ rain &lt;dbl&gt; 0.00, 0.00, 0.01, 0.14, 0.00, 0.00, 0.00, 0.12, 0.41, 0.00, 0.34,…\n$ snow &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ snwd &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ tmax &lt;dbl&gt; 69, 73, 74, 73, 70, 72, 72, 69, 73, 66, 58, 49, 54, 58, 53, 55, 6…\n$ tmin &lt;dbl&gt; 32, 37, 56, 49, 41, 39, 52, 54, 53, 48, 46, 43, 35, 42, 34, 29, 4…\n$ yr   &lt;dbl&gt; 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939, 1939,…\n$ mn   &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, …\n$ yd   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n\n\nWith my prepped data I can now do my calculations:\n\nI start with the atx data and filter to get days where tmax was 100+. I Run the chunk to check it.\nThen I grouped the records by yr.\nThen I summarize to get the count using n(), but with a descriptive column name, hot_days.\nThen I arranged the results to show the most hot_days at the top.\nThen I added a filter to cut off the list at a logical place, years with 50 days or more.\n\n\natx |&gt; \n  filter(tmax &gt;= 100) |&gt; \n  group_by(yr) |&gt; \n  summarize(hot_days = n()) |&gt; \n  arrange(desc(hot_days)) |&gt; \n  filter(hot_days &gt;= 50)\n\n\n\n  \n\n\n\nHere is what that looks like if I try to do it with all the cities. The difference here is I’m not taking as much care with the dates so there might be partial years that are undercounted, but they should at least be lower.\n\ntx_clean |&gt; \n  filter(tmax &gt;= 100) |&gt; \n  group_by(yr, city) |&gt; \n  summarize(hot_days = n()) |&gt; \n  arrange(desc(hot_days)) |&gt; \n  filter(hot_days &gt;= 30)\n\n`summarise()` has grouped output by 'yr'. You can override using the `.groups`\nargument.\n\n\n\n\n  \n\n\n\nApparently the Houston Hobby Airport just doesn’t have that many 100+ degree days compared to Austin and Dallas. That humidity, though …\n\nOYO: Most snow days by city each year\nIf there is time, you could use a similar method to count the number of days with with snow."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#group-and-summarize-math",
    "href": "lesson-day2-01-analysis.html#group-and-summarize-math",
    "title": "Data Wrangling",
    "section": "Group and Summarize: Math",
    "text": "Group and Summarize: Math\nThe next question we want to answer: In each city, which years had the most rain and which had the least? Let’s walk through our logic questions again:\nDo we need to consider all the data?\nIn this case we definitely want only whole years. When we tried summarize above we calculated the first date of all the data, and looking at that we’ll have to start with 1940. We’ll also lop off 2023.\nDo we need to consider our data in any groups?\nWe need to add together values in each city and year, so Austin for 1940, then ’41, etc. We’ll group our data by city and yr.\nWhat do we need to calculate?\nWe need to add together the inches of rain, so we can sum() the rain column.\nWork through this in your practice notebook step by step:\n\nIn the “Group and Summarize: Math” section, add a code chunk.\nStart with our data, then filter it to start after 1939 and before 2023. Run the chunk to make sure it works. You might even pipe into a slice_max(date) to test if it worked (but then remove it after you have checked.)\nGroup your data by city and yr.\nSummarize your data using sum() on the rain column. You probably have to add an argument na.rm = TRUE to make this work because some days have blank values for rain and sum() doesn’t know what to do with those blank values. Run it!\nArrange the values first by city, then by the summed rain in descending order. Run it!\nSave your result into an object and then print it back out so you can see it. Run it!\n\n\ntx_yr_rain &lt;- tx_clean |&gt; \n  filter(yr &gt; 1939, yr &lt; 2023) |&gt;\n  group_by(city, yr) |&gt; \n  summarise(tot_rain = sum(rain, na.rm = TRUE)) |&gt; \n  arrange(city, desc(tot_rain))\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\ntx_yr_rain\n\n\n\n  \n\n\n\nAt this point we have all the values, through they are hard to read. Some paths we could take to get more clarity:\n\nWe could take our new object and build new blocks to filter by city and arrange by most or least rain. Six new code chunks. Very clear.\nWe could maybe use group and slice_max to find highest values within each city, and then again with slice_min. There is an option to set the number of returns.\nWe could plot this on a chart (a lesson for another day!).\n\nHere is the group and slice method where I use the n = argument to set how many records I want in the slice.\n\ntx_yr_rain |&gt; \n  group_by(city) |&gt; \n  slice_max(tot_rain, n = 3)\n\n\n\n  \n\n\n\nHere is the least rain:\n\ntx_yr_rain |&gt; \n  group_by(city) |&gt; \n  slice_min(tot_rain, n = 3)\n\n\n\n  \n\n\n\n\nOYO: Years with most snow\nTry to do the same to find the years with the most snow in each city."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#working-through-logic",
    "href": "lesson-day2-01-analysis.html#working-through-logic",
    "title": "Data Wrangling",
    "section": "Working through logic",
    "text": "Working through logic\nHere is a question that takes a couple of steps to accomplish: What is the average monthly rainfall for each city? i.e., How much rain should we expect each month, based on history?\nDo we need to consider all our data? We don’t want partial years or months or our math won’t work out right, so again, we want just full years, so 1940 through 2022 for the Texas data.\nDo we need to consider data in groups? This is tricky. We can’t just group by month and get the average because then we would be averaging the rain each day within a month. We have to total the rain within a month for each year, then we can get the average.\nWhat calculations do we need? Kinda answered that in that we need two calculations: One to total the rain within each month/year, and another to get the averages across those months.\nWe could do the code all in one chunk, but you wouldn’t see the result of the first group and sum, so we’ll do it in two.\n\nFirst: Sum rain within a city, month, year\n\nUse the copy-to-clipboard option to add this to your notebook in the “Working through logic” section. Annotations about the code follow.\n\n\ntx_mn_yr_rain &lt;- tx_clean |&gt; \n1  filter(yr &gt;= 1940, yr &lt;= 2022) |&gt;\n2  group_by(city, mn, yr) |&gt;\n3  summarize(mn_yr_rain = sum(rain, na.rm = TRUE))\n\ntx_mn_yr_rain  \n\n\n1\n\nWe filter to get our full years.\n\n2\n\nWe group by three things so we can add the rain in each city for each month of each year.\n\n3\n\nWe then sum the rain with a nice name. We use na.rm = TRUE because there were cases where the rain value was blank instead of 0.0. Perhaps the station was down? Would need to do some reporting to figure that out.\n\n\n\n\n\n\n  \n\n\n\n\n\nSecond: Average rain by city, month\nNow we can group by city and month to average together all the “Jan” in “Austin”, then all the “Feb” in “Austin”, etc.\n\nUse the copy-to-clipboard option to add this to your notebook. Annotations follow.\n\n\ncity_avg_rain &lt;- tx_mn_yr_rain |&gt; \n1  group_by(city, mn) |&gt;\n2  summarise(avg_mn_rain = mean(mn_yr_rain))\n\n\n1\n\nWe take the result from above and group now by each city and each month, so we can work with all the “Austin in Jan” to get our average.\n\n2\n\nNow we can get the average mean() of those months.\n\n\n\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\ncity_avg_rain\n\n\n\n  \n\n\n\n\n\nPlot the results\nAnd to tease you to what is in store for you tomorrow, let’s plot those results. We won’t get into each line of code here, I just want to show you it can be done.\n\ncity_avg_rain |&gt; \n  ggplot(aes(x = mn, y = avg_mn_rain, group = city)) +\n  geom_line(aes(color = city)) +\n  ylim(0,6) +\n  labs(\n    title = \"Average monthly rainfall, 1940-2022\",\n    x = \"\", y = \"Average monthly rain\",\n    color = \"City\"\n  )"
  },
  {
    "objectID": "lesson-day2-01-analysis.html#challenge-earliest-100-day-each-city",
    "href": "lesson-day2-01-analysis.html#challenge-earliest-100-day-each-city",
    "title": "Data Wrangling",
    "section": "Challenge: Earliest 100+ day each city",
    "text": "Challenge: Earliest 100+ day each city\nFind the earliest date in each city where it reached at least 100 degrees. Here is a hint: Slice is nice and we have the yd field that is the number of days into a year."
  },
  {
    "objectID": "lesson-day2-01-analysis.html#what-do-you-want-to-learn-about-the-weather",
    "href": "lesson-day2-01-analysis.html#what-do-you-want-to-learn-about-the-weather",
    "title": "Data Wrangling",
    "section": "What do you want to learn about the weather?",
    "text": "What do you want to learn about the weather?\nDream up a question and answer it. Maybe import your own state’s data and find something there. The earliest snowfall. The snowiest month on average. The first freeze. The last freeze. Or more challenging: the average day of the last freeze (i.e., when can I plant my garden!)."
  },
  {
    "objectID": "lesson-day1-01-intro.html",
    "href": "lesson-day1-01-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The purpose of this first module is to:\n\nExplain the benefits of using scripted data journalism techniques.\nGet the first two days of material onto your computer.\nGet you familiar with RStudio, the program we use to write R code.\nIntroduce Quarto documents, which allow you to write notes and code together in the same document, just like this document that you are reading."
  },
  {
    "objectID": "lesson-day1-01-intro.html#welcome-to-the-center-for-health-journalism-r-course.",
    "href": "lesson-day1-01-intro.html#welcome-to-the-center-for-health-journalism-r-course.",
    "title": "Introduction",
    "section": "",
    "text": "The purpose of this first module is to:\n\nExplain the benefits of using scripted data journalism techniques.\nGet the first two days of material onto your computer.\nGet you familiar with RStudio, the program we use to write R code.\nIntroduce Quarto documents, which allow you to write notes and code together in the same document, just like this document that you are reading."
  },
  {
    "objectID": "lesson-day1-01-intro.html#about-quarto-r-and-scripted-journalism",
    "href": "lesson-day1-01-intro.html#about-quarto-r-and-scripted-journalism",
    "title": "Introduction",
    "section": "About Quarto, R and scripted journalism",
    "text": "About Quarto, R and scripted journalism\nBefore we dive into RStudio and programming and all that, I want to show you where we are heading, so you can “visualize success”. We are teaching a method for data journalism that is repeatable, transparent and annotated. When you do your work, you should intersperse notes and code, creating a document your future self can easily catch up on, or that you can share with others. The best way to explain this is to show you an example.\n\nGo to this link in a new browser window: Major League Soccer salaries.\n\nThis is a website with all the code from a data journalism project. If you click on the navigation link for Cleaning you can read where the data come from and see all the steps I went through – with code and explanation – to process the data so I could work with it. And in the Analysis 2023 notebook you’ll see I set out with some questions for the data, and then I wrote the code to find my answers. Along with the way I wrote explanations of how and why I did what I did.\n\n\n\nQuarto Pub page\n\n\nThis website was created using Quarto and R, and the tool I used to write everything was RStudio.\nThis document you are reading is also a Quarto document. These lessons have explanations, instructions and code you can run right on your computer. You’ll also write and run some on your own code.\n\nThe written words and explanations are written in a syntax called Markdown. It’s a language commonly used by programmers to create documentation. It’s understandable as text, but also easily converted to other formats like HTML.\nThe programming code for data is written in R in code chunks. We’ll introduce those in a moment."
  },
  {
    "objectID": "lesson-day1-01-intro.html#rstudio-tour",
    "href": "lesson-day1-01-intro.html#rstudio-tour",
    "title": "Introduction",
    "section": "RStudio tour",
    "text": "RStudio tour\nWhen you launch RStudio, you get a screen that looks like this:\n\n\n\nRStudio launch screen\n\n\nSome things of note here:\n\nAt the top-right of your RStudio window is a Project menu that should show the name of the project that you have open. If you click on the dropdown, you’ll see other options.\n\nWe always want to be working in a project, which is basically a folder to hold your documents and data.\n\nNote the quadrant on the bottom right that includes the Files, Help and Viewer panes. You’ll likely be flipping back and forth among those.\n\nThe Files pane shows all the documents in your project. This is where you go to open new documents in your project.\nThe Help pane allows us to search for help using R.\nThe Viewer pane is where our rendered documents display."
  },
  {
    "objectID": "lesson-day1-01-intro.html#updating-preferences",
    "href": "lesson-day1-01-intro.html#updating-preferences",
    "title": "Introduction",
    "section": "Updating preferences",
    "text": "Updating preferences\nThere are some preferences in RStudio that I would like you to change. By default, the program wants to save the state of your work (all the variables and such) when you close a project, but that is typically not good practice. We’ll change that.\n\nGo to the Tools menu and choose Global Options.\nUnder the General tab, uncheck the first four boxes.\nOn the option “Save Workspace to .Rdata on exit”, change that to Never.\nClick Apply to save the change (but don’t close the box yet).\n\n\n\n\nRStudio preferences\n\n\nNext we will set some value is the Code pane.\n\nOn the left options, click on the Code pane.\nCheck the box for Use native pipe operator, |&gt;.\nClick OK to save and close the box.\n\n\n\n\nNative pipe preference\n\n\nWe’ll get into why we did this part later."
  },
  {
    "objectID": "lesson-day1-01-intro.html#the-r-package-environment",
    "href": "lesson-day1-01-intro.html#the-r-package-environment",
    "title": "Introduction",
    "section": "The R Package environment",
    "text": "The R Package environment\nR is an open-source language, which means that other programmers can contribute to how it works. It is what makes R beautiful.\nWhat happens is developers will find it difficult to do a certain task, so they will write code that solves that problem and save it into an R “package” so they can use it later. They share that code with the community, and suddenly the R garage has an “ultimate set of tools” that would make Spicoli’s dad proud.\nOne set of these tools is the tidyverse developed by Hadley Wickham and his team at Posit. It’s a set of R packages for data science that are designed to work together. I highly recommend Wickham’s book R for data science, which is free.\nThere are also a series of useful tidyverse cheatsheets that can help you as you use the packages and functions from the tidyverse. We’ll refer to these throughout the course.\n\nInstall packages we’ll use\n\nCopy the code below and paste it into Console of RStudio (The left pane). Hit return to run the code.\n\ninstall.packages(c(\"quarto\", \"rmarkdown\", \"tidyverse\", \"janitor\"))\nYou’ll see a bunch of response fly by in the Console. It’s probably all fine unless it ends the last response with an error.\nThis installs R software packages onto your computer we’ll use later. You only have to install these packages once."
  },
  {
    "objectID": "lesson-day1-01-intro.html#go-to-the-next-chapter",
    "href": "lesson-day1-01-intro.html#go-to-the-next-chapter",
    "title": "Introduction",
    "section": "Go to the next chapter",
    "text": "Go to the next chapter\nWe’re moving on to the next chapter: Importing & Cleaning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CHJR 2023",
    "section": "",
    "text": "Lessons for the 2023 version of the Center for Health Journalism’s Data Fellowship.\nWith these lessons we’ll learn about:\n\nBasics of R, RStudio and Quarto\nImporting, cleaning and exporting data\nBasic data wrangling\nworking with text\nShaping data\nVisualizing data\nProject management\n\n\n\nYou need to have both R and RStudio Desktop installed on your computer. Hopefully this is already done.\n\nYou can find the installers here. You need both R and RStudio.\nIf on a Mac, make sure you install RStudio into your Applications folder.\n\nThere is a chance we might have to install Quarto CLI, but don’t worry about it unless we need it. If we have challenges wih Macs installing xcode-cli tools, I have a copy here.\n\n\n\nNow we’ll download the course materials for the first couple of days of class. They are stored on Github.\n\nGo to the repo on Github: github.com/r-journalism/chjr23\nClick on the big green Code button and choose Download zip.\nFind the downloaded file and unzip the folder. Mac: Double-click on it. PC directions here\nMove the unzipped folder to your Desktop (or some other place you can find it).\nGo back to RStudio and go to File &gt; Open Project. Go find your folder and open the file chjr23.Rproj.\n\n\n\n\nFor at least the first couple of days we’ll use web-published versions of the lessons to read as we work through practice notebooks in the project.\n\nStart here\n\n\n\n\n\nDay 3: Tidying data\nDay 4: Visualizing data"
  },
  {
    "objectID": "index.html#installing-the-software",
    "href": "index.html#installing-the-software",
    "title": "CHJR 2023",
    "section": "",
    "text": "You need to have both R and RStudio Desktop installed on your computer. Hopefully this is already done.\n\nYou can find the installers here. You need both R and RStudio.\nIf on a Mac, make sure you install RStudio into your Applications folder.\n\nThere is a chance we might have to install Quarto CLI, but don’t worry about it unless we need it. If we have challenges wih Macs installing xcode-cli tools, I have a copy here."
  },
  {
    "objectID": "index.html#download-this-repo",
    "href": "index.html#download-this-repo",
    "title": "CHJR 2023",
    "section": "",
    "text": "Now we’ll download the course materials for the first couple of days of class. They are stored on Github.\n\nGo to the repo on Github: github.com/r-journalism/chjr23\nClick on the big green Code button and choose Download zip.\nFind the downloaded file and unzip the folder. Mac: Double-click on it. PC directions here\nMove the unzipped folder to your Desktop (or some other place you can find it).\nGo back to RStudio and go to File &gt; Open Project. Go find your folder and open the file chjr23.Rproj."
  },
  {
    "objectID": "index.html#lessons-etc",
    "href": "index.html#lessons-etc",
    "title": "CHJR 2023",
    "section": "",
    "text": "For at least the first couple of days we’ll use web-published versions of the lessons to read as we work through practice notebooks in the project.\n\nStart here"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "CHJR 2023",
    "section": "",
    "text": "Day 3: Tidying data\nDay 4: Visualizing data"
  },
  {
    "objectID": "lesson-day1-02-clean.html",
    "href": "lesson-day1-02-clean.html",
    "title": "Importing & Cleaning",
    "section": "",
    "text": "In this second lesson we will work through building a notebook where you import data, manipulate it and do some analysis. While you may be viewing these lessons online, know they are also in the project folder all starting with lesson-. You’ll build your notebook in another file in practice-day1.qmd where you’ll take the pre-written code found here and apply it there. You’ll also have chances to write your own code with mini on-your-own quests.\nThe purpose of this module is to import and clean a data set, then export it to use in the next lesson.\nWe’ll be using daily weather summaries from Climate Data Online – daily temperature and precipitation readings.\n\n\n\n\n\n\nTip\n\n\n\nIn my projects I typically have one notebook for downloading and cleaning my data, and then another notebook for analyzing my data. Since this is a guided training, the organization of this project is a little different. We’ll walk through building a new project later."
  },
  {
    "objectID": "lesson-day1-02-clean.html#goals-of-this-lesson",
    "href": "lesson-day1-02-clean.html#goals-of-this-lesson",
    "title": "Importing & Cleaning",
    "section": "",
    "text": "In this second lesson we will work through building a notebook where you import data, manipulate it and do some analysis. While you may be viewing these lessons online, know they are also in the project folder all starting with lesson-. You’ll build your notebook in another file in practice-day1.qmd where you’ll take the pre-written code found here and apply it there. You’ll also have chances to write your own code with mini on-your-own quests.\nThe purpose of this module is to import and clean a data set, then export it to use in the next lesson.\nWe’ll be using daily weather summaries from Climate Data Online – daily temperature and precipitation readings.\n\n\n\n\n\n\nTip\n\n\n\nIn my projects I typically have one notebook for downloading and cleaning my data, and then another notebook for analyzing my data. Since this is a guided training, the organization of this project is a little different. We’ll walk through building a new project later."
  },
  {
    "objectID": "lesson-day1-02-clean.html#open-the-practice-file",
    "href": "lesson-day1-02-clean.html#open-the-practice-file",
    "title": "Importing & Cleaning",
    "section": "Open the practice file",
    "text": "Open the practice file\nLet’s get started.\n\nMake sure the Files page is open in the bottom right pane of RStudio.\nClick on the gear icon and choose Go To Working Directory. This takes the file explorer to our project folder so we know where everything is.\nClick and open the practice-day1-clean.qmd file.\n\nOur notebooks start with metadata at the top that includes the title listing, like this one, written in YAML and bracketed by the three dashes. There are other configurations you can apply in the metadata, but we won’t here.\nBelow the metadata you’ll want to explain the goals of what you are doing in this notebook. We write these notes in Markdown in between our code."
  },
  {
    "objectID": "lesson-day1-02-clean.html#packages-and-libraries",
    "href": "lesson-day1-02-clean.html#packages-and-libraries",
    "title": "Importing & Cleaning",
    "section": "Packages and libraries",
    "text": "Packages and libraries\nAfter the goals in a notebook, the next thing you should always have is the libraries you’ll use. While there is a lot of functionality baked into R, users can also write and package pre-written code into libraries. Different libraries have different “functions” that we use to manipulate our data in some way. Learning how to use these functions IS programming.\nWe almost always load the tidyverse library which is actually a collection of libraries, including:\n\nreadr has functions that import and export data\ndplyr has functions to manipulate data, like sorting and filtering\nstringr helps us work with text\ntidyr helps us shape data for different purposes\nggplot helps us visualize data through charts\n\nWe’ll use functions from all of these libraries, but they come in the one big toolbox, tidyverse.\nWe’ll use another function from another library, janitor to standardize some column names.\nHere is how we set up the libraries. It is usually the first code chunk you’ll have in your notebook.\n\n\n\n\n\n\nTip\n\n\n\nThe code block below is displayed online in a special way to show you the tick marks, language designation and some execution options that are explained below. Usually you will need to insert a code block yourself, then copy or type in the code.\n\n\nThis code chunk below has two special execution options that affect how the code works.\n\nlabel: setup gives this chunk a special name that tells RStudio to run this block before any other if it hasn’t been run already.\nmessage: false suppresses the usual messages we see in our notebook after loading libraries. With most code chunks we want to see these messages, but not this one because they are standard. Plus, I wanted to show you how the options work.\n\nExecution options are not required, but those two are useful for our libraries chunk. That’s often the only place I use any.\n\nIn your practice notebook after the ## Libraries headline …\nUse the copy-to-clipboard icon at top-right of the code block below, then paste it into your notebook.\nRun the code block above using either the play button inside your Quarto document, or by placing your cursor in the code chunk and using Cmd-shift-return on your keyboard.\n\nYou’ll see a flash of green but you won’t see any feedback in your notebook because we suppressed it.\n\n```{r}\n#| label: setup\n#| message: false\n\nlibrary(tidyverse)\nlibrary(janitor)\n```\n\nAs noted earlier, from now on you’ll need to insert your own code block and then write the code inside. We’ll do that next."
  },
  {
    "objectID": "lesson-day1-02-clean.html#functions",
    "href": "lesson-day1-02-clean.html#functions",
    "title": "Importing & Cleaning",
    "section": "Functions",
    "text": "Functions\nThose library() commands used above are what we call a function in R, and it is similar to formulas in a spreadsheet. They are the “verbs” of R where all the action happens.\nInside the parenthesis of the function we add arguments. In that library function it needed to know which package to load. Usually the first argument what data we are inserting into the function. There can be other options to control the function.\nfunction(data, option_name = \"value\")\nWe can also string these functions together, taking the result of one and piping it into the next function. We’ll do that soon."
  },
  {
    "objectID": "lesson-day1-02-clean.html#importing-data",
    "href": "lesson-day1-02-clean.html#importing-data",
    "title": "Importing & Cleaning",
    "section": "Importing data",
    "text": "Importing data\nWe will use a function from the readr library to import our weather data. We choose which function to use based on the format of the data we are trying to import.\nThe data I have for you here is in “csv” format, or comma separated values. In this project we have two data folders, data-raw where we put our original data, and data-processed where we put anything we export out. Our aim here is to avoid changing our original raw data.\n\nInsert your import chunk\n\n\n\n\n\n\nImportant\n\n\n\nFrom now on you’ll mostly create your own chunks in your practice notebook and type in the code indicated in the book. While it is possible to copy/paste the code easily, I implore you to type all the code here so you get used to using the RStudio editor.\n\n\n\nAfter the ## Import headline and description there, insert a new code chunk. You can use the keyboard command Cmd+option+i or use the green +C icon in the notebook toolbar.\nType in read_csv() into the code chunk. You’ll see type-assist trying to help you.\nOnce that is there, put your cursor in between the parenthesis (if it isn’t already) and type in an opening quote \". You’ll see that the closing quote is automatically added and your cursor is again put in the middle of them.\nType in data-raw/ and then hit tab on your keyboard. You should see a menu pop up with the available files. Choose the tx.csv file.\nOnce your code looks like what I have below, run the chunk. (Use Cmd-shift-return from inside the chunk or click the green arrow at the top-right of the chunk.)\n\n\nread_csv(\"data-raw/tx.csv\")\n\nRows: 94503 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\nWe get two outputs here in our notebook:\n\nThe R Console pane shows messages about our import. The column data types and things like that.\nThe second pane shows our data. The data construct here is called a “data frame” or “tibble”.\n\n\n\nMore about readr\nThere is a cheatsheet in the readr documentation that outlines functions to import different kinds of data. There are also options for things like skipping lines, renaming columns, setting data types and other common challenges."
  },
  {
    "objectID": "lesson-day1-02-clean.html#the-pipe",
    "href": "lesson-day1-02-clean.html#the-pipe",
    "title": "Importing & Cleaning",
    "section": "The pipe",
    "text": "The pipe\nTo provide some consistency and save from having to use the shift key so much, we are going to run our data through a function called clean_names() after we read it in. As we do this we’ll learn about the “pipe” which moves the result of an object or function into a new function.\n\nEdit your import chunk to add the code below: |&gt; clean_names().\nRun the chunk.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Cmd+shift+m inside a code block to type a pipe. If you get %&gt;% instead, don’t fret. Keep reading.\n\n\n\nread_csv(\"data-raw/tx.csv\") |&gt; clean_names()\n\nRows: 94503 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\nIf you look at the second result that shows our data, you may notice that the names of our columns are different. That’s what clean_names() does.\n\nAbout clean_names()\nThe clean_names() function is from the janitor package, and it standardizes the names of our columns, which we call “variables” in R. (Our rows are called “observations”.)\n\nIt lowercases all the variable names.\nIt standardizes the text, removing special characters, etc.\nIf the variable names have more than one word, it will put an underscore between them: birth_date.\n\nUsing clean_names() is a preference and you don’t have to do it. I almost always do. It can save keyboard strokes later and makes it easy to copy/paste the variable names.\n\n\nAbout the pipe |&gt;\nThe pipe is a construct that takes the result of an object or function and passes it into another function. Think of it like a sentence that says “AND THEN” the next thing.\nLike this:\nI woke up |&gt; \n  got out of bed |&gt;\n  dragged a comb across my head\nYou can’t start a new line with a pipe. If you are breaking your code into multiple lines, then the |&gt; needs to be at the end of a line and the next line should be indented so there is a visual clue it is related to line above it, like this:\nread_csv(\"data-raw/tx.csv\") |&gt; \n  clean_names()\nIt might look like there are no arguments inside clean_names(), but what we are actually doing is nesting the imported data frame into it like this:\nclean_names(read_csv(\"data-raw/tx.csv\"))\nFor a lot of functions in R the first argument is “what data are you taking about?” The pipe allows us to say “hey, take the data we just mucked with (i.e., the code before the pipe) and use that in this new function.”\nYou can see from the nested example above that code without the pipe can get confusing. Using the pipe makes our code much more readable, like a sentence.\n\nA rabbit dives into a pipe\nThe concept of the pipe was first introduced by tidyverse developers in 2014 in a package called magrittr. They used the symbol %&gt;% as the pipe. It was so well received the concept was written directly into base R in 2021, but using the symbol |&gt;. Hadley Wickham’s 2022 rewriting of R for Data Science uses the base R pipe |&gt; by default, so we are too. We configured which version to use in RStudio when we updated preferences.\nThis switch to |&gt; is quite recent so you will still see %&gt;% used in our training and in documentation online. Assume |&gt; and %&gt;% are interchangeable."
  },
  {
    "objectID": "lesson-day1-02-clean.html#objects",
    "href": "lesson-day1-02-clean.html#objects",
    "title": "Importing & Cleaning",
    "section": "Objects",
    "text": "Objects\nWhile we have data printing to our screen, it hasn’t been saved and we can’t reuse it. That’s next.\nTo save something in our R environment to reuse it, we create an “object”. An object can be made from a vector (a list of one or more like items), a data frame (a collection of vectors, like a structured spreadsheet) or even a plot. In short, it is how we save things in our environment (in memory) to reuse later.\nBy convention we name the object first, then use &lt;- to fill it with our data. Think of it like this: You must have a bucket first before you can fill it with water. The arrow shows you which way the water is flowing.\n\nEdit your import code block to add the tx_raw &lt;- part shown below\nRe-run the chunk. Again, Cmd+shift+return will run the entire chunk.\n\n\ntx_raw &lt;- read_csv(\"data-raw/tx.csv\") |&gt; clean_names()\n\nRows: 94503 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nWe still get messages about our input\nBut instead of printing our data to the screen, we have saved it into tx_raw.\nIf you look at your Environment pane at the top-right of RStudio, you’ll see your saved object listed there.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Option+i in a code chunk to type in &lt;-. See the Keyboard shortcuts chapter.\n\n\nLet’s print the data out again so we can see it.\n\nEdit your import chunk to add two returns after our line of code and then type out our object so it will display again.\n\n\ntx_raw &lt;- read_csv(\"data-raw/tx.csv\") |&gt; clean_names()\n\nRows: 94503 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntx_raw\n\n\n\n  \n\n\n\nLet’s talk about this output a little because there is a lot of useful information here and it looks different in your notebook vs a rendered page.\n\n\n\nTibble display\n\n\nUse the image above to orient yourself to the data frame display in your notebook. Click on the column arrows and page links to see more of the data.\nWe have now imported our data and saved it into an object we can continue to work with.\n\nOYO: Import new data\nHere I want you to import weather data from a different state and save it into an object. You can look in the data-raw folder to see the files to choose from, perhaps from your state.\nYou’ll do this all in the same chunk, editing the code and running it after each step.\n\nIn your practice notebook in the “OYO: Import a different state” space, add a new code chunk.\nUse the read_csv() command to read in your data and run it to make sure it works.\nUsing a pipe, add the clean_names() function to it and run it to make sure it still works.\nEdit your chunk to save your data into a new object. (I really recommend you use the same naming convention, but with your state, like nc_raw.) Make sure you see it in your Environment tab.\nAdd a new line in your chunk with your new object so it will print out so you can see it.\nOutside of the chunk, add some notes in text to tell your future self what you’ve done."
  },
  {
    "objectID": "lesson-day1-02-clean.html#peeking-at-data",
    "href": "lesson-day1-02-clean.html#peeking-at-data",
    "title": "Importing & Cleaning",
    "section": "Peeking at data",
    "text": "Peeking at data\nThere are a number of ways to look at your data. We’ll tour through some here.\n\nHead, Tail\nWith head() and tail() you can look at the “top” and “bottom” of your data. The default is to show six lines of data, but you can add an argument to do more.\n\nWhere indicated after the ## Peeking headline, add a new chunk.\nStart with your tx_raw data and pipe into head() like below.\n\n\ntx_raw |&gt; head()\n\n\n\n  \n\n\n\n\nAs indicated in the Peeking section, add a new chunk and get 8 lines from the bottom of your data, like this:\n\n\ntx_raw |&gt; tail(8)\n\n\n\n  \n\n\n\n\n\nGlimpse\nThe glimpse() function allows you to look at your data in another way … to see all the variables and their data types, no matter how many there are.\n\nAs indicated in the practice notebook, add a chunk and glimpse your data like below, and then run it.\n\n\ntx_raw |&gt; glimpse()\n\nRows: 94,503\nColumns: 10\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n\n\nThis is super handy to have because you can see all your variable names in the same screen. I use it all the time.\n\n\nSummary\nThe summary() function loops through all your variables and gives you some basic information about them, especially when they are numbers or dates.\n\nAt the indicated spot in the notebook, add a chunk and get a summary of your data like this below:\n\n\ntx_raw |&gt; summary()\n\n   station              name                date                 prcp        \n Length:94503       Length:94503       Min.   :1930-08-01   Min.   : 0.0000  \n Class :character   Class :character   1st Qu.:1959-01-12   1st Qu.: 0.0000  \n Mode  :character   Mode  :character   Median :1980-08-05   Median : 0.0000  \n                                       Mean   :1980-06-04   Mean   : 0.1121  \n                                       3rd Qu.:2002-03-09   3rd Qu.: 0.0000  \n                                       Max.   :2023-09-30   Max.   :12.0700  \n                                                            NA's   :1867     \n      snow            snwd            tavg            tmax       \n Min.   :0.000   Min.   :0.000   Min.   : 0.0    Min.   : 13.00  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:60.0    1st Qu.: 69.00  \n Median :0.000   Median :0.000   Median :73.0    Median : 81.00  \n Mean   :0.003   Mean   :0.004   Mean   :70.2    Mean   : 78.56  \n 3rd Qu.:0.000   3rd Qu.:0.000   3rd Qu.:82.0    3rd Qu.: 91.00  \n Max.   :7.800   Max.   :7.000   Max.   :98.0    Max.   :112.00  \n NA's   :15369   NA's   :15463   NA's   :78843   NA's   :16      \n      tmin            tobs      \n Min.   :-2.00   Min.   :24.00  \n 1st Qu.:47.00   1st Qu.:61.00  \n Median :61.00   Median :72.00  \n Mean   :58.65   Mean   :69.65  \n 3rd Qu.:72.00   3rd Qu.:80.00  \n Max.   :93.00   Max.   :99.00  \n NA's   :16      NA's   :91914  \n\n\nThis is super useful to get basic stats like the lowest, highest, average and median values.\nTo get a summary for single column you can name the data frame first, then append $ and the column name, like this:\n\ntx_raw$date |&gt; summary()\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"1930-08-01\" \"1959-01-12\" \"1980-08-05\" \"1980-06-04\" \"2002-03-09\" \"2023-09-30\" \n\n\n\n\nOYO: Peek at your own data\n\nAt the place indicated in your practice notebook, add new chunks to use these “peeking” functions to look at the data in your state you imported above. At least try glimpse() and summary()."
  },
  {
    "objectID": "lesson-day1-02-clean.html#create-or-change-data",
    "href": "lesson-day1-02-clean.html#create-or-change-data",
    "title": "Importing & Cleaning",
    "section": "Create or change data",
    "text": "Create or change data\nA little later in our analysis we will want to do some calculations in our data based on the year and month of our date. If we were doing this analysis for the first time we might not realize that yet and would end up coming back to this notebook to do these things below, but we have the knowledge of foresight here.\nWe’ll use the mutate() function to create a new column based on other columns. As the name implies, mutate() changes or creates data.\nLet’s explain how mutate works first:\n# This is just explanatory psuedo code\n# You don't need this in your notebook\ndata |&gt; \n  mutate(\n    newcol = new_stuff_from_math_or_whatever\n  )\nThat new value could be arrived at through math or any combination of other functions. In our case, we will be plucking out parts of our date variable to create some other useful variables. The first one we’ll build is to get the “year” from our date.\n\nBuild the machine\nWe are going to build this code chunk piece by piece, like we would if we were figuring it out for the first time. I want you to see the logic of working through a task like this.\n\nWhere indicated in ## Mutate section, add a new code chunk to create your date parts.\nType the code I have below and run the chunk. I’ll explain it afterward.\n\n\ntx_dates &lt;- tx_raw\n\ntx_dates |&gt; glimpse()\n\nRows: 94,503\nColumns: 10\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n\n\nWhat are doing here is creating a machine of sorts that we will continue to tinker with.\n\nWe start with a new object called tx_dates and then fill it with tx_raw.\nWe then glimpse the new tx_dates object so we can see all the columns and some of the values.\n\nRight now there is no difference between tx_dates and tx_raw but we’ll fix that. Doing it this way allows us to see all our columns at once with glimpse.\n\n\nAdd on the mutate\n\nEdit your code chunk to add a pipe at the end of the first line, then hit return.\nType in the mutate function, then add a return in the middle so we can add multiple arguments in a clean way.\nAdd the line yr = year(date) inside the mutate.\nRun the code and inspect the bottom of the glimpse.\n\n\ntx_dates &lt;- tx_raw |&gt; \n  mutate(\n    yr = year(date)\n  )\n\ntx_dates |&gt; glimpse()\n\nRows: 94,503\nColumns: 11\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n$ yr      &lt;dbl&gt; 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 19…\n\n\nDo you see the new column added at the end? That is our new column that just has the year from each date. Some notes about this:\n\nWithin the mutate I started with the name of the new column first: yr. I called the new variable “yr” instead of “year” because there actually is a year() function that we use in that same line and I don’t want to get confused. FWIW, R wouldn’t care, but we are human.\nThe year(date) code is using the year() function to pluck those four numbers out of each the date column for each row of our data. Since we are creating a new column to put this in, we aren’t changing our original data at all.\n\nThis is the equivalent of adding a new column to a spreadsheet, and then using a formula that builds from other columns in the spreadsheet and then copying it all the way down the sheet.\nIf you want to see this in a table view, you can highlight just the tx_dates object in the last line of the code chunk and do Cmd+return on your keyboard to print it to your screen. You could then page over to see the new column. I like using glimpse instead so I can see all the columns at once, but it takes some getting used to.\n\n\nAdd more components\nWe’ll add two more columns to our spreadsheet within the same mutate() function.\n\nEdit your code chunk to add two new arguments to the code chunk as noted below. I explain them after.\n\n\ntx_dates &lt;- tx_raw |&gt; \n  mutate(\n    yr = year(date),\n    mn = month(date, label = TRUE),\n    yd = yday(date)\n  )\n\ntx_dates |&gt; glimpse()\n\nRows: 94,503\nColumns: 13\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n$ yr      &lt;dbl&gt; 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 19…\n$ mn      &lt;ord&gt; Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Au…\n$ yd      &lt;dbl&gt; 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 22…\n\n\nThis added two new columns to our date, one for the month and one for “day of the year”.\n\nThe month(date, label = TRUE) function gives us what is called an “ordered factor” with an abbreviation of our month name. Factors are text strings that have an order to them, so this field knows that “Jan” comes before “Feb” instead of ordering things alphabetically. If this were a string &lt;chr&gt; then “Apr” would come first when we sorted it. If we didn’t include the part label = TRUE argument then we would’ve gotten a number for the date, like “8” for August.\nThe yday(date) function is calculating how many days into the year each date falls. So if it were February 1st it would give us “32” since there are 31 days in January then the one day into February. This is probably unusual to create this, but I have a challenge for you later that needs it.\n\n\n\nAbout Lubridate\nThe functions we used above to get those date components come from the lubridate package, which gets loaded with our tidyverse library. It is a package designed to ease the friction of working with dates (get it?), which can be a challenge in programming. You can use it to convert text into dates, get date components, adjust time zones and all kinds of things. I use the cheatsheet from this package a lot, usually to “parse date-times” or “get and set components”.\n\n\nOYO: Create new date compoents\nOn your own, create date components with your state’s data like we did above. Follow the same steps to build the machine like we did in the example instead of copy/pasting."
  },
  {
    "objectID": "lesson-day1-02-clean.html#recoding-values",
    "href": "lesson-day1-02-clean.html#recoding-values",
    "title": "Importing & Cleaning",
    "section": "Recoding values",
    "text": "Recoding values\nIn our weather data we have the name column that has the name of the station the readings came from. Those names are pretty long and will get unwieldy later, so let’s create more simple names for the cities, like “Austin”, “Houston” and “Dallas”.\n\nFind distinct values\nIt would be nice to see the station names easily so we can spell them correctly. We’ll use a function called distinct() to find the unique values for name.\n\nIn the ## Recoding values section of your notebok, add a code chunk after the prompt about distinct.\nAdd the code below and run it.\n\n\ntx_dates |&gt; distinct(name)\n\n\n\n  \n\n\n\nWe are taking our data AND THEN finding the “distinct” values in our name column.\nWe don’t need to save this into a new object or anything. It’s just to help us copy/paste the names of the stations in the next step.\n\n\nUse mutate to recode\nNow we’ll create a new column called city that we build based off the original names above. We use a more complicated version of mutate to do this because in the end we are creating new data.\nIn the interest of time we’ll provide the finished code with an explanation vs building it piece by piece.\n\nAdd a new code chunk for the recode.\nTo save time, I want you to use the copy-to-clipboard button (top right of the chunk when you roll your cursor over it) to copy this code and then paste it into your chunk and run it.\n\n(The numbers in the code chunk match the annotations below.)\n\n1tx_names &lt;- tx_dates |&gt;\n  mutate(\n2    city = recode(\n3      name,\n4      \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\" = \"Houston\",\n      \"AUSTIN CAMP MABRY, TX US\" = \"Austin\",\n      \"DALLAS FAA AIRPORT, TX US\" = \"Dallas\"\n    )\n  )\n\n5tx_names |&gt; glimpse()\n\n\n1\n\nWe start with our new object and then start filling it with our tx_dates data. We pipe into the mutate on the next line.\n\n2\n\nInside the mutate function we start with the name of the new column, city, and then set that equal to the values that come from our recode() function.\n\n3\n\nThe first argument of recode is what column we are looking into for our original values. For us this is the name column.\n\n4\n\nFor each line here, we start with our existing value (which we get from the step above) and then set it to our new value. (This construction is counter to the way R normally works where we put our new thing before the old thing.)\n\n5\n\nOn the last line we glimpse our new object so we can see if it worked.\n\n\n\n\nRows: 94,503\nColumns: 14\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n$ yr      &lt;dbl&gt; 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 19…\n$ mn      &lt;ord&gt; Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Au…\n$ yd      &lt;dbl&gt; 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 22…\n$ city    &lt;chr&gt; \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Housto…\n\n\n\n\nCheck our work\nSince we can’t see all the cities, it is a good idea to check our data to make sure this worked the way we wanted. We can use the distinct() function again but with both name and city.\n\nAdd a new chunk in the indicated place.\nAdd the code to check your results using distinct on name and city.\n\n\ntx_names |&gt; distinct(name, city)\n\n\n\n  \n\n\n\nLooks good.\n\n\nOYO: Recode station names for your state\nWe might be pressed for time by this point, but if possible recode your own state’s data with the proper city. Check your work with distinct, as well."
  },
  {
    "objectID": "lesson-day1-02-clean.html#select-columns",
    "href": "lesson-day1-02-clean.html#select-columns",
    "title": "Importing & Cleaning",
    "section": "Select columns",
    "text": "Select columns\nDifferent weather stations can offer different data, and we are just concerned with some specific variables in or data. We can use the select() command to keep or drop columns.\nTo make decisions about what to keep, we would normally spend time with the documentation for the data to make sure we know what is what.\nIn the interest of time, I’ve done that for you and I’ve made a list. In short we are saving the date, rain, snow and high/low temperature values, plus the columns we created. We don’t need TOBS or TAVG, and in some states they have other variables we don’t need.\nAlso in the interest of time, we’ll copy/paste this instead of typing it all in.\n\nIn the ## Select columns section of your notebook, add a code chunk.\nUse the copy-to-clipboard button to copy this code and paste it into your chunk. Run it.\n\nExplanations follow.\n\n1tx_tight &lt;- tx_names |&gt;\n  select(\n2    city,\n    date,\n3    rain = prcp,\n    snow,\n    snwd,\n    tmax,\n    tmin,\n    yr,\n    mn,\n    yd\n  )\n\n4tx_tight |&gt; glimpse()\n\n\n1\n\nWe start with our new object and pour into it our tx_names data with its piped changes.\n\n2\n\nInside select() we list the variables we want to keep in the order we want them.\n\n3\n\nFor the prcp column, we are also renaming the variable to the more familiar rain. It’s easier to remember and type. In typical R fashion the new name comes first.\n\n4\n\nLastly we glimpse the data so we can check if we got what we wanted.\n\n\n\n\nRows: 94,503\nColumns: 10\n$ city &lt;chr&gt; \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Houston\",…\n$ date &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1930…\n$ rain &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, NA,…\n$ snow &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ snwd &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ tmax &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97, 9…\n$ tmin &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70, 7…\n$ yr   &lt;dbl&gt; 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930,…\n$ mn   &lt;ord&gt; Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, …\n$ yd   &lt;dbl&gt; 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, …\n\n\n\nOYO: Select\nAgain, if we have time you could do the same for your state."
  },
  {
    "objectID": "lesson-day1-02-clean.html#export-your-data",
    "href": "lesson-day1-02-clean.html#export-your-data",
    "title": "Importing & Cleaning",
    "section": "Export your data",
    "text": "Export your data\nAfter all that, we finally have the data the way we want it. I do all this work in a separate notebook like this so I don’t have to rerun all the steps when I’m doing analysis (tomorrow!). It is not unusual that during my analysis I come back to this cleaning notebook, fix things and then rerun all the code. That way the fixes are available to all other notebooks using the data.\nOK, how to do we get this data out? We’ll use another readr function called write_rds() to save our data to our computer. We use the .rds file type (which stands for “R data store”?) because unlike CSVs it saves all our data types. Fixing these data types is often our goal in cleaning, like converting text to a date, or a ZIP code to text.\n\nIn the ## Export your data section of your notebook add a new code chunk.\nTake your most recent object tx_tight and pipe it into write_rds() as indicated below.\nAs you type in the path (inside the quotes) note you can type a few letters and then use tab to complete the path. Write the path for to the data-processed folder and then name the file tx_clean.rds, as indicated below.\n\n\ntx_tight |&gt; write_rds(\"data-processed/tx_clean.rds\")\n\n\nOYO: Write out your data\nUse the same methods as above to write out your state’s data."
  },
  {
    "objectID": "lesson-day1-02-clean.html#check-your-notebook",
    "href": "lesson-day1-02-clean.html#check-your-notebook",
    "title": "Importing & Cleaning",
    "section": "Check your notebook",
    "text": "Check your notebook\nLast thing … we haven’t talked yet about the projects, Quarto and rendering notebooks, but let’s take a brief moment to do two things to make sure our notebooks are working properly.\n\nGo under the Run menu and choose Restart R and Clear Output. This cleans out everything in your notebook.\nGo back under Run and choose Run All. This will run all the chunks in your notebook from the top to the bottom.\nCheck closely through the whole thing for errors. If you have them, you might have feedback in your Console, and in your notebook the code chunk will have a red bar along the left edge.\n\nIf there are errors you’ll need to fix them. It is not unusual, especially if you are going up and down the notebook as you work."
  },
  {
    "objectID": "lesson-day1-02-clean.html#render-the-notebook",
    "href": "lesson-day1-02-clean.html#render-the-notebook",
    "title": "Importing & Cleaning",
    "section": "Render the notebook",
    "text": "Render the notebook\nNow that everything is working, you can click the Render button at the top of the notebook. RStudio will format your notebook as an HTML page and show it in your Viewer in bottom-right pane.\nThere is a resource chapter Managing a project where you can read about how to set up projects as a website much like these lessons."
  },
  {
    "objectID": "lesson-day3-01-wrangling.html",
    "href": "lesson-day3-01-wrangling.html",
    "title": "Tidying data",
    "section": "",
    "text": "Let’s start out with two data frames: x and y\n\nx &lt;- data.frame(id=c(1,2,3), x=c(\"x1\", \"x2\", \"x3\"))\n\nx\n\n\n\n  \n\n\n\n\n\ny &lt;- data.frame(id=c(1,2,4), y=c(\"y1\", \"y2\", \"y4\"))\n\ny\n\n\n\n  \n\n\n\n\nTwo data frames\n\n\n\n\n\n\n\n\nlibrary(dplyr)\n\nleft_join(x, y)\n\n\n\n  \n\n\n\nleft_join() illustrated\n\nTwo data frames: x and y but with different column names\n\nx &lt;- data.frame(id=c(1,2,3), x=c(\"x1\", \"x2\", \"x3\"))\n\nx\n\n\n\n  \n\n\n\n\n\ny &lt;- data.frame(new_id=c(1,2,4), y=c(\"y1\", \"y2\", \"y4\"))\n\ny\n\n\n\n  \n\n\n\n\n\n\nleft_join(x, y, by=c(\"id\"=\"new_id\"))\n\n\n\n  \n\n\n\n\nWatch out for repeated data\n\n\n\n\n  \n\n\n\n\n\n\n\n  \n\n\n\n\n\nleft_join(x, y)\n\nJoining with `by = join_by(id)`\n\n\n\n\n  \n\n\n\n\nExtra rows illustrated\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey stringr functions:\nIn this section, we will learn the following stringr functions:\n\n\nstr_to_upper() str_to_lower() str_to_title()\nstr_trim() str_squish()\nstr_c()\nstr_detect()\nstr_subset()\nstr_sub()\n\n\n\n\n\n\nlibrary(stringr)\n\ntest_text &lt;- \"tHiS iS A rANsOM noTE!\"\n\n\n\nstr_to_upper(test_text)\n\n[1] \"THIS IS A RANSOM NOTE!\"\n\n\n\n\n\nstr_to_lower(test_text)\n\n[1] \"this is a ransom note!\"\n\n\n\n\n\nstr_to_title(test_text)\n\n[1] \"This Is A Ransom Note!\"\n\n\n\n\n\n\n\ntest_text &lt;- \"  trim both   \"\n\ntest_text \n\n[1] \"  trim both   \"\n\n\n\n\nstr_trim(test_text, side=\"both\")\n\n[1] \"trim both\"\n\n\n\n\n\nstr_trim(test_text, side=\"left\")\n\n[1] \"trim both   \"\n\n\n\n\n\nstr_trim(test_text, side=\"right\")\n\n[1] \"  trim both\"\n\n\n\n\n\nmessy_text &lt;- \"  sometimes  you get   this \"\n\n\n\n\nstr_squish(messy_text)\n\n[1] \"sometimes you get this\"\n\n\n\n\n\n\n\ntext_a &lt;- \"one\"\n\ntext_b &lt;- \"two\"\n\ntext_a\n\n[1] \"one\"\n\ntext_b\n\n[1] \"two\"\n\n\n\n\nstr_c(text_a, text_b)\n\n[1] \"onetwo\"\n\n\n\n\n\nstr_c(text_a, text_b, sep=\"-\")\n\n[1] \"one-two\"\n\n\n\n\n\nstr_c(text_a, text_b, sep=\" and a \")\n\n[1] \"one and a two\"\n\n\n\n\n\nstr_c(text_a, \" and a \", text_b)\n\n[1] \"one and a two\"\n\n\n\n\n\n\n\ntest_text &lt;- \"Hello world\"\n\ntest_text \n\n[1] \"Hello world\"\n\n\n\n\nstr_sub(test_text, start = 6)\n\n[1] \" world\"\n\n\n\n\n\nstr_sub(test_text, end = 5) &lt;- \"Howdy\"\n\ntest_text\n\n[1] \"Howdy world\"\n\n\n\n\n\ncn &lt;- \"Kemp County, Georgia\"\n\ncn \n\n[1] \"Kemp County, Georgia\"\n\nstr_replace(cn, \" County, .*\", \"\")\n\n[1] \"Kemp\"\n\n\n\n\n\n\nMore functions in stringr and more info on regular expressions here.\n\n\n\n(from the readr package)\n\nlibrary(readr)\nmessy_numbers &lt;- c(\"$5.00\", \"9,343,200\", \"6.0%\")\n\nmessy_numbers\n\n[1] \"$5.00\"     \"9,343,200\" \"6.0%\"     \n\n\n\n\nparse_number(messy_numbers)\n\n[1]       5 9343200       6\n\n\n\n\n\n\n\n\n\n\npractice-day3-wrangling\nGet as far as you can in the time we have!\n\n\n\nSample data\n(You don’t have to type this out)\n2 rows x 3 columns\n\ndf &lt;- data.frame(id=c(1,2), x=c(\"a\", \"b\"),\n                 y=c(\"c\", \"d\"), z=c(\"e\", \"f\"))\n\ndf\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyr)\n\ndf |&gt; \n  pivot_longer(cols=x:z,\n               names_to=\"key\",\n               values_to=\"val\")\n\n\n\n  \n\n\n\n\n\n\n\ndf |&gt; \n  pivot_longer(cols=2:4,\n               names_to=\"key\",\n               values_to=\"val\")\n\n\n\n  \n\n\n\n\ndf &lt;- data.frame(state=c(\"TX\", \"NY\", \"FL\"),\n                 ducks=c(23, 39, 47),\n                 fish=c(6,30,20),\n                 birds=c(99,3,64))\n\n\n\n\n\ndf\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\")\n\n\n\n  \n\n\n\n\n\n\n\n\ndf\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  pivot_longer(cols=2:4,\n               names_to=\"animals\",\n               values_to=\"totals\")\n\n\n\n  \n\n\n\n\n\n\n\n\ndf &lt;- data.frame(state=c(\"TX\", \"NY\", \"FL\"),\n                 ducks=c(23, 39, 47),\n                 fish=c(6,30,20),\n                 birds=c(99,3,64))\n\n\ndf\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\") |&gt; \n  group_by(state) |&gt; \n  mutate(percent=\n           round(total/sum(total)*100,1))\n\n\n\n  \n\n\n\n\n\n\n\n\ndf_long &lt;- df |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\") |&gt; \n  group_by(state) |&gt; \n  mutate(percent=\n           round(total/sum(total)*100,1))\n\n\ndf_long\n\n\n\n  \n\n\n\n\n\n\n\n\ndf_long |&gt; \n  pivot_wider(names_from=\"animals\", \n              values_from=\"percent\")\n\n\n\n  \n\n\n\n\n\n\ndf_long |&gt; \n  select(-total) |&gt; \n  pivot_wider(names_from=\"animals\", \n              values_from=\"percent\") |&gt; \n  mutate(birds_fish_diff=\n           birds-fish)\n\n\n\n  \n\n\n\n\n\n\n\n\ndf_long &lt;- df |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\") |&gt; \n  group_by(state) |&gt; \n  mutate(percent=\n           round(total/sum(total)*100,1))\n\n\ndf_long\n\n\n\n  \n\n\n\n\n\ndf_long |&gt; \n  pivot_wider(names_from=\"animals\", \n              values_from=c(\"total\", \"percent\")) \n\n\n\n  \n\n\n\n\n\n\n\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndf &lt;- data.frame(First=c(\"Charlie\", \"Lucy\", \"Peppermint\"),\n                   Last=c(\"Brown\", \"van Pelt\", \"Patty\"),\n                   birthday=c(\"10-31-06\", \"2/4/2007\", \"June 1, 2005\"))\n\ndf\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  mutate(birthday_clean=mdy(birthday))\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nOrder of elements in date-time\nParse function\n\n\n\n\nyear, month, day\nymd()\n\n\nyear, day, month\nydm()\n\n\nmonth, day, year\nmdy()\n\n\nday, month, year\ndmy()\n\n\nhour, minute\nhm()\n\n\nhour, minute, second\nhms()\n\n\nyear, month, day, hour, minute, second\nymd_hms()\n\n\n\n\n\n\n\n\n\nDate component\nFunction\n\n\n\n\nYear\nyear()\n\n\nMonth\nmonth()\n\n\nWeek\nweek()\n\n\nDay of year\nyday()\n\n\nDay of month\nmday()\n\n\nDay of week\nwday()\n\n\nHour\nhour()\n\n\nMinute\nminute()\n\n\nSecond\nymd_hms()\n\n\nTime zone\nymd_hms()\n\n\n\n\n\n\n\ndf\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  mutate(birthday_clean=mdy(birthday)) |&gt; \n  mutate(month=month(birthday_clean)) |&gt; \n  mutate(year=year(birthday_clean)) |&gt; \n  mutate(week=week(birthday_clean))"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#stringr-functions",
    "href": "lesson-day3-01-wrangling.html#stringr-functions",
    "title": "Tidying data",
    "section": "",
    "text": "Key stringr functions:\nIn this section, we will learn the following stringr functions:\n\n\nstr_to_upper() str_to_lower() str_to_title()\nstr_trim() str_squish()\nstr_c()\nstr_detect()\nstr_subset()\nstr_sub()"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#stringr-in-action",
    "href": "lesson-day3-01-wrangling.html#stringr-in-action",
    "title": "Tidying data",
    "section": "",
    "text": "library(stringr)\n\ntest_text &lt;- \"tHiS iS A rANsOM noTE!\"\n\n\n\nstr_to_upper(test_text)\n\n[1] \"THIS IS A RANSOM NOTE!\"\n\n\n\n\n\nstr_to_lower(test_text)\n\n[1] \"this is a ransom note!\"\n\n\n\n\n\nstr_to_title(test_text)\n\n[1] \"This Is A Ransom Note!\""
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#trimming-strings",
    "href": "lesson-day3-01-wrangling.html#trimming-strings",
    "title": "Tidying data",
    "section": "",
    "text": "test_text &lt;- \"  trim both   \"\n\ntest_text \n\n[1] \"  trim both   \"\n\n\n\n\nstr_trim(test_text, side=\"both\")\n\n[1] \"trim both\"\n\n\n\n\n\nstr_trim(test_text, side=\"left\")\n\n[1] \"trim both   \"\n\n\n\n\n\nstr_trim(test_text, side=\"right\")\n\n[1] \"  trim both\"\n\n\n\n\n\nmessy_text &lt;- \"  sometimes  you get   this \"\n\n\n\n\nstr_squish(messy_text)\n\n[1] \"sometimes you get this\""
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#combining-strings",
    "href": "lesson-day3-01-wrangling.html#combining-strings",
    "title": "Tidying data",
    "section": "",
    "text": "text_a &lt;- \"one\"\n\ntext_b &lt;- \"two\"\n\ntext_a\n\n[1] \"one\"\n\ntext_b\n\n[1] \"two\"\n\n\n\n\nstr_c(text_a, text_b)\n\n[1] \"onetwo\"\n\n\n\n\n\nstr_c(text_a, text_b, sep=\"-\")\n\n[1] \"one-two\"\n\n\n\n\n\nstr_c(text_a, text_b, sep=\" and a \")\n\n[1] \"one and a two\"\n\n\n\n\n\nstr_c(text_a, \" and a \", text_b)\n\n[1] \"one and a two\""
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#extracting-strings",
    "href": "lesson-day3-01-wrangling.html#extracting-strings",
    "title": "Tidying data",
    "section": "",
    "text": "test_text &lt;- \"Hello world\"\n\ntest_text \n\n[1] \"Hello world\"\n\n\n\n\nstr_sub(test_text, start = 6)\n\n[1] \" world\"\n\n\n\n\n\nstr_sub(test_text, end = 5) &lt;- \"Howdy\"\n\ntest_text\n\n[1] \"Howdy world\"\n\n\n\n\n\ncn &lt;- \"Kemp County, Georgia\"\n\ncn \n\n[1] \"Kemp County, Georgia\"\n\nstr_replace(cn, \" County, .*\", \"\")\n\n[1] \"Kemp\""
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#more-stringr-functions",
    "href": "lesson-day3-01-wrangling.html#more-stringr-functions",
    "title": "Tidying data",
    "section": "",
    "text": "More functions in stringr and more info on regular expressions here."
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#parse_number",
    "href": "lesson-day3-01-wrangling.html#parse_number",
    "title": "Tidying data",
    "section": "",
    "text": "(from the readr package)\n\nlibrary(readr)\nmessy_numbers &lt;- c(\"$5.00\", \"9,343,200\", \"6.0%\")\n\nmessy_numbers\n\n[1] \"$5.00\"     \"9,343,200\" \"6.0%\"     \n\n\n\n\nparse_number(messy_numbers)\n\n[1]       5 9343200       6"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#your-turn",
    "href": "lesson-day3-01-wrangling.html#your-turn",
    "title": "Tidying data",
    "section": "",
    "text": "practice-day3-wrangling\nGet as far as you can in the time we have!"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#tidying-data",
    "href": "lesson-day3-01-wrangling.html#tidying-data",
    "title": "Tidying data",
    "section": "",
    "text": "Sample data\n(You don’t have to type this out)\n2 rows x 3 columns\n\ndf &lt;- data.frame(id=c(1,2), x=c(\"a\", \"b\"),\n                 y=c(\"c\", \"d\"), z=c(\"e\", \"f\"))\n\ndf"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot_longer",
    "href": "lesson-day3-01-wrangling.html#pivot_longer",
    "title": "Tidying data",
    "section": "",
    "text": "library(tidyr)\n\ndf |&gt; \n  pivot_longer(cols=x:z,\n               names_to=\"key\",\n               values_to=\"val\")"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot_longer-1",
    "href": "lesson-day3-01-wrangling.html#pivot_longer-1",
    "title": "Tidying data",
    "section": "",
    "text": "df |&gt; \n  pivot_longer(cols=2:4,\n               names_to=\"key\",\n               values_to=\"val\")\n\n\n\n  \n\n\n\n\ndf &lt;- data.frame(state=c(\"TX\", \"NY\", \"FL\"),\n                 ducks=c(23, 39, 47),\n                 fish=c(6,30,20),\n                 birds=c(99,3,64))"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot_longer-again",
    "href": "lesson-day3-01-wrangling.html#pivot_longer-again",
    "title": "Tidying data",
    "section": "",
    "text": "df\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\")"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot_longer-again-1",
    "href": "lesson-day3-01-wrangling.html#pivot_longer-again-1",
    "title": "Tidying data",
    "section": "",
    "text": "df\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  pivot_longer(cols=2:4,\n               names_to=\"animals\",\n               values_to=\"totals\")"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot-for-math",
    "href": "lesson-day3-01-wrangling.html#pivot-for-math",
    "title": "Tidying data",
    "section": "",
    "text": "df &lt;- data.frame(state=c(\"TX\", \"NY\", \"FL\"),\n                 ducks=c(23, 39, 47),\n                 fish=c(6,30,20),\n                 birds=c(99,3,64))\n\n\ndf\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\") |&gt; \n  group_by(state) |&gt; \n  mutate(percent=\n           round(total/sum(total)*100,1))"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot_wider",
    "href": "lesson-day3-01-wrangling.html#pivot_wider",
    "title": "Tidying data",
    "section": "",
    "text": "df_long &lt;- df |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\") |&gt; \n  group_by(state) |&gt; \n  mutate(percent=\n           round(total/sum(total)*100,1))\n\n\ndf_long"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot_wider-1",
    "href": "lesson-day3-01-wrangling.html#pivot_wider-1",
    "title": "Tidying data",
    "section": "",
    "text": "df_long |&gt; \n  pivot_wider(names_from=\"animals\", \n              values_from=\"percent\")\n\n\n\n  \n\n\n\n\n\n\ndf_long |&gt; \n  select(-total) |&gt; \n  pivot_wider(names_from=\"animals\", \n              values_from=\"percent\") |&gt; \n  mutate(birds_fish_diff=\n           birds-fish)"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#pivot_wider-more-columns",
    "href": "lesson-day3-01-wrangling.html#pivot_wider-more-columns",
    "title": "Tidying data",
    "section": "",
    "text": "df_long &lt;- df |&gt; \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\") |&gt; \n  group_by(state) |&gt; \n  mutate(percent=\n           round(total/sum(total)*100,1))\n\n\ndf_long\n\n\n\n  \n\n\n\n\n\ndf_long |&gt; \n  pivot_wider(names_from=\"animals\", \n              values_from=c(\"total\", \"percent\"))"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#lubridate-for-dates",
    "href": "lesson-day3-01-wrangling.html#lubridate-for-dates",
    "title": "Tidying data",
    "section": "",
    "text": "library(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndf &lt;- data.frame(First=c(\"Charlie\", \"Lucy\", \"Peppermint\"),\n                   Last=c(\"Brown\", \"van Pelt\", \"Patty\"),\n                   birthday=c(\"10-31-06\", \"2/4/2007\", \"June 1, 2005\"))\n\ndf\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  mutate(birthday_clean=mdy(birthday))"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#reading-dates",
    "href": "lesson-day3-01-wrangling.html#reading-dates",
    "title": "Tidying data",
    "section": "",
    "text": "Order of elements in date-time\nParse function\n\n\n\n\nyear, month, day\nymd()\n\n\nyear, day, month\nydm()\n\n\nmonth, day, year\nmdy()\n\n\nday, month, year\ndmy()\n\n\nhour, minute\nhm()\n\n\nhour, minute, second\nhms()\n\n\nyear, month, day, hour, minute, second\nymd_hms()"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#accessing-date-parts",
    "href": "lesson-day3-01-wrangling.html#accessing-date-parts",
    "title": "Tidying data",
    "section": "",
    "text": "Date component\nFunction\n\n\n\n\nYear\nyear()\n\n\nMonth\nmonth()\n\n\nWeek\nweek()\n\n\nDay of year\nyday()\n\n\nDay of month\nmday()\n\n\nDay of week\nwday()\n\n\nHour\nhour()\n\n\nMinute\nminute()\n\n\nSecond\nymd_hms()\n\n\nTime zone\nymd_hms()"
  },
  {
    "objectID": "lesson-day3-01-wrangling.html#lubridate-in-action",
    "href": "lesson-day3-01-wrangling.html#lubridate-in-action",
    "title": "Tidying data",
    "section": "",
    "text": "df\n\n\n\n  \n\n\n\n\n\ndf |&gt; \n  mutate(birthday_clean=mdy(birthday)) |&gt; \n  mutate(month=month(birthday_clean)) |&gt; \n  mutate(year=year(birthday_clean)) |&gt; \n  mutate(week=week(birthday_clean))"
  },
  {
    "objectID": "resources/functions.html",
    "href": "resources/functions.html",
    "title": "R Functions",
    "section": "",
    "text": "An opinionated list of the most common data wrangling functions. It leans heavily into the Tidyverse.\n\n\n\nread_csv() imports data from a CSV file. (It handles data types better than the base R read.csv()). Also write_csv() when you need export as CSV. Example: read_csv(\"path/to/file.csv\").\nwrite_rds to save a data frame as an .rds R data data file. This preserves all the data types. read_rds() to import R data. Example: read_rds(\"path/to/file.rds\").\nreadxl is a package we didn’t use, but it has read_excel() that allows you to import from an Excel file, including specified sheets and ranges.\nclean_names() from the library(janitor) package standardizes column names.\n\n\n\n\n\nselect() to select columns. Example: select(col01, col02) or select(-excluded_col).\nrename() to rename a column. Example: rename(new_name = old_name).\nfilter() to filter rows of data. Example: filter(column_name == \"value\").\n\nSee Relational Operators like ==, &gt;, &gt;= etc.\nSee Logical operators like &, | etc.\nSee is.na tests if a value is missing.\n\ndistinct() will filter rows down to the unique values of the columns given.\narrange() sorts data based on values in a column. Use desc() to reverse the order. Example: arrange(col_name %&gt;% desc())\nmutate() changes an existing column or creates a new one. Example: mutate(new_col = (col01 / col02)).\nround() is a base R function that can round a number to a set decimal point. Often used within a mutate() function.\nrecode(), if_else() and case_when() are all functions that can be used with mutate() to create new categorizations with your data.\npivot_longer() “lengthens” data, increasing the number of rows and decreasing the number of columns.\npivot_wider() is the opposite of pivot_longer().\n\n\n\n\n\ngroup_by() and summarize() often come together. When you use group_by(), every function after it is broken down by that grouping. We often add arrange() to these, calling this our GSA functions.\nTo break or remove groupings, use ungroup().\ncount() is a shortcut for GSA that counts the number rows based on variable groups you feed it.\n\nA group_by/summarize/arrange example that would find the songs with the most weeks on the Billboard Hot 100, along with the “top” position the song reached on the chart:\nhot100 |&gt; \n  group_by(song, artist) |&gt; \n  summarize(\n    weeks = n(),\n    top_chart_position = min(peak_position)\n  ) |&gt; \n  arrange(desc(weeks))\n\n\n\nThese are the function often used within summarize():\n\nn() to count the number of rows. n_distinct() counts the unique values.\nsum() to add things together.\nmean() to get an average.\nmedian() to get the median.\nmin() to get the smallest value. max() for the largest.\n+, -, *, / are math operators similar to a calculator."
  },
  {
    "objectID": "resources/functions.html#importexport",
    "href": "resources/functions.html#importexport",
    "title": "R Functions",
    "section": "",
    "text": "read_csv() imports data from a CSV file. (It handles data types better than the base R read.csv()). Also write_csv() when you need export as CSV. Example: read_csv(\"path/to/file.csv\").\nwrite_rds to save a data frame as an .rds R data data file. This preserves all the data types. read_rds() to import R data. Example: read_rds(\"path/to/file.rds\").\nreadxl is a package we didn’t use, but it has read_excel() that allows you to import from an Excel file, including specified sheets and ranges.\nclean_names() from the library(janitor) package standardizes column names."
  },
  {
    "objectID": "resources/functions.html#data-manipulation",
    "href": "resources/functions.html#data-manipulation",
    "title": "R Functions",
    "section": "",
    "text": "select() to select columns. Example: select(col01, col02) or select(-excluded_col).\nrename() to rename a column. Example: rename(new_name = old_name).\nfilter() to filter rows of data. Example: filter(column_name == \"value\").\n\nSee Relational Operators like ==, &gt;, &gt;= etc.\nSee Logical operators like &, | etc.\nSee is.na tests if a value is missing.\n\ndistinct() will filter rows down to the unique values of the columns given.\narrange() sorts data based on values in a column. Use desc() to reverse the order. Example: arrange(col_name %&gt;% desc())\nmutate() changes an existing column or creates a new one. Example: mutate(new_col = (col01 / col02)).\nround() is a base R function that can round a number to a set decimal point. Often used within a mutate() function.\nrecode(), if_else() and case_when() are all functions that can be used with mutate() to create new categorizations with your data.\npivot_longer() “lengthens” data, increasing the number of rows and decreasing the number of columns.\npivot_wider() is the opposite of pivot_longer()."
  },
  {
    "objectID": "resources/functions.html#aggregation",
    "href": "resources/functions.html#aggregation",
    "title": "R Functions",
    "section": "",
    "text": "group_by() and summarize() often come together. When you use group_by(), every function after it is broken down by that grouping. We often add arrange() to these, calling this our GSA functions.\nTo break or remove groupings, use ungroup().\ncount() is a shortcut for GSA that counts the number rows based on variable groups you feed it.\n\nA group_by/summarize/arrange example that would find the songs with the most weeks on the Billboard Hot 100, along with the “top” position the song reached on the chart:\nhot100 |&gt; \n  group_by(song, artist) |&gt; \n  summarize(\n    weeks = n(),\n    top_chart_position = min(peak_position)\n  ) |&gt; \n  arrange(desc(weeks))"
  },
  {
    "objectID": "resources/functions.html#math",
    "href": "resources/functions.html#math",
    "title": "R Functions",
    "section": "",
    "text": "These are the function often used within summarize():\n\nn() to count the number of rows. n_distinct() counts the unique values.\nsum() to add things together.\nmean() to get an average.\nmedian() to get the median.\nmin() to get the smallest value. max() for the largest.\n+, -, *, / are math operators similar to a calculator."
  },
  {
    "objectID": "resources/keyboard-shortcuts.html",
    "href": "resources/keyboard-shortcuts.html",
    "title": "Keyboard commands",
    "section": "",
    "text": "Keyboard commands\nLearning keyboard shortcuts can make you a more efficient coder.\nYou can find a list of all the shortcuts at Tools → Keyboard Shortcuts Help.\nAnother way to find a menu item (and to learn its keyboard equivalent) is with the Help menu. Just type in the menu item and it will show you where it is.\nThe must-learn keyboard commands:\n\nMacPC\n\n\n\nInsert code chunk: Cmd+Option+i\nType a pipe |&gt; or %&gt;$: Cmd+Shift+m\nType the assignment arrow &lt;-: Option+-\nRun a single line of code: While on the line, do Cmd+Return\nRun a chunk of code: While in the code chunk, so Cmd+Shift+Return\nRun all chunks: Cmd+Option+r\nToggle to/from a comment: While on line(s) of code, do Cmd+Shift+c\n\n\n\n\nInsert code chunk: Ctrl+Alt+i\nType a pipe |&gt; or %&gt;$: Ctrl+Shift+m\nType the assignment arrow &lt;-: Alt+- ???\nRun a single line of code: While on the line, do Ctrl+Enter\nRun a chunk of code: While in the code chunk, so Ctrl+Shift+Enter\nRun all chunks: Ctrl+Alt+r\nToggle to/from a comment: While on line(s) of code, do Ctrl+Shift+c"
  },
  {
    "objectID": "resources/projects.html",
    "href": "resources/projects.html",
    "title": "Managing a project",
    "section": "",
    "text": "Using RStudio projects make it straightforward to collect related work into its own folder. All your data, notebooks and related documents can be stored together. It saves you from some R-related hassles of setting working directories.\nWith the addition of Quarto, you can turn your notebooks into readable documents for others, including your future self. While you don’t have to publish your work, it makes it easy to do so using Quarto Pub, or Github if you are already using source control.\nMy favorite default configuration for projects is the Quarto Website. With one extra configuration file you can tie all your notebooks together into a linked website. There are other bells and whistles, but that’s the gist.\nIn the video below, I go through the process of creating and configuring a Quarto Website. It’s not perfect, but you may find it useful.\n\n\nSome things I do are for personal preference, but they are usually based on some logic. One thing I can say for sure: The more consistent you are setting up your projects, easier it is for your future self and collaborators.\n\n\n\n\nI create two data folders: data-raw for my original data, and data-processed for anything I generate. I don’t want to write over original data, which should not be changed. It doesn’t stop me from making mistakes, but I’m less likely to make them.\nI do my data cleaning in its own notebook, then export the cleaned data as an RDS file that preserves data types and the like. (I start any analysis from this cleaned data.) This helps avoid repetition in cleaning the same data over and over, and possibly in different ways. It’s not unusual for me to be doing an analysis and realize I need to back into my cleaning notebook and fix or add something. I just do that and re-run everything so all future notebooks have the fix.\nI name my notebooks in the order they should be run, like 01-cleaning.qmd needs to be run before 02-analysis.qmd. This might be overkill, but it is what I do. Also, when I export data from a notebook, I include that number or notebook name so I know where it came from.\nI sometimes have a resources folder where I might store data dictionaries or documents related to my source data.\nI use the index.qmd file of the Quarto website as my “about this project” page. I explain there what the project is about and link to the source of my data. Depending on the need I might have notes about how I downloaded or processed the data (though that might be in my cleaning notebook.) I include links to stories born from the project once they publish.\nI don’t use the default about.qmd page created with a Quarto website. I just rename that as my cleaning notebook.\n\n\n\n\n\nThe Quarto Website documentation is pretty good. It explains different configurations and navigation options you can use in the _quarto.yml file.\nOne thing I always add to _quarto.yml is df-print: paged. It makes your table output much nicer on the rendered pages. You can read about it here.\n\n\n\n\nhttps://quartopub.com/ is a website where you can publish your notebooks for free through a simple command. You have to install the Quarto CLI and use the Terminal to push the files, but it is just a simple two-word command.\n\n\n\nIf you use git and Github, you can take advantage of Github Pages to publish rendered pages along with your project.\n\nYou can update the _quarto.yml to change the output directory to use docs. This allows you to use Github Pages to publish your html for free right with your code. Directions are here.\nIf you use gitignore.io to create your gitignore file, be sure to comment out or remove the line about the docs folder. By default that configuration excludes pushing docs, but you want it if you are using Github Pages.\nYou can using Quarto includes to pull your standard README.md into your index page. I guild a regular README and then in my index I put one line: {{&lt; include README.md &gt;}}. This way your README works on Github, but also serves as the index of your project website. You just can’t use fancy Quarto stuff like callouts, but regular Markdown is fine."
  },
  {
    "objectID": "resources/projects.html#preference-vs-convention",
    "href": "resources/projects.html#preference-vs-convention",
    "title": "Managing a project",
    "section": "",
    "text": "Some things I do are for personal preference, but they are usually based on some logic. One thing I can say for sure: The more consistent you are setting up your projects, easier it is for your future self and collaborators."
  },
  {
    "objectID": "resources/projects.html#file-and-folder-management",
    "href": "resources/projects.html#file-and-folder-management",
    "title": "Managing a project",
    "section": "",
    "text": "I create two data folders: data-raw for my original data, and data-processed for anything I generate. I don’t want to write over original data, which should not be changed. It doesn’t stop me from making mistakes, but I’m less likely to make them.\nI do my data cleaning in its own notebook, then export the cleaned data as an RDS file that preserves data types and the like. (I start any analysis from this cleaned data.) This helps avoid repetition in cleaning the same data over and over, and possibly in different ways. It’s not unusual for me to be doing an analysis and realize I need to back into my cleaning notebook and fix or add something. I just do that and re-run everything so all future notebooks have the fix.\nI name my notebooks in the order they should be run, like 01-cleaning.qmd needs to be run before 02-analysis.qmd. This might be overkill, but it is what I do. Also, when I export data from a notebook, I include that number or notebook name so I know where it came from.\nI sometimes have a resources folder where I might store data dictionaries or documents related to my source data.\nI use the index.qmd file of the Quarto website as my “about this project” page. I explain there what the project is about and link to the source of my data. Depending on the need I might have notes about how I downloaded or processed the data (though that might be in my cleaning notebook.) I include links to stories born from the project once they publish.\nI don’t use the default about.qmd page created with a Quarto website. I just rename that as my cleaning notebook."
  },
  {
    "objectID": "resources/projects.html#quarto-yaml-configurations",
    "href": "resources/projects.html#quarto-yaml-configurations",
    "title": "Managing a project",
    "section": "",
    "text": "The Quarto Website documentation is pretty good. It explains different configurations and navigation options you can use in the _quarto.yml file.\nOne thing I always add to _quarto.yml is df-print: paged. It makes your table output much nicer on the rendered pages. You can read about it here."
  },
  {
    "objectID": "resources/projects.html#quarto-pub",
    "href": "resources/projects.html#quarto-pub",
    "title": "Managing a project",
    "section": "",
    "text": "https://quartopub.com/ is a website where you can publish your notebooks for free through a simple command. You have to install the Quarto CLI and use the Terminal to push the files, but it is just a simple two-word command."
  },
  {
    "objectID": "resources/projects.html#github-specific-tricks",
    "href": "resources/projects.html#github-specific-tricks",
    "title": "Managing a project",
    "section": "",
    "text": "If you use git and Github, you can take advantage of Github Pages to publish rendered pages along with your project.\n\nYou can update the _quarto.yml to change the output directory to use docs. This allows you to use Github Pages to publish your html for free right with your code. Directions are here.\nIf you use gitignore.io to create your gitignore file, be sure to comment out or remove the line about the docs folder. By default that configuration excludes pushing docs, but you want it if you are using Github Pages.\nYou can using Quarto includes to pull your standard README.md into your index page. I guild a regular README and then in my index I put one line: {{&lt; include README.md &gt;}}. This way your README works on Github, but also serves as the index of your project website. You just can’t use fancy Quarto stuff like callouts, but regular Markdown is fine."
  },
  {
    "objectID": "slides/03_tidying_data.html#lets-start-out-with-two-data-frames-x-and-y",
    "href": "slides/03_tidying_data.html#lets-start-out-with-two-data-frames-x-and-y",
    "title": "Joining and Tidying",
    "section": "Let’s start out with two data frames: x and y",
    "text": "Let’s start out with two data frames: x and y\n\nx &lt;- data.frame(id=c(1,2,3), x=c(\"x1\", \"x2\", \"x3\"))\n\nx\n\n  id  x\n1  1 x1\n2  2 x2\n3  3 x3\n\n\n\n\ny &lt;- data.frame(id=c(1,2,4), y=c(\"y1\", \"y2\", \"y4\"))\n\ny\n\n  id  y\n1  1 y1\n2  2 y2\n3  4 y4"
  },
  {
    "objectID": "slides/03_tidying_data.html#two-data-frames",
    "href": "slides/03_tidying_data.html#two-data-frames",
    "title": "Joining and Tidying",
    "section": "Two data frames",
    "text": "Two data frames"
  },
  {
    "objectID": "slides/03_tidying_data.html#left_join",
    "href": "slides/03_tidying_data.html#left_join",
    "title": "Joining and Tidying",
    "section": "left_join()",
    "text": "left_join()\n\nlibrary(dplyr)\n\nleft_join(x, y)\n\n  id  x    y\n1  1 x1   y1\n2  2 x2   y2\n3  3 x3 &lt;NA&gt;"
  },
  {
    "objectID": "slides/03_tidying_data.html#left_join-illustrated",
    "href": "slides/03_tidying_data.html#left_join-illustrated",
    "title": "Joining and Tidying",
    "section": "left_join() illustrated",
    "text": "left_join() illustrated"
  },
  {
    "objectID": "slides/03_tidying_data.html#two-data-frames-x-and-y-but-with-different-column-names",
    "href": "slides/03_tidying_data.html#two-data-frames-x-and-y-but-with-different-column-names",
    "title": "Joining and Tidying",
    "section": "Two data frames: x and y but with different column names",
    "text": "Two data frames: x and y but with different column names\n\nx &lt;- data.frame(id=c(1,2,3), x=c(\"x1\", \"x2\", \"x3\"))\n\nx\n\n  id  x\n1  1 x1\n2  2 x2\n3  3 x3\n\n\n\n\ny &lt;- data.frame(new_id=c(1,2,4), y=c(\"y1\", \"y2\", \"y4\"))\n\ny\n\n  new_id  y\n1      1 y1\n2      2 y2\n3      4 y4\n\n\n\n\n\nleft_join(x, y, by=c(\"id\"=\"new_id\"))\n\n  id  x    y\n1  1 x1   y1\n2  2 x2   y2\n3  3 x3 &lt;NA&gt;"
  },
  {
    "objectID": "slides/03_tidying_data.html#watch-out-for-repeated-data",
    "href": "slides/03_tidying_data.html#watch-out-for-repeated-data",
    "title": "Joining and Tidying",
    "section": "Watch out for repeated data",
    "text": "Watch out for repeated data\n\n\n  id  x\n1  1 x1\n2  2 x2\n3  3 x3\n\n\n\n\n  id  y\n1  1 y1\n2  2 y2\n3  4 y4\n4  2 y5\n\n\n\n\nleft_join(x, y)\n\n  id  x    y\n1  1 x1   y1\n2  2 x2   y2\n3  2 x2   y5\n4  3 x3 &lt;NA&gt;"
  },
  {
    "objectID": "slides/03_tidying_data.html#extra-rows-illustrated",
    "href": "slides/03_tidying_data.html#extra-rows-illustrated",
    "title": "Joining and Tidying",
    "section": "Extra rows illustrated",
    "text": "Extra rows illustrated"
  },
  {
    "objectID": "slides/03_tidying_data.html#right_join",
    "href": "slides/03_tidying_data.html#right_join",
    "title": "Joining and Tidying",
    "section": "right_join()",
    "text": "right_join()"
  },
  {
    "objectID": "slides/03_tidying_data.html#full_join",
    "href": "slides/03_tidying_data.html#full_join",
    "title": "Joining and Tidying",
    "section": "full_join()",
    "text": "full_join()"
  },
  {
    "objectID": "slides/03_tidying_data.html#inner_join",
    "href": "slides/03_tidying_data.html#inner_join",
    "title": "Joining and Tidying",
    "section": "inner_join()",
    "text": "inner_join()"
  },
  {
    "objectID": "slides/03_tidying_data.html#anti_join",
    "href": "slides/03_tidying_data.html#anti_join",
    "title": "Joining and Tidying",
    "section": "anti_join()",
    "text": "anti_join()"
  },
  {
    "objectID": "slides/03_tidying_data.html#stringr-package",
    "href": "slides/03_tidying_data.html#stringr-package",
    "title": "Joining and Tidying",
    "section": "stringr package",
    "text": "stringr package"
  },
  {
    "objectID": "slides/03_tidying_data.html#stringr-functions",
    "href": "slides/03_tidying_data.html#stringr-functions",
    "title": "Joining and Tidying",
    "section": "stringr functions",
    "text": "stringr functions\nKey stringr functions:\nIn this section, we will learn the following stringr functions:\n\n\nstr_to_upper() str_to_lower() str_to_title()\nstr_trim() str_squish()\nstr_c()\nstr_detect()\nstr_subset()\nstr_sub()"
  },
  {
    "objectID": "slides/03_tidying_data.html#stringr-in-action",
    "href": "slides/03_tidying_data.html#stringr-in-action",
    "title": "Joining and Tidying",
    "section": "stringr in action",
    "text": "stringr in action\n\nlibrary(stringr)\n\ntest_text &lt;- \"tHiS iS A rANsOM noTE!\"\n\n\n\nstr_to_upper(test_text)\n\n[1] \"THIS IS A RANSOM NOTE!\"\n\n\n\n\n\nstr_to_lower(test_text)\n\n[1] \"this is a ransom note!\"\n\n\n\n\n\nstr_to_title(test_text)\n\n[1] \"This Is A Ransom Note!\""
  },
  {
    "objectID": "slides/03_tidying_data.html#trimming-strings",
    "href": "slides/03_tidying_data.html#trimming-strings",
    "title": "Joining and Tidying",
    "section": "Trimming strings",
    "text": "Trimming strings\n\ntest_text &lt;- \"  trim both   \"\n\ntest_text \n\n[1] \"  trim both   \"\n\n\n\n\nstr_trim(test_text, side=\"both\")\n\n[1] \"trim both\"\n\n\n\n\n\nstr_trim(test_text, side=\"left\")\n\n[1] \"trim both   \"\n\n\n\n\n\nstr_trim(test_text, side=\"right\")\n\n[1] \"  trim both\"\n\n\n\n\n\nmessy_text &lt;- \"  sometimes  you get   this \"\n\n\n\n\nstr_squish(messy_text)\n\n[1] \"sometimes you get this\""
  },
  {
    "objectID": "slides/03_tidying_data.html#combining-strings",
    "href": "slides/03_tidying_data.html#combining-strings",
    "title": "Joining and Tidying",
    "section": "Combining strings",
    "text": "Combining strings\n\ntext_a &lt;- \"one\"\n\ntext_b &lt;- \"two\"\n\ntext_a\n\n[1] \"one\"\n\ntext_b\n\n[1] \"two\"\n\n\n\n\nstr_c(text_a, text_b)\n\n[1] \"onetwo\"\n\n\n\n\n\nstr_c(text_a, text_b, sep=\"-\")\n\n[1] \"one-two\"\n\n\n\n\n\nstr_c(text_a, text_b, sep=\" and a \")\n\n[1] \"one and a two\"\n\n\n\n\n\nstr_c(text_a, \" and a \", text_b)\n\n[1] \"one and a two\""
  },
  {
    "objectID": "slides/03_tidying_data.html#extracting-strings",
    "href": "slides/03_tidying_data.html#extracting-strings",
    "title": "Joining and Tidying",
    "section": "Extracting strings",
    "text": "Extracting strings\n\ntest_text &lt;- \"Hello world\"\n\ntest_text \n\n[1] \"Hello world\"\n\n\n\n\nstr_sub(test_text, start = 6)\n\n[1] \" world\"\n\n\n\n\n\nstr_sub(test_text, end = 5) &lt;- \"Howdy\"\n\ntest_text\n\n[1] \"Howdy world\"\n\n\n\n\n\ncn &lt;- \"Kemp County, Georgia\"\n\ncn \n\n[1] \"Kemp County, Georgia\"\n\nstr_replace(cn, \" County, .*\", \"\")\n\n[1] \"Kemp\""
  },
  {
    "objectID": "slides/03_tidying_data.html#more-stringr-functions",
    "href": "slides/03_tidying_data.html#more-stringr-functions",
    "title": "Joining and Tidying",
    "section": "More stringr functions",
    "text": "More stringr functions\nMore functions in stringr and more info on regular expressions here."
  },
  {
    "objectID": "slides/03_tidying_data.html#parse_number",
    "href": "slides/03_tidying_data.html#parse_number",
    "title": "Joining and Tidying",
    "section": "parse_number()",
    "text": "parse_number()\n(from the readr package)\n\nlibrary(readr)\nmessy_numbers &lt;- c(\"$5.00\", \"9,343,200\", \"6.0%\")\n\nmessy_numbers\n\n[1] \"$5.00\"     \"9,343,200\" \"6.0%\"     \n\n\n\n\nparse_number(messy_numbers)\n\n[1]       5 9343200       6"
  },
  {
    "objectID": "slides/03_tidying_data.html#parse_number-1",
    "href": "slides/03_tidying_data.html#parse_number-1",
    "title": "Joining and Tidying",
    "section": "parse_number()",
    "text": "parse_number()"
  },
  {
    "objectID": "slides/03_tidying_data.html#tidy-data",
    "href": "slides/03_tidying_data.html#tidy-data",
    "title": "Joining and Tidying",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "slides/03_tidying_data.html#messy-data",
    "href": "slides/03_tidying_data.html#messy-data",
    "title": "Joining and Tidying",
    "section": "Messy data",
    "text": "Messy data"
  },
  {
    "objectID": "slides/03_tidying_data.html#section",
    "href": "slides/03_tidying_data.html#section",
    "title": "Joining and Tidying",
    "section": "_",
    "text": "_"
  },
  {
    "objectID": "slides/03_tidying_data.html#one-tool-similar-data-structures",
    "href": "slides/03_tidying_data.html#one-tool-similar-data-structures",
    "title": "Joining and Tidying",
    "section": "One tool, similar data structures",
    "text": "One tool, similar data structures"
  },
  {
    "objectID": "slides/03_tidying_data.html#multiple-tools-more-efficient",
    "href": "slides/03_tidying_data.html#multiple-tools-more-efficient",
    "title": "Joining and Tidying",
    "section": "Multiple tools, more efficient",
    "text": "Multiple tools, more efficient"
  },
  {
    "objectID": "slides/03_tidying_data.html#success",
    "href": "slides/03_tidying_data.html#success",
    "title": "Joining and Tidying",
    "section": "Success!",
    "text": "Success!"
  },
  {
    "objectID": "slides/03_tidying_data.html#you-happy",
    "href": "slides/03_tidying_data.html#you-happy",
    "title": "Joining and Tidying",
    "section": "You! Happy!",
    "text": "You! Happy!"
  },
  {
    "objectID": "slides/03_tidying_data.html#sample-data",
    "href": "slides/03_tidying_data.html#sample-data",
    "title": "Joining and Tidying",
    "section": "Sample data",
    "text": "Sample data\n(You don’t have to type this out)\n2 rows x 3 columns\n\ndf &lt;- data.frame(id=c(1,2), x=c(\"a\", \"b\"),\n                 y=c(\"c\", \"d\"), z=c(\"e\", \"f\"))\n\ndf\n\n  id x y z\n1  1 a c e\n2  2 b d f"
  },
  {
    "objectID": "slides/03_tidying_data.html#wide-vs-long",
    "href": "slides/03_tidying_data.html#wide-vs-long",
    "title": "Joining and Tidying",
    "section": "wide vs long",
    "text": "wide vs long"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_longer-illustrated",
    "href": "slides/03_tidying_data.html#pivot_longer-illustrated",
    "title": "Joining and Tidying",
    "section": "pivot_longer() illustrated",
    "text": "pivot_longer() illustrated"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_longer",
    "href": "slides/03_tidying_data.html#pivot_longer",
    "title": "Joining and Tidying",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\nlibrary(tidyr)\n\ndf %&gt;% \n  pivot_longer(cols=x:z,\n               names_to=\"key\",\n               values_to=\"val\")\n\n# A tibble: 6 × 3\n     id key   val  \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x     a    \n2     1 y     c    \n3     1 z     e    \n4     2 x     b    \n5     2 y     d    \n6     2 z     f"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_longer-1",
    "href": "slides/03_tidying_data.html#pivot_longer-1",
    "title": "Joining and Tidying",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\ndf %&gt;% \n  pivot_longer(cols=2:4,\n               names_to=\"key\",\n               values_to=\"val\")\n\n# A tibble: 6 × 3\n     id key   val  \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x     a    \n2     1 y     c    \n3     1 z     e    \n4     2 x     b    \n5     2 y     d    \n6     2 z     f"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_longer-again",
    "href": "slides/03_tidying_data.html#pivot_longer-again",
    "title": "Joining and Tidying",
    "section": "pivot_longer() again",
    "text": "pivot_longer() again\n\ndf\n\n  state ducks fish birds\n1    TX    23    6    99\n2    NY    39   30     3\n3    FL    47   20    64\n\n\n\n\ndf %&gt;% \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\")\n\n# A tibble: 9 × 3\n  state animals total\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 TX    ducks      23\n2 TX    fish        6\n3 TX    birds      99\n4 NY    ducks      39\n5 NY    fish       30\n6 NY    birds       3\n7 FL    ducks      47\n8 FL    fish       20\n9 FL    birds      64"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_longer-again-1",
    "href": "slides/03_tidying_data.html#pivot_longer-again-1",
    "title": "Joining and Tidying",
    "section": "pivot_longer() again",
    "text": "pivot_longer() again\n\ndf\n\n  state ducks fish birds\n1    TX    23    6    99\n2    NY    39   30     3\n3    FL    47   20    64\n\n\n\n\ndf %&gt;% \n  pivot_longer(cols=2:4,\n               names_to=\"animals\",\n               values_to=\"totals\")\n\n# A tibble: 9 × 3\n  state animals totals\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 TX    ducks       23\n2 TX    fish         6\n3 TX    birds       99\n4 NY    ducks       39\n5 NY    fish        30\n6 NY    birds        3\n7 FL    ducks       47\n8 FL    fish        20\n9 FL    birds       64"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot-for-math",
    "href": "slides/03_tidying_data.html#pivot-for-math",
    "title": "Joining and Tidying",
    "section": "pivot for math",
    "text": "pivot for math\n\ndf\n\n  state ducks fish birds\n1    TX    23    6    99\n2    NY    39   30     3\n3    FL    47   20    64\n\n\n\n\ndf %&gt;% \n  pivot_longer(cols=ducks:birds,\n               names_to=\"animals\",\n               values_to=\"total\") %&gt;% \n  group_by(state) %&gt;% \n  mutate(percent=\n           round(total/sum(total)*100,1))\n\n# A tibble: 9 × 4\n# Groups:   state [3]\n  state animals total percent\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 TX    ducks      23    18  \n2 TX    fish        6     4.7\n3 TX    birds      99    77.3\n4 NY    ducks      39    54.2\n5 NY    fish       30    41.7\n6 NY    birds       3     4.2\n7 FL    ducks      47    35.9\n8 FL    fish       20    15.3\n9 FL    birds      64    48.9"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_wider",
    "href": "slides/03_tidying_data.html#pivot_wider",
    "title": "Joining and Tidying",
    "section": "pivot_wider()",
    "text": "pivot_wider()\n\ndf_long\n\n# A tibble: 9 × 4\n# Groups:   state [3]\n  state animals total percent\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 TX    ducks      23    18  \n2 TX    fish        6     4.7\n3 TX    birds      99    77.3\n4 NY    ducks      39    54.2\n5 NY    fish       30    41.7\n6 NY    birds       3     4.2\n7 FL    ducks      47    35.9\n8 FL    fish       20    15.3\n9 FL    birds      64    48.9"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_wider-1",
    "href": "slides/03_tidying_data.html#pivot_wider-1",
    "title": "Joining and Tidying",
    "section": "pivot_wider()",
    "text": "pivot_wider()\n\n\ndf_long %&gt;% \n  pivot_wider(names_from=\"animals\", \n              values_from=\"percent\")\n\n# A tibble: 9 × 5\n# Groups:   state [3]\n  state total ducks  fish birds\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 TX       23  18    NA    NA  \n2 TX        6  NA     4.7  NA  \n3 TX       99  NA    NA    77.3\n4 NY       39  54.2  NA    NA  \n5 NY       30  NA    41.7  NA  \n6 NY        3  NA    NA     4.2\n7 FL       47  35.9  NA    NA  \n8 FL       20  NA    15.3  NA  \n9 FL       64  NA    NA    48.9\n\n\n\n\n\ndf_long %&gt;% \n  select(-total) %&gt;% \n  pivot_wider(names_from=\"animals\", \n              values_from=\"percent\") %&gt;% \n  mutate(birds_fish_diff=\n           birds-fish)\n\n# A tibble: 3 × 5\n# Groups:   state [3]\n  state ducks  fish birds birds_fish_diff\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1 TX     18     4.7  77.3            72.6\n2 NY     54.2  41.7   4.2           -37.5\n3 FL     35.9  15.3  48.9            33.6"
  },
  {
    "objectID": "slides/03_tidying_data.html#pivot_wider-more-columns",
    "href": "slides/03_tidying_data.html#pivot_wider-more-columns",
    "title": "Joining and Tidying",
    "section": "pivot_wider() more columns",
    "text": "pivot_wider() more columns\n\ndf_long\n\n# A tibble: 9 × 4\n# Groups:   state [3]\n  state animals total percent\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 TX    ducks      23    18  \n2 TX    fish        6     4.7\n3 TX    birds      99    77.3\n4 NY    ducks      39    54.2\n5 NY    fish       30    41.7\n6 NY    birds       3     4.2\n7 FL    ducks      47    35.9\n8 FL    fish       20    15.3\n9 FL    birds      64    48.9\n\n\n\n\ndf_long %&gt;% \n  pivot_wider(names_from=\"animals\", \n              values_from=c(\"total\", \"percent\")) \n\n# A tibble: 3 × 7\n# Groups:   state [3]\n  state total_ducks total_fish total_birds percent_ducks percent_fish\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 TX             23          6          99          18            4.7\n2 NY             39         30           3          54.2         41.7\n3 FL             47         20          64          35.9         15.3\n# ℹ 1 more variable: percent_birds &lt;dbl&gt;"
  },
  {
    "objectID": "slides/03_tidying_data.html#lubridate-for-dates",
    "href": "slides/03_tidying_data.html#lubridate-for-dates",
    "title": "Joining and Tidying",
    "section": "Lubridate for dates",
    "text": "Lubridate for dates\n\nlibrary(lubridate)\n\ndf &lt;- data.frame(First=c(\"Charlie\", \"Lucy\", \"Peppermint\"),\n                   Last=c(\"Brown\", \"van Pelt\", \"Patty\"),\n                   birthday=c(\"10-31-06\", \"2/4/2007\", \"June 1, 2005\"))\n\ndf\n\n       First     Last     birthday\n1    Charlie    Brown     10-31-06\n2       Lucy van Pelt     2/4/2007\n3 Peppermint    Patty June 1, 2005\n\n\n\n\ndf %&gt;% \n  mutate(birthday_clean=mdy(birthday))\n\n       First     Last     birthday birthday_clean\n1    Charlie    Brown     10-31-06     2006-10-31\n2       Lucy van Pelt     2/4/2007     2007-02-04\n3 Peppermint    Patty June 1, 2005     2005-06-01"
  },
  {
    "objectID": "slides/03_tidying_data.html#reading-dates",
    "href": "slides/03_tidying_data.html#reading-dates",
    "title": "Joining and Tidying",
    "section": "Reading dates",
    "text": "Reading dates\n\n\n\nOrder of elements in date-time\nParse function\n\n\n\n\nyear, month, day\nymd()\n\n\nyear, day, month\nydm()\n\n\nmonth, day, year\nmdy()\n\n\nday, month, year\ndmy()\n\n\nhour, minute\nhm()\n\n\nhour, minute, second\nhms()\n\n\nyear, month, day, hour, minute, second\nymd_hms()"
  },
  {
    "objectID": "slides/03_tidying_data.html#accessing-date-parts",
    "href": "slides/03_tidying_data.html#accessing-date-parts",
    "title": "Joining and Tidying",
    "section": "Accessing date parts",
    "text": "Accessing date parts\n\n\n\nDate component\nFunction\n\n\n\n\nYear\nyear()\n\n\nMonth\nmonth()\n\n\nWeek\nweek()\n\n\nDay of year\nyday()\n\n\nDay of month\nmday()\n\n\nDay of week\nwday()\n\n\nHour\nhour()\n\n\nMinute\nminute()\n\n\nSecond\nymd_hms()\n\n\nTime zone\nymd_hms()"
  },
  {
    "objectID": "slides/03_tidying_data.html#lubridate-in-action",
    "href": "slides/03_tidying_data.html#lubridate-in-action",
    "title": "Joining and Tidying",
    "section": "Lubridate in action",
    "text": "Lubridate in action\n\ndf\n\n       First     Last     birthday\n1    Charlie    Brown     10-31-06\n2       Lucy van Pelt     2/4/2007\n3 Peppermint    Patty June 1, 2005\n\n\n\n\ndf %&gt;% \n  mutate(birthday_clean=mdy(birthday)) %&gt;% \n  mutate(month=month(birthday_clean)) %&gt;% \n  mutate(year=year(birthday_clean)) %&gt;% \n  mutate(week=week(birthday_clean))\n\n       First     Last     birthday birthday_clean month year week\n1    Charlie    Brown     10-31-06     2006-10-31    10 2006   44\n2       Lucy van Pelt     2/4/2007     2007-02-04     2 2007    5\n3 Peppermint    Patty June 1, 2005     2005-06-01     6 2005   22"
  },
  {
    "objectID": "slides/03_tidying_data.html#recognizing-dates",
    "href": "slides/03_tidying_data.html#recognizing-dates",
    "title": "Joining and Tidying",
    "section": "Recognizing dates",
    "text": "Recognizing dates"
  },
  {
    "objectID": "solution-day1-clean.html",
    "href": "solution-day1-clean.html",
    "title": "Solutions Day 1",
    "section": "",
    "text": "This is a completed notebook based on the first day of the Center for Health Journalism Hands-On R course.\nYou will be using daily weather summaries that have been downloaded from Climate Data Online. The explanations use Texas, but there are files for Arkansas, California, New York and North Carolina for practice."
  },
  {
    "objectID": "solution-day1-clean.html#goals",
    "href": "solution-day1-clean.html#goals",
    "title": "Solutions Day 1",
    "section": "Goals",
    "text": "Goals\nOur goals are to:\n\nImport our data.\nCheck all the column data types.\nAdd some new columns based on the date.\nRecode some values in our data.\nRemove some unnecessary variables/columns.\nExport our cleaned data."
  },
  {
    "objectID": "solution-day1-clean.html#setup",
    "href": "solution-day1-clean.html#setup",
    "title": "Solutions Day 1",
    "section": "Setup",
    "text": "Setup\nAdd the entire code block for libraries.\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "solution-day1-clean.html#import",
    "href": "solution-day1-clean.html#import",
    "title": "Solutions Day 1",
    "section": "Import",
    "text": "Import\nFollow the directions in the lesson to import the Texas data, starting with adding a new code block:\n\ntx_raw &lt;- read_csv(\"data-raw/tx.csv\") |&gt; clean_names()\n\nRows: 94503 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): STATION, NAME\ndbl  (7): PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, TOBS\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntx_raw\n\n\n\n  \n\n\n\n\nOYO: Import a different state\nGo through all the steps above, but with different a different state.\n\nI don’t include the answers for OYO in these solutions because it depends on the state used. If you stick with the same naming convention using two letters for your state intead of tx, then everything else should be the same as the code above. Hollar if you need help."
  },
  {
    "objectID": "solution-day1-clean.html#peeking-at-data",
    "href": "solution-day1-clean.html#peeking-at-data",
    "title": "Solutions Day 1",
    "section": "Peeking at data",
    "text": "Peeking at data\nUse head, tail, glimpse and summary to look at the Texas data.\nLook at the top of your data:\n\ntx_raw |&gt; head()\n\n\n\n  \n\n\n\nLook at 8 lines of the bottom of your data:\n\ntx_raw |&gt; tail(8)\n\n\n\n  \n\n\n\nUse glimpse to see all your columns:\n\ntx_raw |&gt; glimpse()\n\nRows: 94,503\nColumns: 10\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n\n\nUse summary to learn about all your variables:\n\ntx_raw |&gt; summary()\n\n   station              name                date                 prcp        \n Length:94503       Length:94503       Min.   :1930-08-01   Min.   : 0.0000  \n Class :character   Class :character   1st Qu.:1959-01-12   1st Qu.: 0.0000  \n Mode  :character   Mode  :character   Median :1980-08-05   Median : 0.0000  \n                                       Mean   :1980-06-04   Mean   : 0.1121  \n                                       3rd Qu.:2002-03-09   3rd Qu.: 0.0000  \n                                       Max.   :2023-09-30   Max.   :12.0700  \n                                                            NA's   :1867     \n      snow            snwd            tavg            tmax       \n Min.   :0.000   Min.   :0.000   Min.   : 0.0    Min.   : 13.00  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:60.0    1st Qu.: 69.00  \n Median :0.000   Median :0.000   Median :73.0    Median : 81.00  \n Mean   :0.003   Mean   :0.004   Mean   :70.2    Mean   : 78.56  \n 3rd Qu.:0.000   3rd Qu.:0.000   3rd Qu.:82.0    3rd Qu.: 91.00  \n Max.   :7.800   Max.   :7.000   Max.   :98.0    Max.   :112.00  \n NA's   :15369   NA's   :15463   NA's   :78843   NA's   :16      \n      tmin            tobs      \n Min.   :-2.00   Min.   :24.00  \n 1st Qu.:47.00   1st Qu.:61.00  \n Median :61.00   Median :72.00  \n Mean   :58.65   Mean   :69.65  \n 3rd Qu.:72.00   3rd Qu.:80.00  \n Max.   :93.00   Max.   :99.00  \n NA's   :16      NA's   :91914  \n\n\n\ntx_raw$date |&gt; summary()\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"1930-08-01\" \"1959-01-12\" \"1980-08-05\" \"1980-06-04\" \"2002-03-09\" \"2023-09-30\" \n\n\n\nOYO: Peek at your state’s data\n\nThis will depend on the state you use. Hollar if you need help."
  },
  {
    "objectID": "solution-day1-clean.html#create-or-change-data",
    "href": "solution-day1-clean.html#create-or-change-data",
    "title": "Solutions Day 1",
    "section": "Create or change data",
    "text": "Create or change data\nCreate year, month values based on the date.\n\ntx_dates &lt;- tx_raw |&gt; \n  mutate(\n    yr = year(date),\n    mn = month(date, label = TRUE),\n    yd = yday(date)\n  )\n\ntx_dates |&gt; glimpse()\n\nRows: 94,503\nColumns: 13\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n$ yr      &lt;dbl&gt; 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 19…\n$ mn      &lt;ord&gt; Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Au…\n$ yd      &lt;dbl&gt; 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 22…\n\n\n\nOYO: Make date parts\nMake the same date parts, but with your own state data:\n\nThis will depend on the state you use. Hollar if you need help."
  },
  {
    "objectID": "solution-day1-clean.html#recoding-values",
    "href": "solution-day1-clean.html#recoding-values",
    "title": "Solutions Day 1",
    "section": "Recoding values",
    "text": "Recoding values\nUse distinct so you can see the station names:\n\ntx_dates |&gt; distinct(name)\n\n\n\n  \n\n\n\n\nUse mutate to recode\nUse recode to create a new column of short city names:\n\ntx_names &lt;- tx_dates |&gt; \n  mutate(\n    city = recode(\n      name,\n      \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\" = \"Houston\",\n      \"AUSTIN CAMP MABRY, TX US\" = \"Austin\",\n      \"DALLAS FAA AIRPORT, TX US\" = \"Dallas\"\n    )\n  )\n\ntx_names |&gt; glimpse()\n\nRows: 94,503\nColumns: 14\n$ station &lt;chr&gt; \"USW00012918\", \"USW00012918\", \"USW00012918\", \"USW00012918\", \"U…\n$ name    &lt;chr&gt; \"HOUSTON WILLIAM P HOBBY AIRPORT, TX US\", \"HOUSTON WILLIAM P H…\n$ date    &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1…\n$ prcp    &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, …\n$ snow    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ snwd    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tavg    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ tmax    &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97…\n$ tmin    &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70…\n$ tobs    &lt;dbl&gt; 86, 89, 89, 85, 83, 89, 83, 82, 85, 85, 87, 86, 86, 85, 86, 84…\n$ yr      &lt;dbl&gt; 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 19…\n$ mn      &lt;ord&gt; Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Au…\n$ yd      &lt;dbl&gt; 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 22…\n$ city    &lt;chr&gt; \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Housto…\n\n\nNow check your results using distinct on name and city.\n\ntx_names |&gt; distinct(name, city)\n\n\n\n  \n\n\n\n\n\nOYO: Recode your cities\nMake similar short names, but for your state.\n\nThis will depend on the state you use. Hollar if you need help."
  },
  {
    "objectID": "solution-day1-clean.html#select",
    "href": "solution-day1-clean.html#select",
    "title": "Solutions Day 1",
    "section": "Select",
    "text": "Select\nCreate a new version of your data with only the columns you need, in the order you want them.\n\ntx_tight &lt;- tx_names |&gt; \n  select(\n    city,\n    date,\n    rain = prcp,\n    snow,\n    snwd,\n    tmax,\n    tmin,\n    yr,\n    mn,\n    yd\n  )\n\ntx_tight |&gt; glimpse()\n\nRows: 94,503\nColumns: 10\n$ city &lt;chr&gt; \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Houston\", \"Houston\",…\n$ date &lt;date&gt; 1930-08-01, 1930-08-02, 1930-08-03, 1930-08-04, 1930-08-05, 1930…\n$ rain &lt;dbl&gt; 3.00, 0.09, NA, 0.02, 0.12, NA, NA, NA, 0.00, 0.00, 0.00, NA, NA,…\n$ snow &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ snwd &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ tmax &lt;dbl&gt; 99, 97, 95, 95, 92, 92, 96, 97, 94, 92, 99, 99, 98, 98, 98, 97, 9…\n$ tmin &lt;dbl&gt; 75, 79, 78, 79, 76, 74, 71, 71, 75, 72, 70, 71, 78, 72, 73, 70, 7…\n$ yr   &lt;dbl&gt; 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930, 1930,…\n$ mn   &lt;ord&gt; Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, …\n$ yd   &lt;dbl&gt; 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, …\n\n\n\nOYO: Select your cols\nGo through the same process as above, but with your own state data.\n\nThis will depend on the state you use. Hollar if you need help."
  },
  {
    "objectID": "solution-day1-clean.html#export",
    "href": "solution-day1-clean.html#export",
    "title": "Solutions Day 1",
    "section": "Export",
    "text": "Export\nWrite the file out as “rds” to the data-processed folder.\n\ntx_tight |&gt; write_rds(\"data-processed/tx_clean.rds\")\n\n\nOYO: Export your state\nWrite your data to the data-processed folder. Make sure you use a name for your state.\n\nThis will depend on the state you use. Hollar if you need help."
  },
  {
    "objectID": "solution-day1-clean.html#checking-your-notebooks",
    "href": "solution-day1-clean.html#checking-your-notebooks",
    "title": "Solutions Day 1",
    "section": "Checking your notebooks",
    "text": "Checking your notebooks\nClear out your notebook and rerun all the code. Render the HTML page."
  },
  {
    "objectID": "solution-day3-wrangling.html",
    "href": "solution-day3-wrangling.html",
    "title": "Solutions Day 3",
    "section": "",
    "text": "We’ve done some basic exploring of the FEMA disaster declaration data.\nBut there are only so many different ways to slice the data based on the variables in the original data set.\nSo we have to be creative and think of what other variables we can add.\nLast time, we added some decades categories using the case_when() and mutate() functions.\nBut let’s go even further by adding an additional data set.\nLet’s bring in county population data from the U.S. Census so we can estimate how many people were affected by different disasters.\nFor this exercise, the data’s already pre-loaded but these are the commands to bring it in yourself using the excellent package called tidycensus (you’ll need to sub in your own Census API key). Usually, you’d have to search for and download this from data.census.gov and do some manual cleaning up before using. But using this package that interfaces with the Census API using the lines of code below gives you the data cleaned up and in a tidy format.\n\n# this is the code to bring in the data but it's already preloaded in this tutorial\nlibrary(tidycensus)\ncensus_api_key(\"API_KEY_GOES_HERE\")\n\ncounty_pop &lt;- get_acs(geography=\"county\", variables=\"B01003_001\", year=2020)\n\nLet’s take a moment to talk about the significance of tidy data. It’s the prefix to a lot of the packages and functions we’re using in this class. But it’s also a philosophy when approaching the structure of data.\nThere’s an ideal structure for how to stack your data.\nAnd that’s with\n\nEach variable is in its own column\nEach case is in its own row\nEach value is in its own cell\n\nLet’s take a look at the new county_pop data frame we imported from the Census API.\n\n# use the function on the object you just imported from the Census API\n\n\nThe function starts with a *g*\n\nPay attention to the column names and what kind of data is in each column.\nNext, let’s take a look at our original FEMA data set.\n\ndf &lt;- read_csv(\"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries.csv\")\n\n\nglimpse(df)\n\nRows: 64,925\nColumns: 25\n$ femaDeclarationString    &lt;chr&gt; \"FM-5389-AZ\", \"FM-5389-AZ\", \"FM-5464-RI\", \"FM…\n$ disasterNumber           &lt;dbl&gt; 5389, 5389, 5464, 5463, 5462, 4731, 5460, 545…\n$ state                    &lt;chr&gt; \"AZ\", \"AZ\", \"RI\", \"KS\", \"NE\", \"CO\", \"OK\", \"OK…\n$ declarationType          &lt;chr&gt; \"FM\", \"FM\", \"FM\", \"FM\", \"FM\", \"DR\", \"FM\", \"FM…\n$ declarationDate          &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ fyDeclared               &lt;dbl&gt; 2021, 2021, 2023, 2023, 2023, 2023, 2023, 202…\n$ incidentType             &lt;chr&gt; \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Floo…\n$ declarationTitle         &lt;chr&gt; \"TELEGRAPH FIRE\", \"TELEGRAPH FIRE\", \"QUEENS R…\n$ ihProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ iaProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ paProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hmProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ incidentBeginDate        &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ incidentEndDate          &lt;dttm&gt; NA, NA, 2023-04-16, 2023-04-16, NA, 2023-06-…\n$ disasterCloseoutDate     &lt;dttm&gt; 2023-09-29, 2023-09-29, NA, NA, NA, NA, NA, …\n$ tribalRequest            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fipsStateCode            &lt;chr&gt; \"04\", \"04\", \"44\", \"20\", \"31\", \"08\", \"40\", \"40…\n$ fipsCountyCode           &lt;chr&gt; \"007\", \"021\", \"009\", \"201\", \"025\", \"009\", \"14…\n$ placeCode                &lt;dbl&gt; 99007, 99021, 99009, 99201, 99025, 99009, 991…\n$ designatedArea           &lt;chr&gt; \"Gila (County)\", \"Pinal (County)\", \"Washingto…\n$ declarationRequestNumber &lt;dbl&gt; 21041, 21041, 23042, 23038, 23036, 23081, 230…\n$ lastIAFilingDate         &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ lastRefresh              &lt;dttm&gt; 2023-09-29 21:02:14, 2023-09-29 21:02:14, 20…\n$ hash                     &lt;chr&gt; \"14b4a2c314124cae33e8ec790782a12c2af10a4c\", \"…\n$ id                       &lt;chr&gt; \"226d44a3-418a-4102-9a55-385ea6599b2a\", \"7304…\n\n\n\n\n\nGEOID in df and placeCode in county_pop\ndesignatedArea in df and NAME in county_pop\nvariable in county_pop and id in df\nestimate in county_pop and disasterNumber in county_pop"
  },
  {
    "objectID": "solution-day3-wrangling.html#intro",
    "href": "solution-day3-wrangling.html#intro",
    "title": "Solutions Day 3",
    "section": "",
    "text": "We’ve done some basic exploring of the FEMA disaster declaration data.\nBut there are only so many different ways to slice the data based on the variables in the original data set.\nSo we have to be creative and think of what other variables we can add.\nLast time, we added some decades categories using the case_when() and mutate() functions.\nBut let’s go even further by adding an additional data set.\nLet’s bring in county population data from the U.S. Census so we can estimate how many people were affected by different disasters.\nFor this exercise, the data’s already pre-loaded but these are the commands to bring it in yourself using the excellent package called tidycensus (you’ll need to sub in your own Census API key). Usually, you’d have to search for and download this from data.census.gov and do some manual cleaning up before using. But using this package that interfaces with the Census API using the lines of code below gives you the data cleaned up and in a tidy format.\n\n# this is the code to bring in the data but it's already preloaded in this tutorial\nlibrary(tidycensus)\ncensus_api_key(\"API_KEY_GOES_HERE\")\n\ncounty_pop &lt;- get_acs(geography=\"county\", variables=\"B01003_001\", year=2020)\n\nLet’s take a moment to talk about the significance of tidy data. It’s the prefix to a lot of the packages and functions we’re using in this class. But it’s also a philosophy when approaching the structure of data.\nThere’s an ideal structure for how to stack your data.\nAnd that’s with\n\nEach variable is in its own column\nEach case is in its own row\nEach value is in its own cell\n\nLet’s take a look at the new county_pop data frame we imported from the Census API.\n\n# use the function on the object you just imported from the Census API\n\n\nThe function starts with a *g*\n\nPay attention to the column names and what kind of data is in each column.\nNext, let’s take a look at our original FEMA data set.\n\ndf &lt;- read_csv(\"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries.csv\")\n\n\nglimpse(df)\n\nRows: 64,925\nColumns: 25\n$ femaDeclarationString    &lt;chr&gt; \"FM-5389-AZ\", \"FM-5389-AZ\", \"FM-5464-RI\", \"FM…\n$ disasterNumber           &lt;dbl&gt; 5389, 5389, 5464, 5463, 5462, 4731, 5460, 545…\n$ state                    &lt;chr&gt; \"AZ\", \"AZ\", \"RI\", \"KS\", \"NE\", \"CO\", \"OK\", \"OK…\n$ declarationType          &lt;chr&gt; \"FM\", \"FM\", \"FM\", \"FM\", \"FM\", \"DR\", \"FM\", \"FM…\n$ declarationDate          &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ fyDeclared               &lt;dbl&gt; 2021, 2021, 2023, 2023, 2023, 2023, 2023, 202…\n$ incidentType             &lt;chr&gt; \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Floo…\n$ declarationTitle         &lt;chr&gt; \"TELEGRAPH FIRE\", \"TELEGRAPH FIRE\", \"QUEENS R…\n$ ihProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ iaProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ paProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hmProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ incidentBeginDate        &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ incidentEndDate          &lt;dttm&gt; NA, NA, 2023-04-16, 2023-04-16, NA, 2023-06-…\n$ disasterCloseoutDate     &lt;dttm&gt; 2023-09-29, 2023-09-29, NA, NA, NA, NA, NA, …\n$ tribalRequest            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fipsStateCode            &lt;chr&gt; \"04\", \"04\", \"44\", \"20\", \"31\", \"08\", \"40\", \"40…\n$ fipsCountyCode           &lt;chr&gt; \"007\", \"021\", \"009\", \"201\", \"025\", \"009\", \"14…\n$ placeCode                &lt;dbl&gt; 99007, 99021, 99009, 99201, 99025, 99009, 991…\n$ designatedArea           &lt;chr&gt; \"Gila (County)\", \"Pinal (County)\", \"Washingto…\n$ declarationRequestNumber &lt;dbl&gt; 21041, 21041, 23042, 23038, 23036, 23081, 230…\n$ lastIAFilingDate         &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ lastRefresh              &lt;dttm&gt; 2023-09-29 21:02:14, 2023-09-29 21:02:14, 20…\n$ hash                     &lt;chr&gt; \"14b4a2c314124cae33e8ec790782a12c2af10a4c\", \"…\n$ id                       &lt;chr&gt; \"226d44a3-418a-4102-9a55-385ea6599b2a\", \"7304…\n\n\n\n\n\nGEOID in df and placeCode in county_pop\ndesignatedArea in df and NAME in county_pop\nvariable in county_pop and id in df\nestimate in county_pop and disasterNumber in county_pop"
  },
  {
    "objectID": "solution-day3-wrangling.html#joins",
    "href": "solution-day3-wrangling.html#joins",
    "title": "Solutions Day 3",
    "section": "Joins",
    "text": "Joins\nA join combines two data sets by adding the columns of one data set alongside the columns of the other, usually some of the rows of the second data set along with some rows of the first data set.\nA successful join requires something consistent between two data sets to match on: keys.\nThe function that’s used most often is left_join() because you have one main data set you’d like to supplement with additional columns.\nHere’s how that looks in action:\n\n\n\n\n\nThe function works like this:\nIf the two data frames you want to join have the same name:\nleft_join(data_frame1, data_frame2, by=\"shared_column_name\")\nIf the two data frames you want to join have different names:\nleft_join(data_frame1, data_frame2, by=c(\"df1_column\"=\"df_2_column\"))\nNotice that the c() argument in the second example is different from how we’ve used it before as combine. The = column matching operator is specific to _join() functions. Type ?left_join() in the R console to see all the other arguments you can use.\nNow there are a few other joins that have their uses.\n\nright_join()\nfull_join()\nsemi_join()\nanti_join()\n\nSo let’s try to create a new dataframe object starting with the disaster declarations of df.\nIf you looked at the two dataframes in the last exercise, you saw that there were similarities in the county names.\n\nglimpse(df)\n\nRows: 64,925\nColumns: 25\n$ femaDeclarationString    &lt;chr&gt; \"FM-5389-AZ\", \"FM-5389-AZ\", \"FM-5464-RI\", \"FM…\n$ disasterNumber           &lt;dbl&gt; 5389, 5389, 5464, 5463, 5462, 4731, 5460, 545…\n$ state                    &lt;chr&gt; \"AZ\", \"AZ\", \"RI\", \"KS\", \"NE\", \"CO\", \"OK\", \"OK…\n$ declarationType          &lt;chr&gt; \"FM\", \"FM\", \"FM\", \"FM\", \"FM\", \"DR\", \"FM\", \"FM…\n$ declarationDate          &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ fyDeclared               &lt;dbl&gt; 2021, 2021, 2023, 2023, 2023, 2023, 2023, 202…\n$ incidentType             &lt;chr&gt; \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Floo…\n$ declarationTitle         &lt;chr&gt; \"TELEGRAPH FIRE\", \"TELEGRAPH FIRE\", \"QUEENS R…\n$ ihProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ iaProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ paProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hmProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ incidentBeginDate        &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ incidentEndDate          &lt;dttm&gt; NA, NA, 2023-04-16, 2023-04-16, NA, 2023-06-…\n$ disasterCloseoutDate     &lt;dttm&gt; 2023-09-29, 2023-09-29, NA, NA, NA, NA, NA, …\n$ tribalRequest            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fipsStateCode            &lt;chr&gt; \"04\", \"04\", \"44\", \"20\", \"31\", \"08\", \"40\", \"40…\n$ fipsCountyCode           &lt;chr&gt; \"007\", \"021\", \"009\", \"201\", \"025\", \"009\", \"14…\n$ placeCode                &lt;dbl&gt; 99007, 99021, 99009, 99201, 99025, 99009, 991…\n$ designatedArea           &lt;chr&gt; \"Gila (County)\", \"Pinal (County)\", \"Washingto…\n$ declarationRequestNumber &lt;dbl&gt; 21041, 21041, 23042, 23038, 23036, 23081, 230…\n$ lastIAFilingDate         &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ lastRefresh              &lt;dttm&gt; 2023-09-29 21:02:14, 2023-09-29 21:02:14, 20…\n$ hash                     &lt;chr&gt; \"14b4a2c314124cae33e8ec790782a12c2af10a4c\", \"…\n$ id                       &lt;chr&gt; \"226d44a3-418a-4102-9a55-385ea6599b2a\", \"7304…\n\nglimpse(county_pop)\n\nRows: 3,221\nColumns: 5\n$ GEOID    &lt;chr&gt; \"01001\", \"01003\", \"01005\", \"01007\", \"01009\", \"01011\", \"01013\"…\n$ NAME     &lt;chr&gt; \"Autauga County, Alabama\", \"Baldwin County, Alabama\", \"Barbou…\n$ variable &lt;chr&gt; \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B010…\n$ estimate &lt;dbl&gt; 55639, 218289, 25026, 22374, 57755, 10173, 19726, 114324, 334…\n$ moe      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nTry the left_join() function below using the correct syntax and columns you identified.\n\njoined &lt;- left_join(df, county_pop, by=c(\"designatedArea\"=\"NAME\"))\n         \nglimpse(joined)              \n\nRows: 64,925\nColumns: 29\n$ femaDeclarationString    &lt;chr&gt; \"FM-5389-AZ\", \"FM-5389-AZ\", \"FM-5464-RI\", \"FM…\n$ disasterNumber           &lt;dbl&gt; 5389, 5389, 5464, 5463, 5462, 4731, 5460, 545…\n$ state                    &lt;chr&gt; \"AZ\", \"AZ\", \"RI\", \"KS\", \"NE\", \"CO\", \"OK\", \"OK…\n$ declarationType          &lt;chr&gt; \"FM\", \"FM\", \"FM\", \"FM\", \"FM\", \"DR\", \"FM\", \"FM…\n$ declarationDate          &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ fyDeclared               &lt;dbl&gt; 2021, 2021, 2023, 2023, 2023, 2023, 2023, 202…\n$ incidentType             &lt;chr&gt; \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Floo…\n$ declarationTitle         &lt;chr&gt; \"TELEGRAPH FIRE\", \"TELEGRAPH FIRE\", \"QUEENS R…\n$ ihProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ iaProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ paProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hmProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ incidentBeginDate        &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ incidentEndDate          &lt;dttm&gt; NA, NA, 2023-04-16, 2023-04-16, NA, 2023-06-…\n$ disasterCloseoutDate     &lt;dttm&gt; 2023-09-29, 2023-09-29, NA, NA, NA, NA, NA, …\n$ tribalRequest            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fipsStateCode            &lt;chr&gt; \"04\", \"04\", \"44\", \"20\", \"31\", \"08\", \"40\", \"40…\n$ fipsCountyCode           &lt;chr&gt; \"007\", \"021\", \"009\", \"201\", \"025\", \"009\", \"14…\n$ placeCode                &lt;dbl&gt; 99007, 99021, 99009, 99201, 99025, 99009, 991…\n$ designatedArea           &lt;chr&gt; \"Gila (County)\", \"Pinal (County)\", \"Washingto…\n$ declarationRequestNumber &lt;dbl&gt; 21041, 21041, 23042, 23038, 23036, 23081, 230…\n$ lastIAFilingDate         &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ lastRefresh              &lt;dttm&gt; 2023-09-29 21:02:14, 2023-09-29 21:02:14, 20…\n$ hash                     &lt;chr&gt; \"14b4a2c314124cae33e8ec790782a12c2af10a4c\", \"…\n$ id                       &lt;chr&gt; \"226d44a3-418a-4102-9a55-385ea6599b2a\", \"7304…\n$ GEOID                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ variable                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ estimate                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ moe                      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nAlright, did this work?\nWe started out with 24 columns in df and now have 28 in the newly created joined data frame.\nSo columns were added. But did the data come with it?\nWhen you scroll to the bottom of the glimpse() output you see a bunch of NAs.\n\n\n\n\n\n\nSo what happened?\nlet’s take a closer look at the first five data points in the two columns we joined on:\n\ndf |&gt; \n  select(designatedArea) |&gt; \n  slice(1:5) |&gt; \n  pull(designatedArea)\n\n[1] \"Gila (County)\"                           \n[2] \"Pinal (County)\"                          \n[3] \"Washington (County)(in (P)MSA 5520,6480)\"\n[4] \"Washington (County)\"                     \n[5] \"Cass (County)\"                           \n\ncounty_pop |&gt; \n  select(NAME) |&gt; \n  slice(1:5) |&gt; \n  pull(NAME)\n\n[1] \"Autauga County, Alabama\" \"Baldwin County, Alabama\"\n[3] \"Barbour County, Alabama\" \"Bibb County, Alabama\"   \n[5] \"Blount County, Alabama\" \n\n\nAlright, so even though they both contain county names the syntax is completely different.\nThe df data frame has parentheses around “County” and the county_pop data frame has a comma followed by the state names.\nThis is why the join ultimately failed.\nIt’s quite deceptive. You ran the code and didn’t get an error.\nThis is why it’s so important to get into the habit of checking for NAs after a join or inspecting the new data frame.\nFailed joins have thrown off many data analyses and will continue to do so.\nHow to join these data sets\nThe best way to join data is using a uniform identification number.\nFor the Census, they have standardized county numbers called GEOIDS. These geographical entities also exist for census tracts and states and other Census boundaries.\nSo the county_pop data frame has a column called GEOID – that’s perfect!\nIt looks like df has a column called fipsCountyCode but if you joined on those two columns, you’d still fail.\nThat’s because GEOID in county_pop is 5 characters wide and fipsCountyCode in df is 3 characters wide.\nData is rarely ready to join straight out the box.\nIt will take some more wrangling to get these data sets to join.\nIf you’ve had some experience with working with Census data then you know a county GEOID has 5 characters.\nTherefore we need to transform fipsCountyCode in df by adding fipsStateCode in front of it.\nTo do that, we’ll use a new function from a new package, stringr."
  },
  {
    "objectID": "solution-day3-wrangling.html#stringr-intro",
    "href": "solution-day3-wrangling.html#stringr-intro",
    "title": "Solutions Day 3",
    "section": "stringr intro",
    "text": "stringr intro\nThere are many wonderful functions in the stringr package that you do things like detect patterns, see if strings start with with a pattern, or split or join or substitute strings.\nIn this instance, we need to combine strings.\nWe’ll use the str_c() function. Get it? It’s short for String Combine.\nUsing mutate() we’ll also name the new column the same one in the county_pop so it’s easier to join.\n\n#library(stringr)\n#if you've loaded tidyverse, you've already loaded stringr\n\ndf_new &lt;- df |&gt; \n  mutate(GEOID=str_c(fipsStateCode, fipsCountyCode))\n\ndf_new |&gt; \n  select(fipsStateCode, fipsCountyCode, GEOID) |&gt; \n  glimpse()\n\nRows: 64,925\nColumns: 3\n$ fipsStateCode  &lt;chr&gt; \"04\", \"04\", \"44\", \"20\", \"31\", \"08\", \"40\", \"40\", \"40\", \"…\n$ fipsCountyCode &lt;chr&gt; \"007\", \"021\", \"009\", \"201\", \"025\", \"009\", \"147\", \"109\",…\n$ GEOID          &lt;chr&gt; \"04007\", \"04021\", \"44009\", \"20201\", \"31025\", \"08009\", \"…\n\n\nAlright! Now let’s join df_new and county_pop like before.\n\njoined_new &lt;- left_join(df_new, county_pop, by=\"GEOID\")\n\nglimpse(joined_new)              \n\nRows: 64,925\nColumns: 30\n$ femaDeclarationString    &lt;chr&gt; \"FM-5389-AZ\", \"FM-5389-AZ\", \"FM-5464-RI\", \"FM…\n$ disasterNumber           &lt;dbl&gt; 5389, 5389, 5464, 5463, 5462, 4731, 5460, 545…\n$ state                    &lt;chr&gt; \"AZ\", \"AZ\", \"RI\", \"KS\", \"NE\", \"CO\", \"OK\", \"OK…\n$ declarationType          &lt;chr&gt; \"FM\", \"FM\", \"FM\", \"FM\", \"FM\", \"DR\", \"FM\", \"FM…\n$ declarationDate          &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ fyDeclared               &lt;dbl&gt; 2021, 2021, 2023, 2023, 2023, 2023, 2023, 202…\n$ incidentType             &lt;chr&gt; \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Fire\", \"Floo…\n$ declarationTitle         &lt;chr&gt; \"TELEGRAPH FIRE\", \"TELEGRAPH FIRE\", \"QUEENS R…\n$ ihProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ iaProgramDeclared        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ paProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hmProgramDeclared        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ incidentBeginDate        &lt;dttm&gt; 2021-06-06, 2021-06-06, 2023-04-14, 2023-04-…\n$ incidentEndDate          &lt;dttm&gt; NA, NA, 2023-04-16, 2023-04-16, NA, 2023-06-…\n$ disasterCloseoutDate     &lt;dttm&gt; 2023-09-29, 2023-09-29, NA, NA, NA, NA, NA, …\n$ tribalRequest            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fipsStateCode            &lt;chr&gt; \"04\", \"04\", \"44\", \"20\", \"31\", \"08\", \"40\", \"40…\n$ fipsCountyCode           &lt;chr&gt; \"007\", \"021\", \"009\", \"201\", \"025\", \"009\", \"14…\n$ placeCode                &lt;dbl&gt; 99007, 99021, 99009, 99201, 99025, 99009, 991…\n$ designatedArea           &lt;chr&gt; \"Gila (County)\", \"Pinal (County)\", \"Washingto…\n$ declarationRequestNumber &lt;dbl&gt; 21041, 21041, 23042, 23038, 23036, 23081, 230…\n$ lastIAFilingDate         &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ lastRefresh              &lt;dttm&gt; 2023-09-29 21:02:14, 2023-09-29 21:02:14, 20…\n$ hash                     &lt;chr&gt; \"14b4a2c314124cae33e8ec790782a12c2af10a4c\", \"…\n$ id                       &lt;chr&gt; \"226d44a3-418a-4102-9a55-385ea6599b2a\", \"7304…\n$ GEOID                    &lt;chr&gt; \"04007\", \"04021\", \"44009\", \"20201\", \"31025\", …\n$ NAME                     &lt;chr&gt; \"Gila County, Arizona\", \"Pinal County, Arizon…\n$ variable                 &lt;chr&gt; \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B0…\n$ estimate                 &lt;dbl&gt; 53846, 447559, 126139, 5474, 26022, 3570, 519…\n$ moe                      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, 164, NA, …\n\n\nAha! We did it!\nLet’s do some quick analysis on it."
  },
  {
    "objectID": "solution-day3-wrangling.html#summarize",
    "href": "solution-day3-wrangling.html#summarize",
    "title": "Solutions Day 3",
    "section": "Summarize",
    "text": "Summarize\nNow that we have population data with every declared disaster, let’s see which 5 disaster type affected the most people in 2021 (sorted high to low).\nFill in the missing code.\n\njoined_new |&gt; \n  mutate(year=year(incidentBeginDate)) |&gt; \n  filter(year==2021) |&gt; \n  group_by(incidentType) |&gt; \n  summarize(population=sum(estimate, na.rm=T)) |&gt; \n  arrange(desc(population)) |&gt; \n  slice(1:5)\n\n\n\n  \n\n\n\n\nTypes of data\n\nCategorical variables are descriptive labels given to individual records, assigning them to different groups. The simplest categorical data is dichotomous, meaning that there are just two possible groups — in an election, for instance, people either voted, or they did not. More commonly, there are multiple categories. When analyzing traffic accidents, for example, you might consider the day of the week on which each incident occurred, giving seven possible categories.\nContinuous data is richer, consisting of numbers that can have a range of values on a sliding scale. When working with weather data, for instance, continuous variables might include temperature and amount of rainfall.\n\nWe also often need to consider date and time, which can be treated as continuous, like a sequence of years; or categorical, like the days of the week. A common task in data stories is to consider how the values for a variable or variables have changed over time.\n\n\nInterviewing data\nThe goal is to get used to asking questions of data by performing the following basic operations with the functions you’ve learned:\n\nSort: Largest to smallest, oldest to newest, alphabetical etc.\nFilter: Select a defined subset of the data.\nSummarize: Derive one value from a series of other values to produce a summary statistic. Examples include:\n\nCount. The number of records.\nSum. Add the values of a continuous variable.\nMean (aka average). The sum of values for a continuous variable divided by the count.\nMedian. The value in the middle, if the values for a continuous variable are sorted in ascending or descending order of magnitude.\nMax, Min. The largest and smallest value for a continuous value, respectively.\n\nMath: Move the summarized data into a new sheet for additional analysis\n\nOften you will group by a categorical variable first, and then summarize a continuous variable for each category.\nLet’s try to summarize a different way. We added up the population for all incident types in 2021.\nBut this time, let’s find the average and median population affected by all incident types in the entire data set.\nArrange it high to low (on avg_pop) and slice out the top 5 rows.\n\njoined_new |&gt; \n  group_by(incidentType) |&gt; \n  summarize(declarations=n(),\n            avg_pop=mean(estimate, na.rm=T),\n            median_pop=median(estimate, na.rm=T)) |&gt; \n  arrange(desc(avg_pop)) |&gt; \n  slice(1:5)\n\n\n\n  \n\n\n\nPretty interesting, right?\nI don’t know if this could lead to a story because the top three incident types that affected the highest average amount of people occurred so rarely."
  },
  {
    "objectID": "solution-day3-wrangling.html#newsroom-math",
    "href": "solution-day3-wrangling.html#newsroom-math",
    "title": "Solutions Day 3",
    "section": "Newsroom math",
    "text": "Newsroom math\nNearly every news story that involves data analysis can be derived from one these formulas.\n\nDifference\n\nx - y\n\nPercent\n\nx / (x + y) * 100\n\nPercent change\n\n(new - old)/old * 100\n\nPer Capita\n\nx / population * some multiplier to raise result to a round number\n\n\nSo let’s say we want to write a story about Kentucky flooding.\nOne thing we can ask is what has changed? Have things gotten worse or have things improved?\nLet’s wrangle the data so we can easily answer that.\nWe’ll need to only compare the current months of 2022 with the past months (otherwise we’d be comparing 12 months of data in 2021 to 8 in 2022 which would be misleading).\n\njoined_new |&gt; \n  filter(state==\"KY\") |&gt; \n  filter(incidentType==\"Flood\") |&gt; \n  mutate(year=year(incidentBeginDate)) |&gt; \n  # extracting months\n  mutate(month=month(incidentBeginDate)) |&gt; \n  # only paying attention to months in current year of data set\n  filter(month %in% c(1:8)) |&gt; \n  filter(year==2020 | year==2021 | year==2022) |&gt; \n  group_by(year) |&gt; \n  summarize(declarations=n(),\n            avg_pop=mean(estimate, na.rm=T),\n            median_pop=median(estimate, na.rm=T))\n\n\n\n  \n\n\n\nHow many more county Flood declarations were there in Kentucky in 2021 compared to 2022?\n___?\nGreat job so far.\nBefore we try out more math we’ll need to learn more techniques to transform the data."
  },
  {
    "objectID": "solution-day3-wrangling.html#tidyr",
    "href": "solution-day3-wrangling.html#tidyr",
    "title": "Solutions Day 3",
    "section": "tidyr",
    "text": "tidyr\nYou need to understand the basics of math to tell a story.\nLet’s say you’re looking at this data because some local disaster occurred and you want to answer the question:\n\nAre things worse now than they were before?\nWhich place has it worst and most recently?\n\nBecause you can go visit that place and find victims to anchor the story narratively\n\n\nBeing able to come up with types of questions and answer them yourself using raw data will help you stand apart from the competition.\nBecause you’re working with raw data intended for use by an agency for one thing, you’ll need to be able to reshape the data so you can do your own analysis, which will include math (such as difference, percents, percent change, and per capita).\nOne advanced technique for transforming data you’ll learn in this section is from the tidyr package.\n\npivot_wider()\npivot_longer()\n\nNow, these used to be called gather() and spread() but the language is a bit clearer now.\nPivots in R mean something else entirely than pivots in Excel.\nIn Excel, pivot tables are used to group and summarize data.\nIn R, you pivot data as in you reshape it. This way you can do math easier across all rows.\nHere’s how it works (pay attention to the colors):\n\n\n\n\n\nLet’s start with this data that we last ended up with.\nBut this time comment comment out the second line so we include all states with floods this time.\nAnd in the 10th line, add “state” as a second argument in the group_by() option after “year”.\n\nlong_flood &lt;- joined_new |&gt; \n  filter(incidentType==\"Flood\") |&gt; \n  mutate(year=year(incidentBeginDate)) |&gt; \n  # extracting months\n  mutate(month=month(incidentBeginDate)) |&gt; \n  # only paying attention to months in current year of data set\n  filter(month %in% c(1:8)) |&gt; \n  filter(year==2020 | year==2021 | year==2022) |&gt; \n  group_by(year, state) |&gt; \n  summarize(declarations=n(),\n            avg_pop=mean(estimate, na.rm=T),\n            median_pop=median(estimate, na.rm=T))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nlong_flood\n\n\n\n  \n\n\n\nOkay, we have tidy data! Each variable is in its own column. Each case is in its own row and each value is in its own cell.\nThis makes it easier to mutate and manipulate (and is also the preferable data structure for most data viz tools).\nHowever, if we wanted to compare 2020 declaration counts to 2021 and 2022 in each state, that would be difficult."
  },
  {
    "objectID": "solution-day3-wrangling.html#pivot_wider",
    "href": "solution-day3-wrangling.html#pivot_wider",
    "title": "Solutions Day 3",
    "section": "pivot_wider()",
    "text": "pivot_wider()\nSo we need to turn this long data into wide data using pivot_wider()\nYou need to identify what column you want to pull the new column names from and which column the values are stored in (“year” and “declarations” respectively. In this specific circumstance (“pivot_”) you’ll need to put the column names in quotation marks.\nWe’ll also need to drop the avg_pop and median_pop columns or else the pivot will fail.\nTo drop columns, you use the - subtract sign in the select() function.\n\nwide_flood &lt;- long_flood |&gt; \n  select(-avg_pop, -median_pop) |&gt; \n  pivot_wider(names_from=\"year\",\n              values_from=\"declarations\")\n\nwide_flood\n\n\n\n  \n\n\n\n\nIn this instance, you do need to put the column names in quotation marks.\n\nGreat job! We can clearly see most states had flooding in only one year.\nKentucky had 50 in 2021 and 20 in 2022 (Okay, the number may change in 2022 depending on when you pull this data and how unlucky Kentucky continues to be).\nWe knew that from our last section but now we can see that it’s the only state that had floods in both years.\nBut we can now answer one of our questions:\n\nWhich place has it worst and most recently?\n\nIf you paginate through the results you’ll see one state in particular went from no floods in 2021 to 19 in 2022.\nNext, we can use summarize() and mutate() to do some math to answer our first question:\n\nAre things worse now than they were before?\n\nNote: Because the column names we care about starts with a number, we need to surround the column names like `this` in the code. The key to the left of the 1 on the keyboard. This is also what you’d have to do if the column names had spaces in them.\nAdd up all the floods by year with summarize and then add a column that calculates the percent change between 2022 and 2021.\nDon’t forget to add the argument that ignores any NA values in the sum() formula.\n\nflood_percent_change &lt;- wide_flood |&gt; \n  summarize(`2020`=sum(`2020`, na.rm=T),\n            `2021`=sum(`2021`, na.rm=T),\n            `2022`=sum(`2022`, na.rm=T)) |&gt; \n  mutate(percent_change=(round((`2022`-`2021`)/`2021`*100,1)))\n\nflood_percent_change\n\n\n\n  \n\n\n\n\nThe formula for percent change is (new-old)/old*100. Also remember the differences between aggregating a data frame and adding a column to an existing data frame."
  },
  {
    "objectID": "solution-day3-wrangling.html#pivot_wider-multiple",
    "href": "solution-day3-wrangling.html#pivot_wider-multiple",
    "title": "Solutions Day 3",
    "section": "pivot_wider() multiple",
    "text": "pivot_wider() multiple\nWhat’s really powerful about pivot_wider() is the option to be able to draw values from more than one column.\nAdjust the code below:\n\nDelete the -avg_pop argument in line 2 (and the comma)\nin values_from, instead of “declarations” type in `c(“declarations”, “avg_pop”)\n\nSee what happens. Also, type it in, don’t copy and paste otherwise you’ll run into syntax issues.\n\nwide_flood_more &lt;- long_flood |&gt; \n  select(-median_pop) |&gt; \n  pivot_wider(names_from=\"year\",\n              values_from=c(\"declarations\", \"avg_pop\"))\n\nwide_flood_more\n\n\n\n  \n\n\n\n\nBe careful about syntax.\nAlso, you still want to pull names from \"year\"\n\nYou’ll have to paginate a bit but you can see that the declarations and average are now prefixes to the _2021 etc years.\nYou could also bring in the median values this way if you want.\nOkay, now that we know how to make long data to wide, let’s convert wide data to long."
  },
  {
    "objectID": "solution-day3-wrangling.html#pivot_longer",
    "href": "solution-day3-wrangling.html#pivot_longer",
    "title": "Solutions Day 3",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nSometimes you’ll get data that looks like this:\n\nfires_wide &lt;- joined_new |&gt; \n  filter(incidentType==\"Fire\") |&gt; \n  mutate(year=year(incidentBeginDate)) |&gt; \n  count(year, NAME) |&gt; \n  filter(!is.na(NAME)) |&gt; \n  pivot_wider(names_from=\"year\", values_from=n)\n\nThese are the declared fire disasters in each county since 1956.\nA state or column in one row and every column after is a different year of data.\nThis is not tidy data. Every year should be its own row. This is fine if we want to calculate changes between individual years.\nBut if we wanted to visualize this, we’d need to transform it long.\nWe want to increase the number of rows and decrease the number of columns.\nWe’ll use the pivot_longer() function which needs:\n\nWhat columns to focus on cols=\nWhat to name the column with the names of the columns names_to=\nWhat to name the column with the values values_to=\n\n\n# We could do cols=`1967`:`2022`\n# or we could do the number of columns in the dataframe as in cols=2:41\n# let's do the first option in this one\n\nfires_wide |&gt; \n  pivot_longer(cols=`1967`:`2022`,\n               names_to=\"year\",\n               values_to=\"declarations\")\n\n\n\n  \n\n\n\n\nMake sure you name the arguments correctly.\n\nAlright! We did it!\nYou now have a handle on all the biggest verbs used to wrangle and transform data.\nThere are many more functions that do more specific things, of course.\nBut this will hopefully get you started on your way. Everything else you may have questions on how to do has probably been asked and answered out in the R online community."
  }
]